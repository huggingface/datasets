{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('datasets': conda)",
      "language": "python",
      "name": "python37764bitdatasetscondae5d8ff60608e4c5c953d6bb643d8ebc5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "HuggingFace nlp library - Overview",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNp6kK7OvSUg",
        "colab_type": "text"
      },
      "source": [
        "# HuggingFace `nlp` library - Quick overview\n",
        "\n",
        "Models come and go (linear models, LSTM, Transformers, ...) but two core elements have consistently been the beating heart of Natural Language Processing: Datasets & Metrics\n",
        "\n",
        "`nlp` is a lightweight and extensible library to easily share and load dataset and evaluation metrics, already providing access to ~100 datasets and ~10 evaluation metrics.\n",
        "\n",
        "The library has several interesting features (beside easy access to datasets/metrics):\n",
        "\n",
        "- Build-in interoperability with PyTorch, Tensorflow 2, Pandas and Numpy\n",
        "- Small and fast library with a transparent and pythonic API\n",
        "- Strive on large datasets: nlp naturally frees you from RAM memory limits, all datasets are memory-mapped on drive by default.\n",
        "- Smart caching with an intelligent `tf.data`-like cache: never wait for your data to process several times\n",
        "\n",
        "`nlp` originated from a fork of the awesome Tensorflow-Datasets and the HuggingFace team want to deeply thank the team behind this amazing library and user API. We have tried to keep a layer of compatibility with `tfds` and a conversion can provide conversion from one format to the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzk9aEtIvSUh",
        "colab_type": "text"
      },
      "source": [
        "# Main datasets API\n",
        "\n",
        "This notebook is a quick dive in the main user API for loading datasets in `nlp`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my95uHbLyjwR",
        "colab_type": "code",
        "outputId": "27de22d4-f69e-4d31-9029-a0a93db06ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install nlp==0.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlp==0.0.3 in /usr/local/lib/python3.6/dist-packages (0.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (1.18.4)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (0.17.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (0.3.1.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp==0.0.3) (0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.0.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.0.3) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.0.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.0.3) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJHyEmievSUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjXLiYxvSUl",
        "colab_type": "code",
        "outputId": "8af39a41-4ffd-472d-9a6f-966f34eca29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Let's import the library\n",
        "import nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.utils.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:nlp.utils.file_utils:TensorFlow version 2.2.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNloBBx-vSUo",
        "colab_type": "text"
      },
      "source": [
        "## Listing the currently available datasets and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3RJisGLvSUp",
        "colab_type": "code",
        "outputId": "5bddbb72-e44b-4ae8-88ab-fdd23920dd91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Currently available datasets and metrics\n",
        "datasets = nlp.list_datasets()\n",
        "metrics = nlp.list_metrics()\n",
        "\n",
        "print(f\"ðŸ¤© Currently {len(datasets)} datasets are available on HuggingFace AWS bucket: \\n\" \n",
        "      + '\\n'.join(dataset.id for dataset in datasets) + '\\n')\n",
        "print(f\"ðŸ¤© Currently {len(metrics)} metrics are available on HuggingFace AWS bucket: \\n\" \n",
        "      + '\\n'.join(metric.id for metric in metrics))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ¤© Currently 87 datasets are available on HuggingFace AWS bucket: \n",
            "aeslc\n",
            "ai2_arc\n",
            "anli\n",
            "billsum\n",
            "blimp\n",
            "blog_authorship_corpus\n",
            "boolq\n",
            "break_data\n",
            "cfq\n",
            "civil_comments\n",
            "cmrc2018\n",
            "cnn_dailymail\n",
            "coarse_discourse\n",
            "com_qa\n",
            "commonsense_qa\n",
            "coqa\n",
            "cornell_movie_dialog\n",
            "cos_e\n",
            "cosmos_qa\n",
            "crime_and_punish\n",
            "csv\n",
            "definite_pronoun_resolution\n",
            "discofuse\n",
            "drop\n",
            "empathetic_dialogues\n",
            "eraser_multi_rc\n",
            "esnli\n",
            "event2Mind\n",
            "flores\n",
            "fquad\n",
            "gap\n",
            "gigaword\n",
            "glue\n",
            "hansards\n",
            "hellaswag\n",
            "imdb\n",
            "jeopardy\n",
            "librispeech_lm\n",
            "lm1b\n",
            "math_dataset\n",
            "mlqa\n",
            "movie_rationales\n",
            "multi_news\n",
            "multi_nli\n",
            "multi_nli_mismatch\n",
            "openbookqa\n",
            "opinosis\n",
            "para_crawl\n",
            "qangaroo\n",
            "qasc\n",
            "quarel\n",
            "quartz\n",
            "quoref\n",
            "race\n",
            "reclor\n",
            "reddit\n",
            "scan\n",
            "scicite\n",
            "scientific_papers\n",
            "scifact\n",
            "sciq\n",
            "scitail\n",
            "sentiment140\n",
            "snli\n",
            "squad\n",
            "squad_it\n",
            "squad_v1_pt\n",
            "squad_v2\n",
            "super_glue\n",
            "ted_hrlr\n",
            "ted_multi\n",
            "tiny_shakespeare\n",
            "tydiqa\n",
            "wiki40b\n",
            "wiki_qa\n",
            "wiki_split\n",
            "wikihow\n",
            "wikipedia\n",
            "wikitext\n",
            "winogrande\n",
            "wiqa\n",
            "x_stance\n",
            "xcopa\n",
            "xnli\n",
            "xquad\n",
            "xtreme\n",
            "yelp_polarity\n",
            "\n",
            "ðŸ¤© Currently 10 metrics are available on HuggingFace AWS bucket: \n",
            "bleu\n",
            "coval\n",
            "gleu\n",
            "glue\n",
            "rouge\n",
            "sacrebleu\n",
            "seqeval\n",
            "squad\n",
            "squad_v2\n",
            "xnli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T5AG3BxvSUr",
        "colab_type": "code",
        "outputId": "909201f4-74d7-465f-817a-b7915532e6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# You can read a few attributes of the datasets before loading them (they are python dataclasses)\n",
        "from dataclasses import asdict\n",
        "\n",
        "for key, value in asdict(datasets[6]).items():\n",
        "    print('ðŸ‘‰ ' + key + ': ' + str(value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ‘‰ id: boolq\n",
            "ðŸ‘‰ key: nlp/boolq/boolq.py\n",
            "ðŸ‘‰ lastModified: 2020-05-13T10:40:19.000Z\n",
            "ðŸ‘‰ description: \\\n",
            "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally \n",
            "occurring ---they are generated in unprompted and unconstrained settings. \n",
            "Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. \n",
            "The text-pair classification setup is similar to existing natural language inference tasks.\n",
            "ðŸ‘‰ citation: \\\n",
            "@inproceedings{clark2019boolq,\n",
            "  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},\n",
            "  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},\n",
            "  booktitle = {NAACL},\n",
            "  year =      {2019},\n",
            "}\n",
            "ðŸ‘‰ size: 3444\n",
            "ðŸ‘‰ etag: \"bf50ccf7a03c3167addb1032f83f883a\"\n",
            "ðŸ‘‰ siblings: [{'key': 'nlp/boolq/boolq.py', 'etag': '\"bf50ccf7a03c3167addb1032f83f883a\"', 'lastModified': '2020-05-13T10:40:19.000Z', 'size': 3444, 'rfilename': 'boolq.py'}, {'key': 'nlp/boolq/dummy/0.1.0/dummy_data.zip', 'etag': '\"880a58d1776b049ef92bbe9586bf6744\"', 'lastModified': '2020-05-13T09:09:44.000Z', 'size': 2368, 'rfilename': 'dummy/0.1.0/dummy_data.zip'}, {'key': 'nlp/boolq/urls_checksums/checksums.txt', 'etag': '\"36185c756d1096a707d3ac70090d9037\"', 'lastModified': '2020-05-13T09:09:44.000Z', 'size': 190, 'rfilename': 'urls_checksums/checksums.txt'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uqSkkSovSUt",
        "colab_type": "text"
      },
      "source": [
        "## An example with SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOXl6afcvSUu",
        "colab_type": "code",
        "outputId": "3cf3cb79-b47b-4d8d-fb45-0f5d61391f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Downloading and loading a dataset\n",
        "\n",
        "dataset = nlp.load_dataset('squad', split='validation[:10%]')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.load:Checking /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py for additional imports.\n",
            "INFO:filelock:Lock 140291400971936 acquired on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad\n",
            "INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\n",
            "INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.py\n",
            "INFO:nlp.load:Found checksums file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/urls_checksums/checksums.txt to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/urls_checksums/checksums.txt\n",
            "INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.json\n",
            "INFO:filelock:Lock 140291400971936 released on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.builder:No config specified, defaulting to first: squad/plain_text\n",
            "INFO:nlp.builder:Overwrite dataset info from restored data version.\n",
            "INFO:nlp.info:Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:nlp.builder:Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0)\n",
            "INFO:nlp.builder:Constructing Dataset for split validation[:10%], from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ0G-eK3vSUw",
        "colab_type": "text"
      },
      "source": [
        "This call to `nlp.load_dataset()` does the following steps under the hood:\n",
        "\n",
        "1. Download and import in the library the **SQuAD python processing script** from HuggingFace AWS bucket if it's not already stored in the library. You can find the SQuAD processing script [here](https://github.com/huggingface/nlp/tree/master/datasets/squad/squad.py) for instance.\n",
        "\n",
        "   Processing scripts are small python scripts which define the info (citation, description) and format of the dataset and contain the URL to the original SQuAD JSON files and the code to load examples from the original SQuAD JSON files.\n",
        "\n",
        "\n",
        "2. Run the SQuAD python processing script which will:\n",
        "    - **Download the SQuAD dataset** from the original URL (see the script) if it's not already downloaded and cached.\n",
        "    - **Process and cache** all SQuAD in a structured Arrow table for each standard splits stored on the drive.\n",
        "\n",
        "      Arrow table are arbitrarly long tables, typed with types that can be mapped to numpy/pandas/python standard types and can store nested objects. They can be directly access from drive, loaded in RAM or even streamed over the web.\n",
        "    \n",
        "\n",
        "3. Return a **dataset build from the splits** asked by the user (default: all), in the above example we create a dataset with the first 10% of the validation split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fercoFwLvSUx",
        "colab_type": "code",
        "outputId": "28d6fe10-2f80-4c60-e201-76a9911a1a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Informations on the dataset (description, citation, size, splits, format...)\n",
        "# are provided in `dataset.info` (as a python dataclass)\n",
        "for key, value in asdict(dataset.info).items():\n",
        "    print('ðŸ‘‰ ' + key + ': ' + str(value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ‘‰ description: Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
            "\n",
            "ðŸ‘‰ citation: @article{2016arXiv160605250R,\n",
            "       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
            "                 Konstantin and {Liang}, Percy},\n",
            "        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
            "      journal = {arXiv e-prints},\n",
            "         year = 2016,\n",
            "          eid = {arXiv:1606.05250},\n",
            "        pages = {arXiv:1606.05250},\n",
            "archivePrefix = {arXiv},\n",
            "       eprint = {1606.05250},\n",
            "}\n",
            "\n",
            "ðŸ‘‰ homepage: https://rajpurkar.github.io/SQuAD-explorer/\n",
            "ðŸ‘‰ license: \n",
            "ðŸ‘‰ features: {'id': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'title': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'context': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'question': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'answers': {'feature': {'text': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'answer_start': {'dtype': 'int32', 'id': None, '_type': 'Value'}}, 'length': -1, 'id': None, '_type': 'Sequence'}}\n",
            "ðŸ‘‰ supervised_keys: None\n",
            "ðŸ‘‰ builder_name: squad\n",
            "ðŸ‘‰ config_name: plain_text\n",
            "ðŸ‘‰ version: {'version_str': '1.0.0', 'description': 'New split API (https://tensorflow.org/datasets/splits)', 'nlp_version_to_prepare': None, 'major': 1, 'minor': 0, 'patch': 0}\n",
            "ðŸ‘‰ splits: {'train': {'name': 'train', 'num_bytes': 79316886, 'num_examples': 87599, 'dataset_name': 'squad'}, 'validation': {'name': 'validation', 'num_bytes': 10472625, 'num_examples': 10570, 'dataset_name': 'squad'}}\n",
            "ðŸ‘‰ size_in_bytes: 0\n",
            "ðŸ‘‰ download_size: 35142551\n",
            "ðŸ‘‰ dataset_size: 89789511\n",
            "ðŸ‘‰ download_checksums: {'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json': [30288272, '3527663986b8295af4f7fcdff1ba1ff3f72d07d61a20f487cb238a6ef92fd955'], 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json': [4854279, '95aa6a52d5d6a735563366753ca50492a658031da74f301ac5238b03966972c9']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE0E87zsvSUz",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting and using the dataset: elements, slices and columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKf4YFnevSU0",
        "colab_type": "text"
      },
      "source": [
        "The returned `Dataset` object is a memory mapped dataset that behave similarly to a normal map-style dataset. It is backed by an Apache Arrow table which allows many interesting features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP1xPqSyvSU0",
        "colab_type": "code",
        "outputId": "bf269629-22a7-4114-de1e-2ef94569a2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset(schema: {'id': 'string', 'title': 'string', 'context': 'string', 'question': 'string', 'answers': 'struct<text: list<item: string>, answer_start: list<item: int32>>'}, num_rows: 1057)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiO3rC8yvSU2",
        "colab_type": "text"
      },
      "source": [
        "You can query it's length and get items or slices like you would do normally with a python mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxLcdj2yvSU3",
        "colab_type": "code",
        "outputId": "5bd6bae8-768a-47c8-af69-7013830c08af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(f\"ðŸ‘‰Dataset len(dataset): {len(dataset)}\")\n",
        "print(\"\\nðŸ‘‰First item 'dataset[0]':\")\n",
        "pprint(dataset[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ðŸ‘‰Dataset len(dataset): 1057\n",
            "\n",
            "ðŸ‘‰First item 'dataset[0]':\n",
            "{'answers': {'answer_start': [177, 177, 177],\n",
            "             'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
            " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
            "            'champion of the National Football League (NFL) for the 2015 '\n",
            "            'season. The American Football Conference (AFC) champion Denver '\n",
            "            'Broncos defeated the National Football Conference (NFC) champion '\n",
            "            'Carolina Panthers 24â€“10 to earn their third Super Bowl title. The '\n",
            "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
            "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
            "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
            "            'with various gold-themed initiatives, as well as temporarily '\n",
            "            'suspending the tradition of naming each Super Bowl game with '\n",
            "            'Roman numerals (under which the game would have been known as '\n",
            "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
            "            'Arabic numerals 50.',\n",
            " 'id': '56be4db0acb8001400a502ec',\n",
            " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
            " 'title': 'Super_Bowl_50'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1WQ_cczP5w",
        "colab_type": "code",
        "outputId": "25d9ec69-8323-4375-e77c-859d929f6137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# Or get slices with several examples:\n",
        "print(\"\\nðŸ‘‰Slice of the two items 'dataset[10:12]':\")\n",
        "pprint(dataset[10:12])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ‘‰Slice of the two items 'dataset[10:12]':\n",
            "OrderedDict([('id', ['56bea9923aeaaa14008c91bb', '56beace93aeaaa14008c91df']),\n",
            "             ('title', ['Super_Bowl_50', 'Super_Bowl_50']),\n",
            "             ('context',\n",
            "              ['Super Bowl 50 was an American football game to determine the '\n",
            "               'champion of the National Football League (NFL) for the 2015 '\n",
            "               'season. The American Football Conference (AFC) champion Denver '\n",
            "               'Broncos defeated the National Football Conference (NFC) '\n",
            "               'champion Carolina Panthers 24â€“10 to earn their third Super '\n",
            "               \"Bowl title. The game was played on February 7, 2016, at Levi's \"\n",
            "               'Stadium in the San Francisco Bay Area at Santa Clara, '\n",
            "               'California. As this was the 50th Super Bowl, the league '\n",
            "               'emphasized the \"golden anniversary\" with various gold-themed '\n",
            "               'initiatives, as well as temporarily suspending the tradition '\n",
            "               'of naming each Super Bowl game with Roman numerals (under '\n",
            "               'which the game would have been known as \"Super Bowl L\"), so '\n",
            "               'that the logo could prominently feature the Arabic numerals '\n",
            "               '50.',\n",
            "               'Super Bowl 50 was an American football game to determine the '\n",
            "               'champion of the National Football League (NFL) for the 2015 '\n",
            "               'season. The American Football Conference (AFC) champion Denver '\n",
            "               'Broncos defeated the National Football Conference (NFC) '\n",
            "               'champion Carolina Panthers 24â€“10 to earn their third Super '\n",
            "               \"Bowl title. The game was played on February 7, 2016, at Levi's \"\n",
            "               'Stadium in the San Francisco Bay Area at Santa Clara, '\n",
            "               'California. As this was the 50th Super Bowl, the league '\n",
            "               'emphasized the \"golden anniversary\" with various gold-themed '\n",
            "               'initiatives, as well as temporarily suspending the tradition '\n",
            "               'of naming each Super Bowl game with Roman numerals (under '\n",
            "               'which the game would have been known as \"Super Bowl L\"), so '\n",
            "               'that the logo could prominently feature the Arabic numerals '\n",
            "               '50.']),\n",
            "             ('question',\n",
            "              ['What day was the Super Bowl played on?',\n",
            "               'Who won Super Bowl 50?']),\n",
            "             ('answers',\n",
            "              [{'answer_start': [334, 334, 334],\n",
            "                'text': ['February 7, 2016', 'February 7', 'February 7, 2016']},\n",
            "               {'answer_start': [177, 177, 177],\n",
            "                'text': ['Denver Broncos',\n",
            "                         'Denver Broncos',\n",
            "                         'Denver Broncos']}])])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXj2Qr5KvSU5",
        "colab_type": "code",
        "outputId": "55e8e69e-80c4-4e3e-db86-56edb74d6f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# You can get a full column of the dataset by indexing with its name as a string:\n",
        "print(dataset['question'][:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Which NFL team represented the AFC at Super Bowl 50?', 'Which NFL team represented the NFC at Super Bowl 50?', 'Where did Super Bowl 50 take place?', 'Which NFL team won Super Bowl 50?', 'What color was used to emphasize the 50th anniversary of the Super Bowl?', 'What was the theme of Super Bowl 50?', 'What day was the game played on?', 'What is the AFC short for?', 'What was the theme of Super Bowl 50?', 'What does AFC stand for?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Au7rqPMvSU7",
        "colab_type": "text"
      },
      "source": [
        "The `__getitem__` method will return different format depending on the type of query:\n",
        "\n",
        "- Items like `dataset[0]` are returned as dict of elements.\n",
        "- Slices like `dataset[10:20]` are returned as dict of lists of elements.\n",
        "- Columns like `dataset['question']` are returned as a list of elements.\n",
        "\n",
        "This may seems surprising at first but in our experiments it's actually a lot easier to use for data processing than returning the same format for each of these views on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DB_y79cvSU8",
        "colab_type": "text"
      },
      "source": [
        "In particular, you can easily iterate along columns in slices, and also naturally permute consecutive indexings with identical results as showed here by permuting column indexing with elements and slices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjGocqArvSU9",
        "colab_type": "code",
        "outputId": "d5e3a4b7-fd8c-4c4c-cc05-90777a22c667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(dataset[0]['question'] == dataset['question'][0])\n",
        "print(dataset[10:20]['context'] == dataset['context'][10:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1-Kj1xQvSU_",
        "colab_type": "text"
      },
      "source": [
        "### Dataset are internally typed and structured\n",
        "\n",
        "The dataset is backed by one (or several) Apache Arrow tables which are typed and allows for fast retrieval and access as well as arbitrary-size memory mapping.\n",
        "\n",
        "This means respectively that the format for the dataset is clearly defined and that you can load datasets of arbitrary size without worrying about RAM memory limitation (basically the dataset take no space in RAM, it's directly read from drive when needed with fast IO access)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAnp_RyPvSVA",
        "colab_type": "code",
        "outputId": "1b7da1af-e44c-49ae-f8f3-078b1d380afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# You can inspect the dataset column names and type \n",
        "print(dataset.column_names)\n",
        "print(dataset.schema)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id', 'title', 'context', 'question', 'answers']\n",
            "id: string not null\n",
            "title: string not null\n",
            "context: string not null\n",
            "question: string not null\n",
            "answers: struct<text: list<item: string>, answer_start: list<item: int32>> not null\n",
            "  child 0, text: list<item: string>\n",
            "      child 0, item: string\n",
            "  child 1, answer_start: list<item: int32>\n",
            "      child 0, item: int32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4v3mOQvSVC",
        "colab_type": "text"
      },
      "source": [
        "### Additional misc properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efFhDWhlvSVC",
        "colab_type": "code",
        "outputId": "d9509d25-8480-469a-d587-4d901258675a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Datasets also have a bunch of properties you can access\n",
        "print(\"The number of bytes allocated on the drive is \", dataset.nbytes)\n",
        "print(\"For comparison, here is the number of bytes allocated in memory which can be\")\n",
        "print(\"accessed with `nlp.total_allocated_bytes()`: \", nlp.total_allocated_bytes())\n",
        "print(\"The number of rows\", dataset.num_rows)\n",
        "print(\"The number of columns\", dataset.num_columns)\n",
        "print(\"The shape (rows, columns)\", dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of bytes allocated on the drive is  10472625\n",
            "For comparison, here is the number of bytes allocated in memory which can be\n",
            "accessed with `nlp.total_allocated_bytes()`:  0\n",
            "The number of rows 1057\n",
            "The number of columns 5\n",
            "The shape (rows, columns) (1057, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2_FBqAQvSVE",
        "colab_type": "text"
      },
      "source": [
        "### Additional misc methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SznY_XqGvSVF",
        "colab_type": "code",
        "outputId": "815bf471-d4d9-46f8-81ce-be7899ae9577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can list the unique elements in a column. This is done by the backend (so fast!)\n",
        "print(f\"dataset.unique('title'): {dataset.unique('title')}\")\n",
        "\n",
        "# This will drop the column 'id'\n",
        "dataset.drop('id')  # Remove column 'id'\n",
        "print(f\"After dataset.drop('id'), remaining columns are {dataset.column_names}\")\n",
        "\n",
        "# This will flatten nested columns (in 'answers' in our case)\n",
        "dataset.flatten()\n",
        "print(f\"After dataset.flatten(), column names are {dataset.column_names}\")\n",
        "\n",
        "# We can also \"dictionnary encode\" a column if many of it's elements are similar\n",
        "# This will reduce it's size by only storing the distinct elements (e.g. string)\n",
        "# It only has effect on the internal storage (no difference from a user point of view)\n",
        "dataset.dictionary_encode_column('title')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset.unique('title'): ['Super_Bowl_50', 'Warsaw']\n",
            "After dataset.drop('id'), remaining columns are ['title', 'context', 'question', 'answers']\n",
            "After dataset.flatten(), column names are ['title', 'context', 'question', 'answers.text', 'answers.answer_start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdyuKs4VvSVH",
        "colab_type": "text"
      },
      "source": [
        "## Cache\n",
        "\n",
        "`nlp` datasets are backed by Apache Arrow cache files which allows:\n",
        "- to load arbitrary large datasets by using [memory mapping](https://en.wikipedia.org/wiki/Memory-mapped_file) (as long as the datasets can fit on the drive)\n",
        "- to use a fast backend to process the dataset efficiently\n",
        "- to do smart caching by storing and reusing the results of operations performed on the drive\n",
        "\n",
        "Let's dive a bit in these parts now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fUcKwcbvSVH",
        "colab_type": "text"
      },
      "source": [
        "You can check the current cache files backing the dataset with the `.cache_file` property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu8TgHTYvSVI",
        "colab_type": "code",
        "outputId": "8d7cd8d4-d433-4035-e81e-4e7cf4371ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dataset.cache_files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'filename': '/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/squad-validation.arrow',\n",
              "  'skip': 0,\n",
              "  'take': 1057},)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjeICK5GvSVK",
        "colab_type": "text"
      },
      "source": [
        "You can clean up the cache files in the current dataset directory (only keeping the currently used one) with `.cleanup_cache_files()`.\n",
        "\n",
        "Be careful that no other process is using some other cache files when running this command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_WNU3dwvSVL",
        "colab_type": "code",
        "outputId": "77beae1d-2a2e-4755-e03a-7b9836de41b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "dataset.cleanup_cache_files()  # Returns the number of removed cache files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Listing files in /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-d99267fa77c05190a9cefa70b7a33564.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-4dc863be0d0c1e9e15749b73b0061784.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-b5fef4a553eb27e130e299608150b28a.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-ca7a1d9079c4c5bcb746ee34fb9587b6.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-0be0bac8de2ddc42bcc650f62286e93a.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-032674ed152efc659573f8fb4916bc54.arrow\n",
            "INFO:nlp.arrow_dataset:Removing /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-e59fce014c308c7a796f7166088ad089.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ox7ppKDvSVN",
        "colab_type": "text"
      },
      "source": [
        "## Modifying the dataset with `dataset.map`\n",
        "\n",
        "There is a powerful method `.map()` which is inspired by `tf.data` map method and that you can use to apply a function to each examples, independantly or in batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz2-27HevSVN",
        "colab_type": "code",
        "outputId": "6b7403f9-3da4-42e4-d7fd-6aadbe2bfbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# `.map()` takes a callable accepting a dict as argument\n",
        "# (same dict as returned by dataset[i])\n",
        "# and iterate over the dataset by calling the function with each example.\n",
        "\n",
        "# Let's print the length of each `context` string in our subset of the dataset\n",
        "# (10% of the validation i.e. 1057 examples)\n",
        "\n",
        "dataset.map(lambda example: print(len(example['context']), end=','))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "775,775,"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,775,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,637,"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1057it [00:00, 5885.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "637,637,637,637,637,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,347,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,394,179,179,179,179,179,179,179,179,179,179,179,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,168,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,638,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,326,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,704,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,917,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1271,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,1166,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,2060,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,929,704,704,704,704,704,704,704,704,704,704,704,704,704,704,353,353,353,353,353,353,353,353,353,353,353,353,353,353,353,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,464,306,306,306,306,306,306,306,306,306,306,306,306,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,372,496,496,496,496,496,496,496,496,496,496,496,496,496,496,496,260,260,260,260,260,260,260,260,260,874,874,874,874,874,874,874,874,874,874,874,874,874,874,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,1025,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,176,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,782,536,536,536,536,536,536,536,536,536,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,666,495,495,495,495,495,495,495,495,495,495,495,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,385,441,441,441,441,441,441,441,441,441,441,441,357,357,357,357,357,357,357,357,357,296,296,296,296,296,296,296,296,296,296,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,644,804,804,804,804,804,804,804,804,804,804,804,397,397,397,397,397,397,397,397,397,397,397,397,397,397,360,360,360,360,360,360,360,973,973,973,973,973,973,973,973,973,973,973,973,973,973,263,263,263,263,263,263,263,263,263,263,263,568,568,568,568,568,568,568,568,568,568,568,264,264,264,264,264,264,264,264,264,264,264,264,264,264,264,892,892,892,892,892,892,892,892,892,892,892,206,206,206,206,206,489,489,489,489,489,489,489,489,489,489,489,489,489,181,181,181,181,181,181,181,181,181,181,181,181,531,531,531,531,531,531,531,531,531,531,531,531,664,664,664,664,664,664,664,664,664,664,664,664,664,664,672,672,672,672,672,672,672,672,672,672,672,672,672,672,858,858,858,858,858,858,858,858,858,858,858,858,634,634,634,634,634,634,634,634,634,634,634,634,634,634,891,891,891,891,891,891,891,891,891,891,891,891,891,488,488,488,488,488,488,488,488,488,488,488,488,942,942,942,942,942,942,942,942,942,942,942,942,942,942,942,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1162,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,1353,522,522,522,522,522,1643,1643,1643,1643,1643,628,628,628,628,628,758,758,758,758,758,883,883,883,883,883,559,559,559,559,559,603,603,603,603,631,631,631,631,631,626,626,626,626,626,541,541,541,541,541,795,795,795,795,795,591,591,591,591,591,568,568,568,568,568,536,536,536,536,536,575,575,575,575,575,571,571,571,571,571,641,641,641,641,641,665,665,665,665,665,1088,1088,1088,1088,1088,1619,1619,1619,1619,1619,939,939,939,939,939,865,865,865,865,865,711,711,711,711,711,831,831,831,831,831,501,501,501,501,501,676,676,676,676,676,854,854,854,854,854,784,784,784,784,784,641,641,641,641,641,544,544,544,544,544,918,918,918,918,918,763,763,763,763,763,906,906,906,906,906,632,632,632,632,632,869,869,869,869,869,1044,1044,1044,1044,1044,760,760,760,760,760,715,715,715,715,715,838,838,838,838,838,881,881,881,881,881,940,940,940,940,940,618,618,618,618,618,1205,1205,1205,534,534,534,534,534,757,757,757,757,757,1239,1239,1239,1239,1239,609,609,609,609,609,798,798,798,798,798,613,613,613,613,613,613,613,613,613,613,"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(schema: {'title': 'string', 'context': 'string', 'question': 'string', 'answers.text': 'list<item: string>', 'answers.answer_start': 'list<item: int32>'}, num_rows: 1057)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta3celHnvSVP",
        "colab_type": "text"
      },
      "source": [
        "This is basically the same as doing\n",
        "\n",
        "```python\n",
        "for example in dataset:\n",
        "    function(example)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Ouw5gDvSVP",
        "colab_type": "text"
      },
      "source": [
        "The above example had no effect on the dataset because the method we supplied to `.map()` didn't return a `dict` or a `abc.Mapping` that could be used to update the examples in the dataset.\n",
        "\n",
        "In such a case, `.map()` will return the same dataset (`self`).\n",
        "\n",
        "Now let's see how we can use a method that actually modify the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEnCi9DFvSVQ",
        "colab_type": "text"
      },
      "source": [
        "### Modifying the dataset example by example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA37VgZhvSVQ",
        "colab_type": "text"
      },
      "source": [
        "The main interest of `.map()` is to update and modify the content of the table and leverage smart caching and fast backend.\n",
        "\n",
        "To use `.map()` to update elements in the table you need to provide a function with the following signature: `function(example: dict) -> dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUr65K-4vSVQ",
        "colab_type": "code",
        "outputId": "507d8b77-0f39-464f-c683-813c2cf2eda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Let's add a prefix 'My cute title: ' to each of our titles\n",
        "\n",
        "def add_prefix_to_title(example):\n",
        "    example['title'] = 'My cute title: ' + example['title']\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_prefix_to_title)\n",
        "\n",
        "print(dataset.unique('title'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-d99267fa77c05190a9cefa70b7a33564.arrow\n",
            "1057it [00:00, 20685.79it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 905032 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-d99267fa77c05190a9cefa70b7a33564.arrow.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['My cute title: Super_Bowl_50', 'My cute title: Warsaw']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcZ_amDAvSVS",
        "colab_type": "text"
      },
      "source": [
        "This call to `.map()` compute and return the updated table. It will also store the updated table in a cache file indexed by the current state and the mapped function.\n",
        "\n",
        "A subsequent call to `.map()` (even in another python session) will reuse the cached file instead of recomputing the operation.\n",
        "\n",
        "You can test this by running again the previous cell, you will see that the result are directly loaded from the cache and not re-computed again.\n",
        "\n",
        "The updated dataset returned by `.map()` is (again) directly memory mapped from drive and not allocated in RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skbf8LUEvSVT",
        "colab_type": "text"
      },
      "source": [
        "The function you provide to `.map()` should accept an input with the format of an item of the dataset: `function(dataset[0])` and return a python dict.\n",
        "\n",
        "The columns and type of the outputs can be different than the input dict. In this case the new keys will be added as additional columns in the dataset.\n",
        "\n",
        "Bascially each dataset example dict is updated with the dictionary returned by the function like this: `example.update(function(example))`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5De0CfTvSVT",
        "colab_type": "code",
        "outputId": "fa6483a7-7c02-49c7-e19d-9d298b043f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Since the input example dict is updated with our function output dict,\n",
        "# we can actually just return the updated 'title' field\n",
        "dataset = dataset.map(lambda example: {'title': 'My cutest title: ' + example['title']})\n",
        "\n",
        "print(dataset.unique('title'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-b5fef4a553eb27e130e299608150b28a.arrow\n",
            "1057it [00:00, 23051.38it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 923001 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-b5fef4a553eb27e130e299608150b28a.arrow.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['My cutest title: My cute title: Super_Bowl_50', 'My cutest title: My cute title: Warsaw']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vny56-vSVV",
        "colab_type": "text"
      },
      "source": [
        "#### Removing columns\n",
        "You can also remove columns when running map with the `remove_columns=List[str]` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sPWnsz-vSVW",
        "colab_type": "code",
        "outputId": "c640538a-0802-43d4-d47b-9613ba211c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# This will remove the 'title' column while doing the update (after having send it the the mapped function so you can use it in your function!)\n",
        "dataset = dataset.map(lambda example: {'new_title': 'Wouhahh: ' + example['title']},\n",
        "                     remove_columns=['title'])\n",
        "\n",
        "print(dataset.column_names)\n",
        "print(dataset.unique('new_title'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-ca7a1d9079c4c5bcb746ee34fb9587b6.arrow\n",
            "1057it [00:00, 17304.91it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 932514 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-ca7a1d9079c4c5bcb746ee34fb9587b6.arrow.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['context', 'question', 'answers.text', 'answers.answer_start', 'new_title']\n",
            "['Wouhahh: My cutest title: My cute title: Super_Bowl_50', 'Wouhahh: My cutest title: My cute title: Warsaw']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G459HzD-vSVY",
        "colab_type": "text"
      },
      "source": [
        "#### Using examples indices\n",
        "With `with_indices=True`, dataset indices (from `0` to `len(dataset)`) will be supplied to the function which must thus have the following signature: `function(example: dict, indice: int) -> dict`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kFL37R2vSVY",
        "colab_type": "code",
        "outputId": "533a5d05-e842-43bc-b33a-52f6c36e19a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# This will add the index in the dataset to the 'question' field\n",
        "dataset = dataset.map(lambda example, idx: {'question': f'{idx}: ' + example['question']},\n",
        "                      with_indices=True)\n",
        "\n",
        "print('\\n'.join(dataset['question'][:5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-0be0bac8de2ddc42bcc650f62286e93a.arrow\n",
            "1057it [00:00, 18794.19it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 937746 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-0be0bac8de2ddc42bcc650f62286e93a.arrow.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: Which NFL team represented the AFC at Super Bowl 50?\n",
            "1: Which NFL team represented the NFC at Super Bowl 50?\n",
            "2: Where did Super Bowl 50 take place?\n",
            "3: Which NFL team won Super Bowl 50?\n",
            "4: What color was used to emphasize the 50th anniversary of the Super Bowl?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xckhVEWFvSVb",
        "colab_type": "text"
      },
      "source": [
        "### Modifying the dataset with batched updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzmicbSnvSVb",
        "colab_type": "text"
      },
      "source": [
        "`.map()` can also work with batch of examples (slices of the dataset).\n",
        "\n",
        "This is particularly interesting if you have a function that can handle batch of inputs like the tokenizers of HuggingFace `tokenizers`.\n",
        "\n",
        "To work on batched inputs set `batched=True` when calling `.map()` and supply a function with the following signature: `function(examples: Dict[List]) -> Dict[List]` or, if you use indices, `function(examples: Dict[List], indices: List[int]) -> Dict[List]`).\n",
        "\n",
        "Bascially, your function should accept an input with the format of a slice of the dataset: `function(dataset[:10])`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxHbgSTL0itj",
        "colab_type": "code",
        "outputId": "af7a0805-4568-4889-9675-44b785289621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7gpEg0yvSVc",
        "colab_type": "code",
        "outputId": "2f37e0cf-bc15-4e5f-bcf8-a9610208b813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Let's import a fast tokenizer that can work on batched inputs\n",
        "# (the 'Fast' tokenizers in HuggingFace)\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAmLTPC9vSVe",
        "colab_type": "code",
        "outputId": "43a393c2-8096-48af-8d1d-2f8d50b09c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Now let's batch tokenize our dataset 'context'\n",
        "dataset = dataset.map(lambda example: tokenizer.batch_encode_plus(example['context']),\n",
        "                      batched=True)\n",
        "\n",
        "print(\"dataset[0]\", dataset[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-032674ed152efc659573f8fb4916bc54.arrow\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.57it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 4749270 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-032674ed152efc659573f8fb4916bc54.arrow.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset[0] {'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': '0: Which NFL team represented the AFC at Super Bowl 50?', 'answers.text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answers.answer_start': [177, 177, 177], 'new_title': 'Wouhahh: My cutest title: My cute title: Super_Bowl_50', 'input_ids': [101, 3198, 5308, 1851, 1108, 1126, 1237, 1709, 1342, 1106, 4959, 1103, 3628, 1104, 1103, 1305, 2289, 1453, 113, 4279, 114, 1111, 1103, 1410, 1265, 119, 1109, 1237, 2289, 3047, 113, 10402, 114, 3628, 7068, 14722, 2378, 1103, 1305, 2289, 3047, 113, 24743, 114, 3628, 2938, 13598, 1572, 782, 1275, 1106, 7379, 1147, 1503, 3198, 5308, 1641, 119, 1109, 1342, 1108, 1307, 1113, 1428, 128, 117, 1446, 117, 1120, 12388, 112, 188, 3339, 1107, 1103, 1727, 2948, 2410, 3894, 1120, 3364, 10200, 117, 1756, 119, 1249, 1142, 1108, 1103, 13163, 3198, 5308, 117, 1103, 2074, 13463, 1103, 107, 5404, 5453, 107, 1114, 1672, 2284, 118, 12005, 11751, 117, 1112, 1218, 1112, 7818, 28117, 20080, 16264, 1103, 3904, 1104, 10505, 1296, 3198, 5308, 1342, 1114, 2264, 183, 15447, 16179, 113, 1223, 1134, 1103, 1342, 1156, 1138, 1151, 1227, 1112, 107, 3198, 5308, 149, 107, 114, 117, 1177, 1115, 1103, 7998, 1180, 15199, 2672, 1103, 4944, 183, 15447, 16179, 1851, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNaJdKskvSVf",
        "colab_type": "code",
        "outputId": "46dbf830-8f9d-4696-d4a4-274c8c4888d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# we have added additional columns\n",
        "print(dataset.column_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['context', 'question', 'answers.text', 'answers.answer_start', 'new_title', 'input_ids', 'token_type_ids', 'attention_mask']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3To8ztMvSVj",
        "colab_type": "code",
        "outputId": "d56d35ff-fe3e-4bae-b66d-0387b4f7026c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Let show a more complex processing with the full preparation of the SQuAD dataset\n",
        "# for training a model from Transformers\n",
        "def convert_to_features(batch):\n",
        "    # Tokenize contexts and questions (as pairs of inputs)\n",
        "    # keep offset mappings for evaluation\n",
        "    input_pairs = list(zip(batch['context'], batch['question']))\n",
        "    encodings = tokenizer.batch_encode_plus(input_pairs,\n",
        "                                            pad_to_max_length=True,\n",
        "                                            return_offsets_mapping=True)\n",
        "\n",
        "    # Compute start and end tokens for labels\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, (text, start) in enumerate(zip(batch['answers.text'], batch['answers.answer_start'])):\n",
        "        first_char = start[0]\n",
        "        last_char = first_char + len(text[0]) - 1\n",
        "        start_positions.append(encodings.char_to_token(i, first_char))\n",
        "        end_positions.append(encodings.char_to_token(i, last_char))\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "dataset = dataset.map(convert_to_features, batched=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-4dc863be0d0c1e9e15749b73b0061784.arrow\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.78it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 1057 examples in 21643250 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-4dc863be0d0c1e9e15749b73b0061784.arrow.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBnmSa46vSVl",
        "colab_type": "code",
        "outputId": "e4380d82-7d1f-4acd-c46f-0b933c11097b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Now our dataset comprise the labels for the start and end position\n",
        "# as well as the offsets for converting back tokens\n",
        "# in span of the original string for evaluation\n",
        "print(\"column_names\", dataset.column_names)\n",
        "print(\"start_positions\", dataset[:5]['start_positions'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "column_names ['context', 'question', 'answers.text', 'answers.answer_start', 'new_title', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions']\n",
            "start_positions [34, 45, 80, 34, 98]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzOXxNzQvSVo",
        "colab_type": "text"
      },
      "source": [
        "## Formating outputs for numpy/torch/tensorflow\n",
        "\n",
        "Now that we have tokenized our inputs, we probably want to use this dataset in a `torch.Dataloader` or a `tf.data.Dataset`.\n",
        "\n",
        "To be able to do this we need to tweak two things:\n",
        "\n",
        "- format the indexing (`__getitem__`) to return numpy/pytorch/tensorflow tensors, instead of python objects, and probably\n",
        "- format the indexing (`__getitem__`) to return only the subset of the columns that we need for our model inputs.\n",
        "\n",
        "  We don't want the columns `id` or `title` as inputs to train our model, but we could still want to keep them in the dataset, for instance for the evaluation of the model.\n",
        "    \n",
        "This is handled by the `.set_format(type: Union[None, str], columns: Union[None, str, List[str]])` where:\n",
        "\n",
        "- `type` define the return type for our dataset `__getitem__` method and is one of `[None, 'numpy', 'pandas', 'torch', 'tensorflow']` (`None` means return python objects), and\n",
        "- `columns` define the columns returned by `__getitem__` and takes the name of a column in the dataset or a list of columns to return (`None` means return all columns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU2h_qQDvSVo",
        "colab_type": "code",
        "outputId": "c5de46eb-4f96-4818-b611-ffdd5d68c2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "columns_to_return = ['input_ids', 'token_type_ids', 'attention_mask',\n",
        "                     'start_positions', 'end_positions']\n",
        "\n",
        "dataset.set_format(type='torch',\n",
        "                   columns=columns_to_return)\n",
        "\n",
        "# Our dataset indexing output is now ready for being used in a pytorch dataloader\n",
        "print('\\n'.join([' '.join((n, str(type(t)), str(t.shape))) for n, t in dataset[:10].items()]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Set __getitem__(key) output type to torch and filter ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_ids <class 'torch.Tensor'> torch.Size([10, 451])\n",
            "token_type_ids <class 'torch.Tensor'> torch.Size([10, 451])\n",
            "attention_mask <class 'torch.Tensor'> torch.Size([10, 451])\n",
            "start_positions <class 'torch.Tensor'> torch.Size([10])\n",
            "end_positions <class 'torch.Tensor'> torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj1ukGIuvSVq",
        "colab_type": "code",
        "outputId": "97299ccf-3dc3-404b-88f2-8be28261f023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Note that the columns are not removed from the dataset, just not returned when calling __getitem__\n",
        "# Similarly the inner type of the dataset is not changed to torch.Tensor, the conversion and filtering is done on-the-fly when querying the dataset\n",
        "print(dataset.column_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['context', 'question', 'answers.text', 'answers.answer_start', 'new_title', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWmmUnlpvSVs",
        "colab_type": "code",
        "outputId": "c59acdfd-5975-4f77-e33d-a5229bee06bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# We can remove the formating with `.reset_format()`\n",
        "# or, identically, a call to `.set_format()` with no arguments\n",
        "dataset.reset_format()\n",
        "\n",
        "print('\\n'.join([' '.join((n, str(type(t)))) for n, t in dataset[:10].items()]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.arrow_dataset:Set __getitem__(key) output type to python objects and filter no columns  (when key is int or slice).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "context <class 'list'>\n",
            "question <class 'list'>\n",
            "answers.text <class 'list'>\n",
            "answers.answer_start <class 'list'>\n",
            "new_title <class 'list'>\n",
            "input_ids <class 'list'>\n",
            "token_type_ids <class 'list'>\n",
            "attention_mask <class 'list'>\n",
            "offset_mapping <class 'list'>\n",
            "start_positions <class 'list'>\n",
            "end_positions <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyUOA07svSVu",
        "colab_type": "code",
        "outputId": "2c08052b-dc80-43f2-d4c1-765f6ccd36f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# The current format can be checked with `.format`,\n",
        "# which is a dict of the type and formating\n",
        "dataset.format"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'columns': ['context',\n",
              "  'question',\n",
              "  'answers.text',\n",
              "  'answers.answer_start',\n",
              "  'new_title',\n",
              "  'input_ids',\n",
              "  'token_type_ids',\n",
              "  'attention_mask',\n",
              "  'offset_mapping',\n",
              "  'start_positions',\n",
              "  'end_positions'],\n",
              " 'type': 'python'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyi2eMeSvSVv",
        "colab_type": "text"
      },
      "source": [
        "# Wrapping this all up (PyTorch)\n",
        "\n",
        "Let's wrap this all up with the full code to load and prepare SQuAD for training a PyTorch model from HuggingFace `transformers` library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvExTIZWvSVw",
        "colab_type": "code",
        "outputId": "dc5ebf41-cca8-49c8-ccf7-c0f5e043e0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "import nlp\n",
        "import torch \n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load our training dataset and tokenizer\n",
        "dataset = nlp.load_dataset('squad')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "def get_correct_alignement(context, answer):\n",
        "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
        "    gold_text = answer['text'][0]\n",
        "    start_idx = answer['answer_start'][0]\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "        return start_idx, end_idx       # When the gold label position is good\n",
        "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "        return start_idx-1, end_idx-1   # When the gold label is off by one character\n",
        "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "        return start_idx-2, end_idx-2   # When the gold label is off by two character\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "# Tokenize our training dataset\n",
        "def convert_to_features(example_batch):\n",
        "    # Tokenize contexts and questions (as pairs of inputs)\n",
        "    input_pairs = list(zip(example_batch['context'], example_batch['question']))\n",
        "    encodings = tokenizer.batch_encode_plus(input_pairs, pad_to_max_length=True)\n",
        "\n",
        "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
        "        start_idx, end_idx = get_correct_alignement(context, answer)\n",
        "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
        "        end_positions.append(encodings.char_to_token(i, end_idx-1))\n",
        "    encodings.update({'start_positions': start_positions,\n",
        "                      'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "dataset['train'] = dataset['train'].map(convert_to_features, batched=True)\n",
        "\n",
        "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
        "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
        "dataset['train'].set_format(type='torch', columns=columns)\n",
        "\n",
        "# Instantiate a PyTorch Dataloader around our dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.load:Checking /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py for additional imports.\n",
            "INFO:filelock:Lock 140291193479120 acquired on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad\n",
            "INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\n",
            "INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.py\n",
            "INFO:nlp.load:Found checksums file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/urls_checksums/checksums.txt to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/urls_checksums/checksums.txt\n",
            "INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.json\n",
            "INFO:filelock:Lock 140291193479120 released on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.builder:No config specified, defaulting to first: squad/plain_text\n",
            "INFO:nlp.builder:Overwrite dataset info from restored data version.\n",
            "INFO:nlp.info:Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:nlp.builder:Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0)\n",
            "INFO:nlp.builder:Constructing Dataset for split None, from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-e59fce014c308c7a796f7166088ad089.arrow\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:38<00:00,  2.31it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 87599 examples in 1098782410 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-e59fce014c308c7a796f7166088ad089.arrow.\n",
            "INFO:nlp.arrow_dataset:Set __getitem__(key) output type to torch and filter ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mHnwMx2vSVx",
        "colab_type": "code",
        "outputId": "b55c2291-1a90-4b81-b470-2a3cbbd2d23f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Let's load a pretrained Bert model and a simple optimizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json from cache at /root/.cache/torch/transformers/774d52b0be7c2f621ac9e64708a8b80f22059f6d0e264e1bdc4f4d71c386c4ea.f44aaaab97e2ee0f8d9071a5cd694e19bf664237a92aea20ebe04ddb7097b494\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/distilbert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/185eb053d63bc5c2d6994e4b2a8e5eb59f31af90db9c5fae5e38c32a986462cb.857b7d17ad0bfaa2eec50caf481575bab1073303fef16bd5f29bc5248b2b8c7d\n",
            "INFO:transformers.modeling_utils:Weights of BertForQuestionAnswering not initialized from pretrained model: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForQuestionAnswering: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biqDH9vpvSVz",
        "colab_type": "code",
        "outputId": "0915bd05-d086-4ca4-eab7-91ff7d9efa38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Now let's train our model\n",
        "\n",
        "model.train()\n",
        "for i, batch in enumerate(dataloader):\n",
        "    outputs = model(**batch)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "    print(f'Step {i} - loss: {loss:.3}')\n",
        "    if i > 3:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 - loss: 6.24\n",
            "Step 1 - loss: 5.4\n",
            "Step 2 - loss: 4.88\n",
            "Step 3 - loss: 5.44\n",
            "Step 4 - loss: 4.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxZQ9Ms_vSV1",
        "colab_type": "text"
      },
      "source": [
        "# Wrapping this all up (Tensorflow)\n",
        "\n",
        "Let's wrap this all up with the full code to load and prepare SQuAD for training a Tensorflow model (works only from the version 2.2.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE8VSTYovSV2",
        "colab_type": "code",
        "outputId": "231b4955-c8bf-4acd-d4ea-07ae4663b7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import nlp\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load our training dataset and tokenizer\n",
        "train_tf_dataset = nlp.load_dataset('squad', split=\"train\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Tokenize our training dataset\n",
        "# The only one diff here is that start_positions and end_positions\n",
        "# must be single dim list => [[23], [45] ...]\n",
        "# instead of => [23, 45 ...]\n",
        "def convert_to_tf_features(example_batch):\n",
        "    # Tokenize contexts and questions (as pairs of inputs)\n",
        "    input_pairs = list(zip(example_batch['context'], example_batch['question']))\n",
        "    encodings = tokenizer.batch_encode_plus(input_pairs, pad_to_max_length=True)\n",
        "\n",
        "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
        "        start_idx, end_idx = get_correct_alignement(context, answer)\n",
        "        start_positions.append([encodings.char_to_token(i, start_idx)])\n",
        "        end_positions.append([encodings.char_to_token(i, end_idx-1)])\n",
        "    \n",
        "    if start_positions and end_positions:\n",
        "      encodings.update({'start_positions': start_positions,\n",
        "                        'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "train_tf_dataset = train_tf_dataset.map(convert_to_tf_features, batched=True)\n",
        "\n",
        "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
        "train_tf_dataset.set_format(type='tensorflow', columns=columns)\n",
        "features = {x: train_tf_dataset[x] for x in columns[:3]} \n",
        "labels = {\"output_1\": train_tf_dataset[\"start_positions\"]}\n",
        "labels[\"output_2\"] = train_tf_dataset[\"end_positions\"]\n",
        "tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:nlp.load:Checking /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py for additional imports.\n",
            "INFO:filelock:Lock 140291205031360 acquired on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad\n",
            "INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670\n",
            "INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.py\n",
            "INFO:nlp.load:Found checksums file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/urls_checksums/checksums.txt to /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/urls_checksums/checksums.txt\n",
            "INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/squad/squad.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/squad/c0327553d80335e3a3283527f64d9778df7ad04ab28f38148d072782712bb670/squad.json\n",
            "INFO:filelock:Lock 140291205031360 released on /root/.cache/huggingface/datasets/ee43d2be6898ebb9c2afefda4455306911d308bcf924d21c975796832cc7c114.f373b0de1570ca81b50bb03bd371604f7979e35de2cfcf2a3b4521d0b3104d9b.py.lock\n",
            "INFO:nlp.builder:No config specified, defaulting to first: squad/plain_text\n",
            "INFO:nlp.builder:Overwrite dataset info from restored data version.\n",
            "INFO:nlp.info:Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:nlp.builder:Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0)\n",
            "INFO:nlp.builder:Constructing Dataset for split train, from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:nlp.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-9b8a4849358a2045b27513d017ea0dbe.arrow\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:39<00:00,  2.24it/s]\n",
            "INFO:nlp.arrow_writer:Done writing 87599 examples in 1099483906 bytes /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/cache-9b8a4849358a2045b27513d017ea0dbe.arrow.\n",
            "INFO:nlp.arrow_dataset:Set __getitem__(key) output type to tensorflow and filter ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'] columns  (when key is int or slice).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0dfw8K8vSV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's load a pretrained TF2 Bert model and a simple optimizer\n",
        "from transformers import TFBertForQuestionAnswering\n",
        "\n",
        "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-cased\")\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "model.compile(optimizer=opt,\n",
        "              loss={'output_1': loss_fn, 'output_2': loss_fn},\n",
        "              loss_weights={'output_1': 1., 'output_2': 1.},\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcYtiykmvSV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's train our model\n",
        "\n",
        "model.fit(tfdataset, epochs=1, steps_per_epoch=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eREDXWP6vSV8",
        "colab_type": "text"
      },
      "source": [
        "# Metrics API\n",
        "\n",
        "`nlp` also provides easy access and sharing of metrics.\n",
        "\n",
        "This aspect of the library is still experimental and the API may still evolve more than the datasets API.\n",
        "\n",
        "Like datasets, metrics are added as small scripts wrapping common metrics in a common API.\n",
        "\n",
        "There are several reason you may want to use metrics with `nlp` and in particular:\n",
        "\n",
        "- metrics for specific datasets like GLUE or SQuAD are provided out-of-the-box in a simple, convenient and consistant way integrated with the dataset,\n",
        "- metrics in `nlp` leverage the powerful backend to provide smart features out-of-the-box like support for distributed evaluation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUoGMMVKvSV8",
        "colab_type": "text"
      },
      "source": [
        "## Using metrics\n",
        "\n",
        "Using metrics is pretty simple, they have two main methods: `.compute(predictions, references)` to directly compute the metric and `.add(predictions, references)` to only store some results if you want to do the evaluation in one go at the end.\n",
        "\n",
        "Here is a quick gist of a standard use of metrics (the simplest usage):\n",
        "```python\n",
        "import nlp\n",
        "bleu_metric = nlp.load_metric('bleu')\n",
        "\n",
        "# If you only have a single iteration, you can easily compute the score like this\n",
        "predictions = model(inputs)\n",
        "score = bleu_metric.compute(predictions, references)\n",
        "\n",
        "# If you have a loop, you can \"add\" your predictions and references at each iteration instead of having to save them yourself (the metric object store them efficiently for you)\n",
        "for batch in dataloader:\n",
        "    model_input, targets = batch\n",
        "    predictions = model(model_inputs)\n",
        "    bleu.add(predictions, targets)\n",
        "score = bleu_metric.compute()  # Compute the score from all the stored predictions/references\n",
        "```\n",
        "\n",
        "Here is a quick gist of a use in a distributed torch setup (should work for any python multi-process setup actually). It's pretty much identical to the second example above:\n",
        "```python\n",
        "import nlp\n",
        "# You need to give the total number of parallel python processes (num_process) and the id of each process (process_id)\n",
        "bleu = nlp.load_metric('bleu', process_id=torch.distributed.get_rank(),b num_process=torch.distributed.get_world_size())\n",
        "\n",
        "for batch in dataloader:\n",
        "    model_input, targets = batch\n",
        "    predictions = model(model_inputs)\n",
        "    bleu.add(predictions, targets)\n",
        "score = bleu_metric.compute()  # Compute the score on the first node by default (can be set to compute on each node as well)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySL-vDadvSV8",
        "colab_type": "text"
      },
      "source": [
        "Example with a NER metric: `seqeval`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4uZym7MvSV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ner_metric = nlp.load_metric('seqeval')\n",
        "references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "predictions =  [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "ner_metric.compute(predictions, references)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctY6AIAilLdH",
        "colab_type": "text"
      },
      "source": [
        "# Adding a new dataset or a new metric\n",
        "\n",
        "They are two ways to add new datasets and metrics in `nlp`:\n",
        "\n",
        "- datasets can be added with a Pull-Request adding a script in the `datasets` folder of the [`nlp` repository](https://github.com/huggingface/nlp)\n",
        "\n",
        "=> once the PR is merged, the dataset can be instantiate by it's folder name e.g. `nlp.load_dataset('squad')`. If you want HuggingFace to host the data as well you will need to ask the HuggingFace team to upload the data.\n",
        "\n",
        "- datasets can also be added with a direct upload using `nlp` CLI as a user or organization (like for models in `transformers`). In this case the dataset will be accessible under the gien user/organization name, e.g. `nlp.load_dataset('thomwolf/squad')`. In this case you can upload the data yourself at the same time and in the same folder.\n",
        "\n",
        "We will add a full tutorial on how to add and upload datasets soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypLjbtGrljk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
