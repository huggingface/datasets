{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load [CheckList](https://github.com/marcotcr/checklist) suites as datasets and run additional models on them for comparison.  \n",
    "This notebook has examples for running the checklists in the [CheckList paper](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "from nlp import checklist as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/sentiment_checklist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = cl.CheckListSuite(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CheckListSuite` is a class with additional helper functions for running the CheckList.  \n",
    "The data itself is in `suite.dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'tweet': Value(dtype='string', id=None), 'test_name': Value(dtype='string', id=None), 'test_case': Value(dtype='int32', id=None), 'example_idx': Value(dtype='int32', id=None)}, num_rows: 87470)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the CheckList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the CheckList, you have to add predictions to `suite.dataset`, similar to the examples in [here](https://huggingface.co/nlp/processing.html#processing-data-row-by-row).  \n",
    "Here, we will load the predictions used in the CheckList paper, available [here](https://github.com/marcotcr/checklist/raw/master/release_data.tar.gz).  \n",
    "Some CheckLists also require confidence scores. We can check what the right format is by looking at the suite description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CheckList for three-way sentiment analysis (negative, neutral, positive).\n",
      "Predictions: should be integers, where:\n",
      "  - 0: negative\n",
      "  - 1: neutral\n",
      "  - 2: positive\n",
      "Confidences: should be list(float) of length 3, with prediction probabilities\n",
      "for negative, neutral and positive (respectively)\n",
      "\n",
      "Test names for Table 1 in the paper:\n",
      "['neutral words in context', 'Sentiment-laden words in context', 'change neutral words with BERT', 'add positive phrases', 'add negative phrases', 'add random urls and handles', 'typos', 'change locations', 'change names', 'used to, but now', 'simple negations: not negative', 'simple negations: not neutral is still neutral', 'simple negations: I thought x was negative, but it was not (should be neutral or positive)', 'Hard: Negation of positive with neutral stuff in the middle (should be negative)', 'my opinion is what matters', 'Q & A: yes', 'Q & A: no']\n",
      "\n",
      "Use with nlp.checklist.CheckListSuite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(suite.dataset.info.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the predictions and confidences and add them to separate keys in `suite.dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft predictions\n",
      "Loading google predictions\n",
      "Loading amazon predictions\n",
      "Loading bert predictions\n",
      "Loading roberta predictions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "prediction_path = '/home/marcotcr/work/checklist/release_data/sentiment/predictions/'\n",
    "models = ['microsoft', 'google', 'amazon', 'bert', 'roberta']\n",
    "for model in models:\n",
    "    print('Loading %s predictions' % model)\n",
    "    preds = open(os.path.join(prediction_path, model)).read().splitlines()\n",
    "    confs = [list(map(float, (x.split()[1:]))) for x in preds]\n",
    "    preds = [int(x.split()[0]) for x in preds]\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.dataset = suite.dataset.map(lambda _, idx: {model: preds[idx], conf_key: confs[idx]}, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the predictions, we call `suite.compute` to compute test results for each model.  \n",
    "The arguments are the prediction_key and the confidence_key.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for microsoft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a126b95f2ac44348bb330abe55254ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for google\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758bd965d1b34264a962d0deb079a2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for amazon\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214487d34c924c919d241479c7ef9590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4136aed0d5cd48e59c3a4229d2243d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for roberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a6ebe633f74fc998ff7b8af1572bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print('Running tests for %s' % model)\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.compute(model, conf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for every test example is saved under the key `fail`, which returns a dictionary with each model. For example, here is a test example where we expect the prediction to be neutral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an Australian customer service.\n",
      "amazon       Prediction: Neutral    Failed test: No\n",
      "bert         Prediction: Positive   Failed test: Yes\n",
      "google       Prediction: Neutral    Failed test: No\n",
      "microsoft    Prediction: Neutral    Failed test: No\n",
      "roberta      Prediction: Negative   Failed test: Yes\n"
     ]
    }
   ],
   "source": [
    "example = suite.dataset[800]\n",
    "print(example['tweet'])\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "for model in example['fail']:\n",
    "    print('%-12s Prediction: %-10s Failed test: %s' % (model, labels[example[model]], 'Yes' if example['fail'][model] else 'No' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The failure rates for each model are saved in `suite.fail_rate`, indexed by test name.  \n",
    "There are many tests in this suite, but let's say we wanted to replicate Table 1 in the [checklist paper](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_table1 =  [\n",
    "'neutral words in context',\n",
    "'Sentiment-laden words in context',\n",
    "'change neutral words with BERT',\n",
    "'add positive phrases',\n",
    "'add negative phrases',\n",
    "'add random urls and handles',\n",
    "'typos',\n",
    "'change locations',\n",
    "'change names',\n",
    "'used to, but now',\n",
    "'simple negations: not negative',\n",
    "'simple negations: not neutral is still neutral',\n",
    "'simple negations: I thought x was negative, but it was not (should be neutral or positive)',\n",
    "'Hard: Negation of positive with neutral stuff in the middle (should be negative)',\n",
    "'my opinion is what matters',\n",
    "'Q & A: yes',\n",
    "'Q & A: no',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro googl amazo bert rober\n",
      "  0.0   7.6   4.8  94.6  81.8 neutral words in context\n",
      "  4.0  15.0   2.8   0.0   0.2 Sentiment-laden words in context\n",
      "  9.4  16.2  12.4  10.2  10.2 change neutral words with BERT\n",
      " 12.6  12.4   1.4   0.2  10.2 add positive phrases\n",
      "  0.8  34.6   5.0   0.0  13.2 add negative phrases\n",
      "  9.6  13.4  24.8  11.4   7.4 add random urls and handles\n",
      "  5.6  10.2  10.4   5.2   3.8 typos\n",
      "  7.0  20.8  14.8   7.6   6.4 change locations\n",
      "  2.4  15.1   9.1   6.6   2.4 change names\n",
      " 41.0  36.6  42.2  18.8  11.0 used to, but now\n",
      " 18.8  54.2  29.4  13.2   2.6 simple negations: not negative\n",
      " 40.4  39.6  74.2  98.4  95.4 simple negations: not neutral is still neutral\n",
      "100.0  90.4 100.0  84.8   7.2 simple negations: I thought x was negative, but it was not (should be neutral or positive)\n",
      " 98.4 100.0 100.0  74.0  30.2 Hard: Negation of positive with neutral stuff in the middle (should be negative)\n",
      " 45.4  62.4  68.0  38.8  30.0 my opinion is what matters\n",
      "  9.0  57.6  20.8   3.6   3.0 Q & A: yes\n",
      " 96.8  90.8  81.6  55.4  54.8 Q & A: no\n"
     ]
    }
   ],
   "source": [
    "print (' '.join([x[:5] for x in models]))\n",
    "for t in checklist_table1:\n",
    "    r = ' '.join(['%5.1f' % (suite.fail_rate[m][t]) for m in models])\n",
    "    print('%s %s' % (r, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want to compare the pipeline in transformers to these models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(\"sentiment-analysis\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test suite assumes the labels are [negative, neutral, positive], so we have to do some converting from binary sentiment to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pred_and_conf(data):\n",
    "    # change format to softmax, make everything in [0.33, 0.66] range be predicted as neutral\n",
    "    preds = model(data)\n",
    "    pr = np.array([x['score'] if x['label'] == 'POSITIVE' else 1 - x['score'] for x in preds])\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add predictions to `suite.dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd1d5ce88cc45cda8cbfb07e0b27bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_pipeline(x):\n",
    "    preds, confs = pred_and_conf(x['tweet'])\n",
    "    return {'hf_pipeline': preds, 'hf_pipeline_conf': confs}\n",
    "suite.dataset = suite.dataset.map(add_pipeline , batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1565f720a7944403849ee0358f093651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suite.compute('hf_pipeline', 'hf_pipeline_conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append('hf_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro googl amazo bert rober hf_pi\n",
      "  0.0   7.6   4.8  94.6  81.8  95.8 neutral words in context\n",
      "  4.0  15.0   2.8   0.0   0.2   0.8 Sentiment-laden words in context\n",
      "  9.4  16.2  12.4  10.2  10.2   9.8 change neutral words with BERT\n",
      " 12.6  12.4   1.4   0.2  10.2   0.0 add positive phrases\n",
      "  0.8  34.6   5.0   0.0  13.2   6.8 add negative phrases\n",
      "  9.6  13.4  24.8  11.4   7.4  15.4 add random urls and handles\n",
      "  5.6  10.2  10.4   5.2   3.8   6.6 typos\n",
      "  7.0  20.8  14.8   7.6   6.4  10.0 change locations\n",
      "  2.4  15.1   9.1   6.6   2.4   5.1 change names\n",
      " 41.0  36.6  42.2  18.8  11.0  32.6 used to, but now\n",
      " 18.8  54.2  29.4  13.2   2.6  12.8 simple negations: not negative\n",
      " 40.4  39.6  74.2  98.4  95.4  97.4 simple negations: not neutral is still neutral\n",
      "100.0  90.4 100.0  84.8   7.2 100.0 simple negations: I thought x was negative, but it was not (should be neutral or positive)\n",
      " 98.4 100.0 100.0  74.0  30.2  86.8 Hard: Negation of positive with neutral stuff in the middle (should be negative)\n",
      " 45.4  62.4  68.0  38.8  30.0  44.2 my opinion is what matters\n",
      "  9.0  57.6  20.8   3.6   3.0   0.8 Q & A: yes\n",
      " 96.8  90.8  81.6  55.4  54.8  85.8 Q & A: no\n"
     ]
    }
   ],
   "source": [
    "print (' '.join([x[:5] for x in models]))\n",
    "for t in checklist_table1:\n",
    "    r = ' '.join(['%5.1f' % (suite.fail_rate[m][t]) for m in models])\n",
    "    print('%s %s' % (r, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using marcotcr/checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can access the suite object from [`marcotcr/checklist`](https://github.com/marcotcr/checklist) if they want to use it (e.g. for visualizations), with the caveat that it doesn't really allow for model comparison (it only keeps the state of the last model we called `compute` on).  \n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checklist.test_suite.TestSuite"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(suite.suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      8658\n",
      "Test cases run:  500\n",
      "Fails (rate):    4 (0.8%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 This food was average.\n",
      "----\n",
      "0.0 0.0 1.0 The flight is average.\n",
      "----\n",
      "0.0 0.0 1.0 That was a weird food.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "suite.suite.tests['Sentiment-laden words in context'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "single positive words\n",
      "Test cases:      34\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "single negative words\n",
      "Test cases:      35\n",
      "Fails (rate):    1 (2.9%)\n",
      "\n",
      "Example fails:\n",
      "0.3 0.0 0.7 average\n",
      "----\n",
      "\n",
      "\n",
      "single neutral words\n",
      "Test cases:      13\n",
      "Fails (rate):    13 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 see\n",
      "----\n",
      "1.0 0.0 0.0 commercial\n",
      "----\n",
      "0.0 0.0 1.0 private\n",
      "----\n",
      "\n",
      "\n",
      "Sentiment-laden words in context\n",
      "Test cases:      8658\n",
      "Test cases run:  500\n",
      "Fails (rate):    4 (0.8%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 That was a weird food.\n",
      "----\n",
      "0.1 0.0 0.9 This was an average staff.\n",
      "----\n",
      "0.1 0.0 0.9 This food was average.\n",
      "----\n",
      "\n",
      "\n",
      "neutral words in context\n",
      "Test cases:      1716\n",
      "Test cases run:  500\n",
      "Fails (rate):    479 (95.8%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 The crew was Israeli.\n",
      "----\n",
      "1.0 0.0 0.0 The staff was private.\n",
      "----\n",
      "0.0 0.0 1.0 That is an Australian service.\n",
      "----\n",
      "\n",
      "\n",
      "intensifiers\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "After filtering: 496 (99.2%)\n",
      "Fails (rate):    8 (1.6%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.0 0.1 This is a creepy service.\n",
      "0.0 0.0 1.0 This is an amazingly creepy service.\n",
      "\n",
      "----\n",
      "0.9 0.0 0.1 This was a creepy service.\n",
      "0.2 0.0 0.8 This was a truly creepy service.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 That was a weird plane.\n",
      "0.1 0.0 0.9 That was an amazingly weird plane.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "reducers\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "After filtering: 2 (0.4%)\n",
      "Fails (rate):    1 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 0.0 0.2 This plane was average.\n",
      "1.0 0.0 0.0 This plane was mostly average.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change neutral words with BERT\n",
      "Test cases:      500\n",
      "Fails (rate):    49 (9.8%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 @united :take note of this great example of @JetBlue actually making good for an extremely inconvenient situation. http://t.co/t3Gnk2N7LD\n",
      "0.0 0.5 0.5 @united :take note of this great example of @JetBlue actually making good for this extremely inconvenient situation. http://t.co/t3Gnk2N7LD\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 @united I can't even look at you right now. I'll call you when I'm not so mad.\n",
      "0.3 0.7 0.0 @united I can't even look at them right now. I'll call them when I'm not so mad.\n",
      "0.3 0.7 0.0 @united I can't even look at them right now. I'll call them when I'm not so mad.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @AmericanAir it looks like a mistake\n",
      "0.0 0.0 1.0 @AmericanAir never looks like a mistake\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "add positive phrases\n",
      "Test cases:      500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "add negative phrases\n",
      "Test cases:      500\n",
      "Fails (rate):    34 (6.8%)\n",
      "\n",
      "Example fails:\n",
      "0.3 0.0 0.7 @united done as requested.\n",
      "0.0 0.0 1.0 @united done as requested. I abhor you.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @JetBlue wondering if it's possible for my colleague and I to get on an earlier flight LAX&gt;JFK tomorrow. Can you help?\n",
      "0.7 0.0 0.3 @JetBlue wondering if it's possible for my colleague and I to get on an earlier flight LAX&gt;JFK tomorrow. Can you help. You are average.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 @JetBlue I've spent three hours talking to all different people, most of which couldn't help. I probably won't be using JetBlue again.\n",
      "0.0 0.7 0.3 @JetBlue I've spent three hours talking to all different people, most of which couldn't help. I probably won't be using JetBlue again. I abhor you.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "add random urls and handles\n",
      "Test cases:      500\n",
      "Fails (rate):    77 (15.4%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 @united flight 435\n",
      "0.1 0.9 0.0 @united flight 435 @FIOm65\n",
      "0.3 0.7 0.0 @united flight 435 @zVD2X4\n",
      "\n",
      "----\n",
      "0.3 0.7 0.0 @SouthwestAir thx - fingers crossed they are found.\n",
      "0.9 0.0 0.1 @hjMgnL @SouthwestAir thx - fingers crossed they are found.\n",
      "0.9 0.0 0.1 @SouthwestAir thx - fingers crossed they are found. @J9Z8Jd\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 @JetBlue dispatcher keeps yelling and hung up on me!\n",
      "1.0 0.0 0.0 https://t.co/zPtctr @JetBlue dispatcher keeps yelling and hung up on me!\n",
      "1.0 0.0 0.0 https://t.co/MU2qEf @JetBlue dispatcher keeps yelling and hung up on me!\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "punctuation\n",
      "Test cases:      500\n",
      "Fails (rate):    27 (5.4%)\n",
      "\n",
      "Example fails:\n",
      "0.2 0.0 0.8 @JetBlue 1951 BOS to ORD\n",
      "0.0 0.9 0.1 @JetBlue 1951 BOS to ORD.\n",
      "\n",
      "----\n",
      "0.3 0.0 0.7 @united I Want a Plane dad\n",
      "0.9 0.0 0.1 @united I Want a Plane dad.\n",
      "\n",
      "----\n",
      "0.2 0.8 0.0 @SouthwestAir it's just the principle - it's hard to get mugged &amp; not be upset you took my money &amp; didn't give me anything #wrongiswrong\n",
      "0.9 0.0 0.1 @SouthwestAir it's just the principle - it's hard to get mugged &amp; not be upset you took my money &amp; didn't give me anything #\n",
      "0.9 0.0 0.1 @SouthwestAir it's just the principle - it's hard to get mugged &amp; not be upset you took my money &amp; didn't give me anything #.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "typos\n",
      "Test cases:      500\n",
      "Fails (rate):    33 (6.6%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 @AmericanAir dm me your email address,  I will tell you my ordeal, then you tell me what you think is fair\n",
      "0.9 0.0 0.1 @AmericanAir dm me your email address,  I will tel lyou my ordeal, then you tell me what you think is fair\n",
      "\n",
      "----\n",
      "0.9 0.0 0.1 @united I already sent feedback and your reps are who told me about the priority. I'm speechless at your system\n",
      "0.0 0.0 1.0 @united  Ialready sent feedback and your reps are who told me about the priority. I'm speechless at your system\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 @united thanks, just sent :)\n",
      "0.9 0.0 0.1 @unite dthanks, just sent :)\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "2 typos\n",
      "Test cases:      500\n",
      "Fails (rate):    58 (11.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 @JetBlue figured but delay is listed at almost 2 hrs. Thanks for being great! Cheers\n",
      "0.9 0.0 0.1 @JetBlue figured but delay is liste dat almost 2 hrs. Thanks for being great! hCeers\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 @united A 3rd party service you use to sell your tickets has ripped me off. Excellent service United is offering.\n",
      "1.0 0.0 0.0 @united A 3rd party service you use to sell your tickets has ripped me off. Excelelnt servic eUnited is offering.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 @JetBlue 290 to Boston\n",
      "1.0 0.0 0.0 @JetBlue 209 t oBoston\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "contractions\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    21 (4.2%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.5 0.5 @americanair I sure do. I'm running version 3.10.0\n",
      "0.2 0.0 0.8 @americanair I sure do. I am running version 3.10.0\n",
      "\n",
      "----\n",
      "0.0 0.8 0.2 @AmericanAir Didn't really need anything. Saw your mentions are often negative. Keep up the good work. #gratitude  🇺🇸✈️\n",
      "0.9 0.0 0.1 @AmericanAir Did not really need anything. Saw your mentions are often negative. Keep up the good work. #gratitude  🇺🇸✈️\n",
      "\n",
      "----\n",
      "0.0 0.8 0.2 @SouthwestAir everything OK? This is my 3rd call for the day and this time I've been on hold for 1.5 hrs. I'll hang up and try again.\n",
      "0.2 0.0 0.8 @SouthwestAir everything OK? This is my 3rd call for the day and this time I have been on hold for 1.5 hrs. I will hang up and try again.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "change names\n",
      "Test cases:      331\n",
      "Fails (rate):    17 (5.1%)\n",
      "\n",
      "Example fails:\n",
      "0.2 0.0 0.8 @united you have a guy named Otis at ORD that knows what #customerservice is. he was able to get my bag to me. I upgraded just to be sure\n",
      "0.3 0.7 0.0 @united you have a guy named Jesus at ORD that knows what #customerservice is. he was able to get my bag to me. I upgraded just to be sure\n",
      "\n",
      "----\n",
      "0.7 0.0 0.3 @JetBlue The 13th Annual Martha's Vineyard African American Film Festival  August 10-15, 2015   Our attendees deserve a great flight to MV??\n",
      "0.3 0.7 0.0 @JetBlue The 13th Annual Olivia's Vineyard African American Film Festival  August 10-15, 2015   Our attendees deserve a great flight to MV??\n",
      "0.4 0.6 0.0 @JetBlue The 13th Annual Kristen's Vineyard African American Film Festival  August 10-15, 2015   Our attendees deserve a great flight to MV??\n",
      "\n",
      "----\n",
      "0.7 0.0 0.3 @SouthwestAir still haven't left @BWI. Maybe by the time I'm suppose to fly back to Austin on Tuesday we'll have moved.\n",
      "0.4 0.6 0.0 @SouthwestAir still haven't left @BWI. Maybe by the time I'm suppose to fly back to Shawn on Tuesday we'll have moved.\n",
      "0.5 0.5 0.0 @SouthwestAir still haven't left @BWI. Maybe by the time I'm suppose to fly back to Christian on Tuesday we'll have moved.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change locations\n",
      "Test cases:      909\n",
      "Test cases run:  500\n",
      "Fails (rate):    50 (10.0%)\n",
      "\n",
      "Example fails:\n",
      "0.2 0.0 0.8 @USAirways you guys have my luggage in San Jose and were supposed to deliver it to my hotel hours ago!! Please contact me.\n",
      "0.0 1.0 0.0 @USAirways you guys have my luggage in Cuyahoga Falls and were supposed to deliver it to my hotel hours ago!! Please contact me.\n",
      "0.1 0.9 0.0 @USAirways you guys have my luggage in Pflugerville and were supposed to deliver it to my hotel hours ago!! Please contact me.\n",
      "\n",
      "----\n",
      "0.7 0.0 0.3 @united can someone at the airport in Portland, OR buy a ticket to be picked up and used 2days Late Flightr by other from Perth, AUSTRALIA? Thanks\n",
      "0.0 0.7 0.3 @united can someone at the airport in Atlantic City, OR buy a ticket to be picked up and used 2days Late Flightr by other from Perth, AUSTRALIA? Thanks\n",
      "0.0 0.9 0.1 @united can someone at the airport in Marysville, OR buy a ticket to be picked up and used 2days Late Flightr by other from Perth, AUSTRALIA? Thanks\n",
      "\n",
      "----\n",
      "0.1 0.0 0.9 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from San Diego? Pretty please?\n",
      "0.5 0.5 0.0 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from Prescott? Pretty please?\n",
      "0.3 0.7 0.0 @united can you ask your guys with flight 1146 to BWI to wait for us to get off a delayed flight from Waterloo? Pretty please?\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "change numbers\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    14 (2.8%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 @JetBlue It drops 75 when i take off the other two people, and vice versa. back and forth.\n",
      "0.0 0.9 0.1 @JetBlue It drops 80 when i take off the other two people, and vice versa. back and forth.\n",
      "0.0 0.8 0.2 @JetBlue It drops 60 when i take off the other two people, and vice versa. back and forth.\n",
      "\n",
      "----\n",
      "0.3 0.0 0.7 @JetBlue big shoutout to the crews on 2017 Bos&gt;jfk &amp; 486 jfk&gt;roc, &amp; gate crews at c19 Bos &amp; 6 jfk. Long day of delays made better by them!\n",
      "0.2 0.8 0.0 @JetBlue big shoutout to the crews on 1685 Bos&gt;jfk &amp; 486 jfk&gt;roc, &amp; gate crews at c19 Bos &amp; 6 jfk. Long day of delays made better by them!\n",
      "0.3 0.7 0.0 @JetBlue big shoutout to the crews on 1807 Bos&gt;jfk &amp; 486 jfk&gt;roc, &amp; gate crews at c19 Bos &amp; 6 jfk. Long day of delays made better by them!\n",
      "\n",
      "----\n",
      "0.3 0.0 0.7 @AmericanAir flight was 2488 out of EWR STOP AT DALLAS THEN TO LA. I need to be in la tonight!\n",
      "0.0 0.5 0.5 @AmericanAir flight was 2558 out of EWR STOP AT DALLAS THEN TO LA. I need to be in la tonight!\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "protected: race\n",
      "Test cases:      600\n",
      "Test cases run:  500\n",
      "Fails (rate):    341 (68.2%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.0 0.1 Jackson is a black photo.\n",
      "0.0 0.0 1.0 Jackson is an asian photo.\n",
      "0.0 0.0 1.0 Jackson is a hispanic photo.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 Nathaniel is a black accent.\n",
      "0.0 0.0 1.0 Nathaniel is an asian accent.\n",
      "0.0 0.0 1.0 Nathaniel is a hispanic accent.\n",
      "\n",
      "----\n",
      "1.0 0.0 0.0 Jayden is a black country.\n",
      "0.0 0.0 1.0 Jayden is an asian country.\n",
      "0.0 0.0 1.0 Jayden is a hispanic country.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: sexual\n",
      "Test cases:      600\n",
      "Test cases run:  500\n",
      "Fails (rate):    431 (86.2%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Nathaniel is an asexual story.\n",
      "0.4 0.6 0.0 Nathaniel is a lesbian story.\n",
      "1.0 0.0 0.0 Nathaniel is a gay story.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Cameron is an asexual women.\n",
      "0.9 0.0 0.1 Cameron is a gay women.\n",
      "0.9 0.0 0.1 Cameron is a cisgender women.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Elijah is an asexual subject.\n",
      "1.0 0.0 0.0 Elijah is a cisgender subject.\n",
      "1.0 0.0 0.0 Elijah is a gay subject.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: religion\n",
      "Test cases:      600\n",
      "Test cases run:  500\n",
      "Fails (rate):    483 (96.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Melanie is a Christian daughter.\n",
      "1.0 0.0 0.0 Melanie is an Atheist daughter.\n",
      "0.0 0.9 0.1 Melanie is an Agnostic daughter.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Cameron is a Christian figure.\n",
      "0.4 0.6 0.0 Cameron is a Shinto figure.\n",
      "1.0 0.0 0.0 Cameron is an Atheist figure.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Alex is a Christian country.\n",
      "0.3 0.7 0.0 Alex is a Confucian country.\n",
      "0.5 0.5 0.0 Alex is a Shinto country.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "protected: nationality\n",
      "Test cases:      600\n",
      "Test cases run:  500\n",
      "Fails (rate):    57 (11.4%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Mason is a Chinese proverb.\n",
      "0.0 0.9 0.1 Mason is a Bangladeshi proverb.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Kathryn is a Chinese nationalist.\n",
      "0.1 0.9 0.0 Kathryn is a German nationalist.\n",
      "0.0 1.0 0.0 Kathryn is a Bangladeshi nationalist.\n",
      "\n",
      "----\n",
      "0.0 0.0 1.0 Evan is a Chinese activist.\n",
      "0.0 1.0 0.0 Evan is a German activist.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "used to, but now\n",
      "Test cases:      8000\n",
      "Test cases run:  500\n",
      "Fails (rate):    163 (32.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I think this airline is frustrating, but I used to think it was awesome.\n",
      "----\n",
      "0.1 0.0 0.9 In the past I would welcome this airline, although now I despise it.\n",
      "----\n",
      "0.0 0.6 0.4 In the past I would love this airline, although now I regret it.\n",
      "----\n",
      "\n",
      "\n",
      "\"used to\" should reduce\n",
      "Test cases:      4368\n",
      "Test cases run:  500\n",
      "After filtering: 6 (1.2%)\n",
      "Fails (rate):    3 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "0.7 0.0 0.3 it was a creepy service.\n",
      "1.0 0.0 0.0 I used to think it was a creepy service.\n",
      "\n",
      "----\n",
      "0.8 0.0 0.2 it was a creepy cabin crew.\n",
      "1.0 0.0 0.0 I used to think it was a creepy cabin crew.\n",
      "\n",
      "----\n",
      "0.8 0.0 0.2 this was a weird flight.\n",
      "1.0 0.0 0.0 I used to think this was a weird flight.\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "simple negations: negative\n",
      "Test cases:      6318\n",
      "Test cases run:  500\n",
      "Fails (rate):    23 (4.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 1.0 0.0 I would never say I value that aircraft.\n",
      "----\n",
      "0.2 0.0 0.8 I would never say I value that plane.\n",
      "----\n",
      "0.3 0.0 0.7 I can't say I love this cabin crew.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not negative\n",
      "Test cases:      6786\n",
      "Test cases run:  500\n",
      "Fails (rate):    64 (12.8%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.0 0.1 This was not a creepy plane.\n",
      "----\n",
      "0.9 0.0 0.1 That staff isn't average.\n",
      "----\n",
      "0.8 0.0 0.2 This wasn't a creepy staff.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: not neutral is still neutral\n",
      "Test cases:      2496\n",
      "Test cases run:  500\n",
      "Fails (rate):    487 (97.4%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 This wasn't a British company.\n",
      "----\n",
      "1.0 0.0 0.0 I can't say I see that airline.\n",
      "----\n",
      "0.9 0.0 0.1 This aircraft isn't commercial.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: I thought x was positive, but it was not (should be negative)\n",
      "Test cases:      1992\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "simple negations: I thought x was negative, but it was not (should be neutral or positive)\n",
      "Test cases:      2124\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I thought I would regret this crew, but I did not.\n",
      "----\n",
      "1.0 0.0 0.0 I thought this crew would be rough, but it wasn't.\n",
      "----\n",
      "1.0 0.0 0.0 I thought that crew would be annoying, but it wasn't.\n",
      "----\n",
      "\n",
      "\n",
      "simple negations: but it was not (neutral) should still be neutral\n",
      "Test cases:      804\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 I thought this customer service would be Israeli, but it wasn't.\n",
      "----\n",
      "1.0 0.0 0.0 I thought that seat would be international, but it was not.\n",
      "----\n",
      "1.0 0.0 0.0 I thought the seat would be international, but it was not.\n",
      "----\n",
      "\n",
      "\n",
      "Hard: Negation of positive with neutral stuff in the middle (should be negative)\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    434 (86.8%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I wouldn't say, given my history with airplanes, that the is an exceptional pilot.\n",
      "----\n",
      "0.0 0.0 1.0 I can't say, given my history with airplanes, that the was an amazing service.\n",
      "----\n",
      "0.0 0.0 1.0 I can't say, given the time that I've been flying, that this is a sweet service.\n",
      "----\n",
      "\n",
      "\n",
      "Hard: Negation of negative with neutral stuff in the middle (should be positive or neutral)\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    497 (99.4%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 i can't say, given that I am from Brazil, that we dislike the crew.\n",
      "----\n",
      "1.0 0.0 0.0 i don't think, given it's a Tuesday, that the was a lame food.\n",
      "----\n",
      "1.0 0.0 0.0 i wouldn't say, given all that I've seen over the years, that this is a rough seat.\n",
      "----\n",
      "\n",
      "\n",
      "negation of neutral with neutral in the middle, should still neutral\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    477 (95.4%)\n",
      "\n",
      "Example fails:\n",
      "0.1 0.0 0.9 I can't say, given all that I've seen over the years, that this crew is Indian.\n",
      "----\n",
      "0.9 0.0 0.1 I wouldn't say, given it's a Tuesday, that I see that pilot.\n",
      "----\n",
      "0.1 0.0 0.9 I can't say, given all that I've seen over the years, that this plane is Australian.\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "my opinion is what matters\n",
      "Test cases:      8528\n",
      "Test cases run:  500\n",
      "Fails (rate):    221 (44.2%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 I think you are dreadful, but some people think you are adorable.\n",
      "----\n",
      "0.0 0.0 1.0 Some people think you are fantastic, but I think you are creepy.\n",
      "----\n",
      "1.0 0.0 0.0 I like you, but my friends dread you.\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes\n",
      "Test cases:      7644\n",
      "Test cases run:  500\n",
      "Fails (rate):    4 (0.8%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think that customer service was good? Yes\n",
      "----\n",
      "0.0 0.5 0.5 Do I think that company was nice? Yes\n",
      "----\n",
      "0.9 0.0 0.1 Do I think that aircraft was nice? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: yes (neutral)\n",
      "Test cases:      1560\n",
      "Test cases run:  500\n",
      "Fails (rate):    489 (97.8%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think that was a private seat? Yes\n",
      "----\n",
      "1.0 0.0 0.0 Do I think this is a British airline? Yes\n",
      "----\n",
      "1.0 0.0 0.0 Do I think this is a commercial plane? Yes\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no\n",
      "Test cases:      7644\n",
      "Test cases run:  500\n",
      "Fails (rate):    429 (85.8%)\n",
      "\n",
      "Example fails:\n",
      "0.0 0.0 1.0 Do I think that is an excellent seat? No\n",
      "----\n",
      "1.0 0.0 0.0 Did I dread that airline? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think this is a bad flight? No\n",
      "----\n",
      "\n",
      "\n",
      "Q & A: no (neutral)\n",
      "Test cases:      1560\n",
      "Test cases run:  500\n",
      "Fails (rate):    499 (99.8%)\n",
      "\n",
      "Example fails:\n",
      "1.0 0.0 0.0 Do I think this was an Italian food? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think that is an Australian airline? No\n",
      "----\n",
      "1.0 0.0 0.0 Do I think this service is commercial? No\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.suite.summary() # will only display results for the last model we called 'compute' on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440e2b65d7a44c1383aa4a8f05c6ca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'single positive word…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.suite.visual_summary_table() # displays a visualization of the whole table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suite has a few additional functions to help slicing. For example, if you want to see example instances from certain test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987a90fc563d4a5db0c3dce06246d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "simple = suite.get_test('Sentiment-laden words in context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I valued the flight.',\n",
       " 'That is a sad customer service.',\n",
       " 'We like the flight.',\n",
       " 'This was a nice crew.',\n",
       " 'We abhor that flight.',\n",
       " 'This staff is difficult.',\n",
       " 'I valued that aircraft.',\n",
       " 'This was a fantastic flight.',\n",
       " 'I hate the food.',\n",
       " 'I despised the food.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple['tweet'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can use this as we would any `nlp.dataset`. \n",
    "For example, let's filter by examples where google fails and the huggingface pipeline does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b86d2ebba430a89854f0202464632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "google_fails_hf_doesnt = simple.filter(lambda x:x['fail']['hf_pipeline'] == 0 and x['fail']['google'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This seat was lame.            HF:Negative Google:Neutral \n",
      "I dread that service.          HF:Negative Google:Neutral \n",
      "That was a hard pilot.         HF:Negative Google:Neutral \n",
      "The seat is hard.              HF:Negative Google:Neutral \n",
      "That is a sad airline.         HF:Negative Google:Neutral \n"
     ]
    }
   ],
   "source": [
    "for x in np.random.choice(google_fails_hf_doesnt.shape[0], 5):\n",
    "    x = google_fails_hf_doesnt[int(x)]\n",
    "    print('%-30s HF:%-8s Google:%-8s' % (x['tweet'], labels[x['hf_pipeline']], labels[x['google']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbation tests combine multiple examples, so looking at a single row in the dataset would not give us a good picture.  \n",
    "Instead, we want to aggregate each testcase into a row of examples (data goes into the `data` key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3c597a93804a56a9e202a93e8bd336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "perturbation = suite.get_test('change locations', aggregate_testcases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be25745b026a42c4a5aa8eaffee88bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "any_fails = lambda x, key: any([y['fail'][key] for y in x['data']])\n",
    "google_fails_hf_doesnt = perturbation.filter(lambda x: not any_fails(x, 'hf_pipeline') and any_fails(x, 'google'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an INV test, so a failure is a change in prediction. Some examples where HF maintains the invariance after a location change, while Google changes its prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF:Negative Google:Negative | @united you better hold my flight to Tucson #5237, just landed in Houston after an hour delay for some minor computer problem\n",
      "HF:Negative Google:Neutral  | @united you better hold my flight to Santa Maria #5237, just landed in Houston after an hour delay for some minor computer problem\n",
      "\n",
      "\n",
      "HF:Negative Google:Neutral  | @USAirways JUST LANDED flight 545. Any chance of making flight 5530 Phoenix to AUS\n",
      "HF:Negative Google:Positive | @USAirways JUST LANDED flight 545. Any chance of making flight 5530 Newport Beach to AUS\n",
      "\n",
      "\n",
      "HF:Positive Google:Neutral  | @SouthwestAir thanks for the ride to Chicago. #kmdw #b738 http://t.co/6cpYPGFnD6\n",
      "HF:Positive Google:Positive | @SouthwestAir thanks for the ride to Palm Beach Gardens. #kmdw #b738 http://t.co/6cpYPGFnD6\n",
      "\n",
      "\n",
      "HF:Negative Google:Neutral  | @SouthwestAir could u put one here in Baltimore? http://t.co/vLCI2KV1IP\n",
      "HF:Negative Google:Positive | @SouthwestAir could u put one here in Bell Gardens? http://t.co/vLCI2KV1IP\n",
      "\n",
      "\n",
      "HF:Negative Google:Neutral  | @USAirways JUST LANDED flight 545. Any chance of making flight 5530 Phoenix to AUS\n",
      "HF:Negative Google:Positive | @USAirways JUST LANDED flight 545. Any chance of making flight 5530 Newport Beach to AUS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapz = ['negative', 'neutral', 'positive']\n",
    "for x in np.random.choice(google_fails_hf_doesnt.shape[0], 5):\n",
    "    x = google_fails_hf_doesnt[int(x)]\n",
    "    orig = x['data'][0]\n",
    "    fail = [y for y in x['data'] if y['fail']['google']][0]\n",
    "    print('HF:%-8s Google:%-8s | %s' % (labels[orig['hf_pipeline']], labels[orig['google']], orig['tweet']))\n",
    "    print('HF:%-8s Google:%-8s | %s' % (labels[fail['hf_pipeline']], labels[fail['google']], fail['tweet']))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pair (QQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for QQP and SQuAD is the same as for sentiment, but we will run through them for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/qqp_checklist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = cl.CheckListSuite(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CheckList for Quora Question Pair.\n",
      "Predictions: should be integers, where:\n",
      "  - 0: non-duplicate\n",
      "  - 1: duplicate\n",
      "Confidences: should be list(float) of length 2, with prediction probabilities\n",
      "for non-duplicate and duplicate(respectively)\n",
      "\n",
      "Test names for Table 2 in the paper:\n",
      "['Modifier: adj',  'How can I become more {synonym}?', 'Replace synonyms in real pairs', 'How can I become more X = How can I become less antonym(X)', 'add one typo', '(q, paraphrase(q))',  'Change same name in both questions',  'Change first and last name in one of the questions', 'Keep entitites, fill in with gibberish', 'Is person X != Did person use to be X',   'Is it {ok, dangerous, ...} to {smoke, rest, ...} after != before', \"What was person's life before becoming X != What was person's life after becoming X\", 'How can I become a X person != How can I become a person who is not X', 'How can I become a X person == How can I become a person who is not antonym(X)', 'Simple coref: he and she', 'Simple coref: his and her',  'Order does not matter for comparison', 'Order does not matter for symmetric relations', 'Order does matter for asymmetric relations',  'traditional SRL: active / passive swap with people', 'traditional SRL: wrong active / passive swap with people', 'Symmetry: f(a, b) = f(b, a)', 'Testing implications']\n",
      "\n",
      "Use with nlp.checklist.CheckListSuite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(suite.dataset.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading predictions and confidences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert', 'roberta']\n",
    "prediction_path = '/home/marcotcr/work/checklist/release_data/qqp/predictions/'\n",
    "for model in models:\n",
    "    preds = list(map(float, open(os.path.join(prediction_path, model)).read().splitlines()))\n",
    "    confs = [[1 - x, x] for x in preds]\n",
    "    preds = [int(x >= 0.5) for x in preds]\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.dataset = suite.dataset.map(lambda _, idx: {model: preds[idx], conf_key: confs[idx]}, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9abd8d58a224a01bbca779855271837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113985.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99dded927cb41c2930a989e285d680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113985.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.compute(model, conf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_table2 = ['Modifier: adj',  'How can I become more {synonym}?', 'Replace synonyms in real pairs', 'How can I become more X = How can I become less antonym(X)', 'add one typo', '(q, paraphrase(q))',  'Change same name in both questions',  'Change first and last name in one of the questions', 'Keep entitites, fill in with gibberish', 'Is person X != Did person use to be X',   'Is it {ok, dangerous, ...} to {smoke, rest, ...} after != before', \"What was person's life before becoming X != What was person's life after becoming X\", 'How can I become a X person != How can I become a person who is not X', 'How can I become a X person == How can I become a person who is not antonym(X)', 'Simple coref: he and she', 'Simple coref: his and her',  'Order does not matter for comparison', 'Order does not matter for symmetric relations', 'Order does matter for asymmetric relations',  'traditional SRL: active / passive swap with people', 'traditional SRL: wrong active / passive swap with people', 'Symmetry: f(a, b) = f(b, a)', 'Testing implications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert roberta\n",
      " 78.4  78.0 Modifier: adj\n",
      " 22.8  39.2 How can I become more {synonym}?\n",
      " 13.1  12.7 Replace synonyms in real pairs\n",
      " 69.4 100.0 How can I become more X = How can I become less antonym(X)\n",
      " 18.2  12.0 add one typo\n",
      " 69.0  25.0 (q, paraphrase(q))\n",
      " 11.8   9.4 Change same name in both questions\n",
      " 35.1  30.1 Change first and last name in one of the questions\n",
      " 30.0  32.8 Keep entitites, fill in with gibberish\n",
      " 61.8  96.8 Is person X != Did person use to be X\n",
      " 98.0  34.4 Is it {ok, dangerous, ...} to {smoke, rest, ...} after != before\n",
      "100.0   0.0 What was person's life before becoming X != What was person's life after becoming X\n",
      " 18.6   0.0 How can I become a X person != How can I become a person who is not X\n",
      " 81.6  88.6 How can I become a X person == How can I become a person who is not antonym(X)\n",
      " 79.0  96.6 Simple coref: he and she\n",
      " 99.6 100.0 Simple coref: his and her\n",
      " 99.6 100.0 Order does not matter for comparison\n",
      " 81.8 100.0 Order does not matter for symmetric relations\n",
      " 71.4 100.0 Order does matter for asymmetric relations\n",
      " 65.8  98.6 traditional SRL: active / passive swap with people\n",
      " 97.4 100.0 traditional SRL: wrong active / passive swap with people\n",
      "  4.4   2.2 Symmetry: f(a, b) = f(b, a)\n",
      "  9.7   8.5 Testing implications\n"
     ]
    }
   ],
   "source": [
    "print (' '.join([x for x in models]))\n",
    "for t in checklist_table2:\n",
    "    r = ' '.join(['%5.1f' % (suite.fail_rate[m][t]) for m in models])\n",
    "    print('%s %s' % (r, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary with examples (for RoBERTa only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "Modifier: adj\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    390 (78.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Is Kayla Bennett an economist?', 'Is Kayla Bennett an average economist?')\n",
      "----\n",
      "0.8 ('Is Laura Morales an educator?', 'Is Laura Morales an acomplished educator?')\n",
      "----\n",
      "0.9 ('Is Kimberly Hill a historian?', 'Is Kimberly Hill an accredited historian?')\n",
      "----\n",
      "\n",
      "\n",
      "different adjectives\n",
      "Test cases:      954\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Different animals\n",
      "Test cases:      928\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Irrelevant modifiers - animals\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Irrelevant modifiers - people\n",
      "Test cases:      987\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Irrelevant preamble with different examples.\n",
      "Test cases:      938\n",
      "Test cases run:  500\n",
      "Fails (rate):    498 (99.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('My pet fish eats rice. Is it normal for animals to eat rice?', 'My pet rat eats rice. Is it normal for animals to eat rice?')\n",
      "----\n",
      "0.0 ('My pet owl eats butter. Is it normal for animals to eat butter?', 'My pet dog eats butter. Is it normal for animals to eat butter?')\n",
      "----\n",
      "0.0 ('My pet owl eats sugar. Is it normal for animals to eat sugar?', 'My pet chicken eats sugar. Is it normal for animals to eat sugar?')\n",
      "----\n",
      "\n",
      "\n",
      "Preamble is relevant (different injuries)\n",
      "Test cases:      975\n",
      "Test cases run:  500\n",
      "Fails (rate):    8 (1.6%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('I hurt my forearm last time I played golf. Is this going to impact my performance?', 'I hurt my wrist last time I played golf. Is this going to impact my performance?')\n",
      "----\n",
      "0.8 ('I hurt my wrist last time I played golf. Should I never play again?', 'I hurt my hand last time I played golf. Should I never play again?')\n",
      "----\n",
      "0.7 ('I hurt my leg last time I played soccer. Should I never play again?', 'I hurt my thigh last time I played soccer. Should I never play again?')\n",
      "----\n",
      "\n",
      "\n",
      "How can I become more X != How can I become less X\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taxonomy\n",
      "\n",
      "How can I become more {synonym}?\n",
      "Test cases:      6000\n",
      "Test cases run:  500\n",
      "Fails (rate):    196 (39.2%)\n",
      "\n",
      "Example fails:\n",
      "0.1 ('How can I become more furious?', 'How can I become more angry?')\n",
      "----\n",
      "0.0 ('How can I become an outspoken person?', 'How can I become a vocal person?')\n",
      "----\n",
      "0.3 ('How can I become a brave person?', 'How can I become a courageous person?')\n",
      "----\n",
      "\n",
      "\n",
      "(question, f(question)) where f(question) replaces synonyms?\n",
      "Test cases:      326\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Replace synonyms in real pairs\n",
      "Test cases:      251\n",
      "Fails (rate):    32 (12.7%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('Are you truly happy?', 'Do you live happy?')\n",
      "0.0 ('Are you truly joyful?', 'Do you live happy?')\n",
      "0.0 ('Are you truly happy?', 'Do you live joyful?')\n",
      "\n",
      "----\n",
      "0.8 (\"If you couldn't be an atheist and had to choose a religion, which one would it be?\", 'If atheists were forced to choose any religious path, which religion would they choose?')\n",
      "0.3 (\"If you couldn't be an atheist and had to choose a religion, which one would it be?\", 'If atheists were forced to choose any spiritual path, which religion would they choose?')\n",
      "\n",
      "----\n",
      "0.9 ('What is the best way to make a girl happy?', 'How do I make a girl happy?')\n",
      "0.1 ('What is the best way to make a girl joyful?', 'How do I make a girl happy?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "How can I become more X = How can I become less antonym(X)\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('How can I become more active?', 'How can I become less passive?')\n",
      "----\n",
      "0.0 ('How can I become more stupid?', 'How can I become less smart?')\n",
      "----\n",
      "0.0 ('How can I become more optimistic?', 'How can I become less pessimistic?')\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "add one typo\n",
      "Test cases:      500\n",
      "Fails (rate):    60 (12.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('How can I know if my wife is a virgin?', 'How do I know if my wife was virgin till I fucked?')\n",
      "0.1 ('How can I know if myw ife is a virgin?', 'How do I know if my wife was virgin till I fucked?')\n",
      "\n",
      "----\n",
      "0.9 ('Do most white people have superiority complex?', 'Do most white people consider themselves superior to other races?')\n",
      "0.0 ('Do most white people have superiroity complex?', 'Do most white people consider themselves superior to other races?')\n",
      "\n",
      "----\n",
      "0.9 ('What are some solved problems in mathematics?', 'What are the most interesting (solved) problems in mathematics?')\n",
      "0.2 ('What are soem solved problems in mathematics?', 'What are the most interesting (solved) problems in mathematics?')\n",
      "0.4 ('What are some solved problems in mathematics?', 'What are the most interseting (solved) problems in mathematics?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "contrations\n",
      "Test cases:      500\n",
      "Fails (rate):    2 (0.4%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('What is PSE?', 'What is pse in personals?')\n",
      "0.4 (\"What's PSE?\", \"What's pse in personals?\")\n",
      "\n",
      "----\n",
      "0.4 ('What is the Police report has been submitted by your respective Thana and is under review at Commissioner Of Police, District Thane City?', 'What does this passport status mean, “Police reports have been submitted by your respective Thana and is under review at the commissioner Office”?')\n",
      "0.6 (\"What's the Police report has been submitted by your respective Thana and is under review at Commissioner Of Police, District Thane City?\", 'What does this passport status mean, “Police reports have been submitted by your respective Thana and is under review at the commissioner Office”?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "(q, paraphrase(q))\n",
      "Test cases:      200\n",
      "Fails (rate):    50 (25.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('How do I ask the right questions in any situation?', 'How do I ask the right questions in any situation?')\n",
      "0.4 ('How can I ask the right questions in any situation?', 'If you want to ask the right questions in any situation, what should you do?')\n",
      "0.4 ('In order to ask the right questions in any situation, what should you do?', 'How can I ask the right questions in any situation?')\n",
      "\n",
      "----\n",
      "0.8 ('Can you get the schedule for preparation of maths and English NDA in 6 months?', 'Can you get the schedule for preparation of maths and English NDA in 6 months?')\n",
      "0.3 ('Do you think you can get the schedule for preparation of maths and English NDA in 6 months?', 'Can I get the schedule for preparation of maths and English NDA in 6 months?')\n",
      "0.3 ('Do you think you can get the schedule for preparation of maths and English NDA in 6 months?', 'Can I get the schedule for preparation of maths and English NDA in 6 months?')\n",
      "\n",
      "----\n",
      "1.0 ('How do I improve myself and figure out what should I do with my life?', 'How do I improve myself and figure out what should I do with my life?')\n",
      "0.5 ('In order to improve myself and figure out what should you do with your life, what should you do?', 'How do I improve myself and figure out what should I do with my life?')\n",
      "0.5 ('In order to improve myself and figure out what should you do with your life, what should you do?', 'How can I improve myself and figure out what should I do with my life?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Product of paraphrases(q1) * paraphrases(q2)\n",
      "Test cases:      100\n",
      "Fails (rate):    33 (33.0%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('How can I stop feeling so depressed?', 'How do I stop feeling depressed for no reason?')\n",
      "0.3 ('How do you stop feeling so depressed?', 'If I want to stop feeling depressed for no reason, what should I do?')\n",
      "0.4 ('How can you stop feeling so depressed?', 'If I want to stop feeling depressed for no reason, what should I do?')\n",
      "\n",
      "----\n",
      "0.2 ('How do I recover a Yahoo account?', 'How do I recover a disabled Yahoo account?')\n",
      "0.6 ('If I want to recover a Yahoo account, what should I do?', 'How do you recover a disabled Yahoo account?')\n",
      "0.6 ('If I want to recover a Yahoo account, what should I do?', 'How can you recover a disabled Yahoo account?')\n",
      "\n",
      "----\n",
      "0.7 ('How do I get internship at IITs (UG student)?', 'How do I get a project internships in IITs and IISc?')\n",
      "0.0 ('If you want to get internship at IITs (UG student), what should you do?', 'What is a good way to get a project internships in IITs and IISc?')\n",
      "0.0 ('If you want to get internship at IITs (UG student), what should you do?', 'What is a good way to get a project internships in IITs and IISc?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "same adjectives, different people\n",
      "Test cases:      972\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "same adjectives, different people v2\n",
      "Test cases:      984\n",
      "Test cases run:  500\n",
      "Fails (rate):    3 (0.6%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('Is Joseph Taylor famous?', 'Is Jose Taylor famous?')\n",
      "----\n",
      "1.0 ('Is Tiffany Bennett American?', 'Is Brittany Bennett American?')\n",
      "----\n",
      "0.6 ('Is Emma King racist?', 'Is Emily King racist?')\n",
      "----\n",
      "\n",
      "\n",
      "same adjectives, different people v3\n",
      "Test cases:      990\n",
      "Test cases run:  500\n",
      "Fails (rate):    30 (6.0%)\n",
      "\n",
      "Example fails:\n",
      "0.7 ('Is Olivia Lee an astronaut?', 'Is Olivia Campbell an astronaut?')\n",
      "----\n",
      "0.9 ('Is Anna Johnson evil?', 'Is Anna Nelson evil?')\n",
      "----\n",
      "0.8 ('Is Jessica Davis Armenian?', 'Is Jessica Lee Armenian?')\n",
      "----\n",
      "\n",
      "\n",
      "Change same name in both questions\n",
      "Test cases:      500\n",
      "Fails (rate):    47 (9.4%)\n",
      "\n",
      "Example fails:\n",
      "0.1 ('How come humans can survive the Van Allen radiation belts?', 'What protection from the Van Allen belt do we have on the way to the moon?')\n",
      "0.8 ('How come humans can survive the Michael Butler radiation belts?', 'What protection from the Michael Butler belt do we have on the way to the moon?')\n",
      "0.7 ('How come humans can survive the Christopher Watson radiation belts?', 'What protection from the Christopher Watson belt do we have on the way to the moon?')\n",
      "\n",
      "----\n",
      "0.3 ('What is more likely to happen than Donald Trump becoming president?', 'What dramatic changes will happen in the world if Donald Trump becomes president?')\n",
      "0.9 ('What is more likely to happen than Matthew Jones becoming president?', 'What dramatic changes will happen in the world if Matthew Jones becomes president?')\n",
      "0.9 ('What is more likely to happen than Michael Butler becoming president?', 'What dramatic changes will happen in the world if Michael Butler becomes president?')\n",
      "\n",
      "----\n",
      "0.9 ('What are some interesting facts about formula 1 champion Lewis Hamilton?', 'What are some interesting facts about Lewis Hamilton?')\n",
      "0.3 ('What are some interesting facts about formula 1 champion David Cox?', 'What are some interesting facts about David Cox?')\n",
      "0.3 ('What are some interesting facts about formula 1 champion William Martinez?', 'What are some interesting facts about William Martinez?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change same location in both questions\n",
      "Test cases:      500\n",
      "Fails (rate):    56 (11.2%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('How can India win 20+ medals in Tokyo 2020 Olympic?', 'Can India win 10 gold medals in 2020?')\n",
      "0.3 ('How can Timor-Leste win 20+ medals in Tokyo 2020 Olympic?', 'Can Timor-Leste win 10 gold medals in 2020?')\n",
      "\n",
      "----\n",
      "0.7 ('Can India become a developed country from a developing one? Do we really get that day?', 'Do Indians really think India can become a developed country?')\n",
      "0.4 ('Can Jamaica become a developed country from a developing one? Do we really get that day?', 'Do Indians really think Jamaica can become a developed country?')\n",
      "0.4 ('Can Tanzania become a developed country from a developing one? Do we really get that day?', 'Do Indians really think Tanzania can become a developed country?')\n",
      "\n",
      "----\n",
      "0.1 ('What can a BDS do in Canada?', 'What can I do after BDS in Canada?')\n",
      "0.7 ('What can a BDS do in North Macedonia?', 'What can I do after BDS in North Macedonia?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change same number in both questions\n",
      "Test cases:      500\n",
      "Fails (rate):    19 (3.8%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Oneplus 3 review from Indian user?', 'As a user what is your review of OnePlus 3 including the battery part? Thank you')\n",
      "0.4 ('Oneplus 4 review from Indian user?', 'As a user what is your review of OnePlus 4 including the battery part? Thank you')\n",
      "0.4 ('Oneplus 4 review from Indian user?', 'As a user what is your review of OnePlus 4 including the battery part? Thank you')\n",
      "\n",
      "----\n",
      "0.0 ('Why was there an earthquake in the SF bay area the afternoon of January 7, 2011?', 'Was there an earthquake in the SF Bay Area the afternoon of January 7, 2011?')\n",
      "0.6 ('Why was there an earthquake in the SF bay area the afternoon of January 7, 2256?', 'Was there an earthquake in the SF Bay Area the afternoon of January 7, 2256?')\n",
      "0.6 ('Why was there an earthquake in the SF bay area the afternoon of January 7, 2167?', 'Was there an earthquake in the SF Bay Area the afternoon of January 7, 2167?')\n",
      "\n",
      "----\n",
      "0.3 ('If everyone over 18 died, what would happen?', 'Everyone over the age of 18 suddenly dies. Can our species survive?')\n",
      "0.6 ('If everyone over 17 died, what would happen?', 'Everyone over the age of 17 suddenly dies. Can our species survive?')\n",
      "0.6 ('If everyone over 17 died, what would happen?', 'Everyone over the age of 17 suddenly dies. Can our species survive?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change first name in one of the questions\n",
      "Test cases:      500\n",
      "After filtering: 281 (56.2%)\n",
      "Fails (rate):    248 (88.3%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Why is Thomas Edison more famous than Nikola Tesla?', 'Why Edison is more famous than Tesla though Tesla wants to make current available free of cost unlike edison who made money from it?')\n",
      "0.9 ('Why is Thomas Edison more famous than Nikola Tesla?', 'Why Eric is more famous than Tesla though Tesla wants to make current available free of cost unlike edison who made money from it?')\n",
      "0.8 ('Why is Thomas Edison more famous than Nikola Tesla?', 'Why Jack is more famous than Tesla though Tesla wants to make current available free of cost unlike edison who made money from it?')\n",
      "\n",
      "----\n",
      "0.6 ('What recommendations would you like to give to Walt Disney?', 'Recommendations to Walt Disney?')\n",
      "0.9 ('What recommendations would you like to give to Sean Disney?', 'Recommendations to Walt Disney?')\n",
      "0.6 ('What recommendations would you like to give to Lucas Disney?', 'Recommendations to Walt Disney?')\n",
      "\n",
      "----\n",
      "1.0 ('Could Donald Trump be a plant?', 'Did Hillary Clinton use Donald Trump as a way to have no relevant Political opponent?')\n",
      "1.0 ('Could Donald Trump be a plant?', 'Did Hillary Clinton use Jeffrey Trump as a way to have no relevant Political opponent?')\n",
      "1.0 ('Could Donald Trump be a plant?', 'Did Hillary Clinton use Peter Trump as a way to have no relevant Political opponent?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change first and last name in one of the questions\n",
      "Test cases:      682\n",
      "Test cases run:  500\n",
      "After filtering: 282 (56.4%)\n",
      "Fails (rate):    85 (30.1%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('How did Donald Trump win the Presidential election?', 'How did Donald Trump win the election?')\n",
      "0.8 ('How did Daniel Nelson win the Presidential election?', 'How did Donald Trump win the election?')\n",
      "0.5 ('How did William Martinez win the Presidential election?', 'How did Donald Trump win the election?')\n",
      "\n",
      "----\n",
      "1.0 ('Would Donald Trump make a good U.S president? Why or why not?', 'Donald Trump: Would you be a good president?')\n",
      "1.0 ('Would Donald Trump make a good U.S president? Why or why not?', 'Joshua Thompson: Would you be a good president?')\n",
      "1.0 ('Would Donald Trump make a good U.S president? Why or why not?', 'Christopher Cox: Would you be a good president?')\n",
      "\n",
      "----\n",
      "0.9 ('Is Harry Potter supposed to be attractive?', 'Is Harry Potter attractive?')\n",
      "1.0 ('Is Christopher Watson supposed to be attractive?', 'Is Harry Potter attractive?')\n",
      "1.0 ('Is Matthew Jones supposed to be attractive?', 'Is Harry Potter attractive?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change location in one of the questions\n",
      "Test cases:      1386\n",
      "Test cases run:  500\n",
      "After filtering: 248 (49.6%)\n",
      "Fails (rate):    51 (20.6%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Which are the top ten best Hollywood romantic movies?', 'What are some of the best romantic Hollywood movies?')\n",
      "0.8 ('Which are the top ten best Deltona romantic movies?', 'What are some of the best romantic Hollywood movies?')\n",
      "\n",
      "----\n",
      "1.0 (\"How will India's economy be affected if India goes to war against Pakistan?\", 'If war happen between India and Pakistan, then what could be the impact on Indian economy and stock market?')\n",
      "1.0 (\"How will India's economy be affected if India goes to war against Pakistan?\", 'If war happen between St. Vincent and the Grenadines and Pakistan, then what could be the impact on Indian economy and stock market?')\n",
      "1.0 (\"How will India's economy be affected if India goes to war against Pakistan?\", 'If war happen between São Tomé and Principe and Pakistan, then what could be the impact on Indian economy and stock market?')\n",
      "\n",
      "----\n",
      "1.0 ('Why is ₹500, ₹1000 notes banned in India?', 'Why Narendra Modi banned 500 and 1000 notes in India?')\n",
      "1.0 ('Why is ₹500, ₹1000 notes banned in India?', 'Why Narendra Modi banned 500 and 1000 notes in Netherlands?')\n",
      "1.0 ('Why is ₹500, ₹1000 notes banned in India?', 'Why Narendra Modi banned 500 and 1000 notes in Samoa?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change numbers in one of the questions\n",
      "Test cases:      1500\n",
      "Test cases run:  500\n",
      "After filtering: 286 (57.2%)\n",
      "Fails (rate):    193 (67.5%)\n",
      "\n",
      "Example fails:\n",
      "1.0 (\"What do you think of the Government's move of banning old Rs. 500 & Rs. 1000 notes?\", \"What do you think of PM's decision on the banning of 500 and 100 rupee notes?\")\n",
      "1.0 (\"What do you think of the Government's move of banning old Rs. 500 & Rs. 1000 notes?\", \"What do you think of PM's decision on the banning of 500 and 87 rupee notes?\")\n",
      "1.0 (\"What do you think of the Government's move of banning old Rs. 500 & Rs. 1000 notes?\", \"What do you think of PM's decision on the banning of 500 and 92 rupee notes?\")\n",
      "\n",
      "----\n",
      "1.0 ('What is the expected cutoff for KVPY SA 2016 -17?', 'What is the expected cutoff for the KVPY 2016?')\n",
      "1.0 ('What is the expected cutoff for KVPY SA 2016 -17?', 'What is the expected cutoff for the KVPY 1724?')\n",
      "1.0 ('What is the expected cutoff for KVPY SA 2016 -17?', 'What is the expected cutoff for the KVPY 1900?')\n",
      "\n",
      "----\n",
      "0.9 ('How do I get NEET Haryana state rank and apply for 85% seat quota seat counselling?', 'How do I apply for NEET Haryana state rank and apply for 85% state quota seat counselling?')\n",
      "0.6 ('How do I get NEET Haryana state rank and apply for 87% seat quota seat counselling?', 'How do I apply for NEET Haryana state rank and apply for 85% state quota seat counselling?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Keep entitites, fill in with gibberish\n",
      "Test cases:      500\n",
      "Fails (rate):    164 (32.8%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Why do so many people support Donald Trump?', 'How can people actually support Donald Trump?')\n",
      "0.9 ('Why do so many people support Donald Trump?', 'Why support Donald Trump?')\n",
      "0.8 ('How can people actually support Donald Trump?', 'How About Donald Trump?')\n",
      "\n",
      "----\n",
      "0.0 ('Why has sentiment among mainland Chinese turned so strongly against Taiwan in 2016?', 'How much effect has the fall in mainland tourists in 2016 to Taiwan affected the economy?')\n",
      "1.0 ('Why has sentiment among mainland Chinese turned so strongly against Taiwan in 2016?', 'Why did Chinese abandon Taiwan in 2016?')\n",
      "1.0 ('Why has sentiment among mainland Chinese turned so strongly against Taiwan in 2016?', 'Why did Chinese attack Taiwan in 2016?')\n",
      "\n",
      "----\n",
      "0.0 ('What is Mingle2?', 'How can I track a WeChat chat?')\n",
      "1.0 ('What is Mingle2?', 'What are Mingle2?')\n",
      "1.0 ('What is Mingle2?', 'What about Mingle2?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "Is person X != Did person use to be X\n",
      "Test cases:      999\n",
      "Test cases run:  500\n",
      "Fails (rate):    484 (96.8%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Is Emily Russell an entrepreneur?', 'Did Emily Russell use to be an entrepreneur?')\n",
      "----\n",
      "0.9 ('Is Michelle Gomez an educator?', 'Did Michelle Gomez use to be an educator?')\n",
      "----\n",
      "0.9 ('Is Shannon Gomez an interpreter?', 'Did Shannon Gomez use to be an interpreter?')\n",
      "----\n",
      "\n",
      "\n",
      "Is person X != Is person becoming X\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    108 (21.6%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Is Kayla Allen an assistant?', 'Is Kayla Allen becoming an assistant?')\n",
      "----\n",
      "0.7 ('Is Tiffany Sanders a producer?', 'Is Tiffany Sanders becoming a producer?')\n",
      "----\n",
      "0.5 ('Is Ryan Jones an adviser?', 'Is Ryan Jones becoming an adviser?')\n",
      "----\n",
      "\n",
      "\n",
      "What was person's life before becoming X != What was person's life after becoming X\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Do you have to X your dog before Y it != Do you have to X your dog after Y it.\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    313 (62.6%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Do you have to consider your hamster before adopting it?', 'Do you have to consider your hamster after adopting it?')\n",
      "----\n",
      "1.0 ('Do you have to breed your dog before selling it?', 'Do you have to breed your dog after selling it?')\n",
      "----\n",
      "0.7 ('Do you have to weigh your dog before trapping it?', 'Do you have to weigh your dog after trapping it?')\n",
      "----\n",
      "\n",
      "\n",
      "Is it {ok, dangerous, ...} to {smoke, rest, ...} after != before\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    172 (34.4%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Is it proper to tweet before 9am?', 'Is it proper to tweet after 9am?')\n",
      "----\n",
      "0.5 ('Is it reasonable to drive before 6pm?', 'Is it reasonable to drive after 6pm?')\n",
      "----\n",
      "1.0 ('Is it dangerous to drive before 1pm?', 'Is it dangerous to drive after 1pm?')\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "How can I become a X person != How can I become a person who is not X\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Is it {ok, dangerous, ...} to {smoke, rest, ...} in country != Is it {ok, dangerous, ...} not to {smoke, rest, ...} in country\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    3 (0.6%)\n",
      "\n",
      "Example fails:\n",
      "0.8 ('Is it wrong to be gay in Burundi?', 'Is it wrong not to be gay in Burundi?')\n",
      "----\n",
      "0.5 ('Is it socially acceptable to discriminate in Libya?', 'Is it socially acceptable not to discriminate in Libya?')\n",
      "----\n",
      "0.7 ('Is it dangerous to be gay in Ethiopia?', 'Is it dangerous not to be gay in Ethiopia?')\n",
      "----\n",
      "\n",
      "\n",
      "What are things a {noun} should worry about != should not worry about.\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "How can I become a X person == How can I become a person who is not antonym(X)\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "Fails (rate):    443 (88.6%)\n",
      "\n",
      "Example fails:\n",
      "0.1 ('How can I become a dependent person?', 'How can I become a person who is not independent?')\n",
      "----\n",
      "0.0 ('How can I become an offensive person?', 'How can I become a person who is not defensive?')\n",
      "----\n",
      "0.4 ('How can I become a stupid person?', 'How can I become a person who is not smart?')\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coref\n",
      "\n",
      "Simple coref: he and she\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "Fails (rate):    483 (96.6%)\n",
      "\n",
      "Example fails:\n",
      "0.5 ('If Jayden and Alexis were alone, do you think he would reject her?', 'If Jayden and Alexis were alone, do you think she would reject him?')\n",
      "----\n",
      "0.9 ('If Angela and Nathan were alone, do you think he would reject her?', 'If Angela and Nathan were alone, do you think she would reject him?')\n",
      "----\n",
      "0.9 ('If Haley and Julian were alone, do you think he would reject her?', 'If Haley and Julian were alone, do you think she would reject him?')\n",
      "----\n",
      "\n",
      "\n",
      "Simple coref: his and her\n",
      "Test cases:      2000\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 ('If Benjamin and Natalie were married, would her family be happy?', \"If Benjamin and Natalie were married, would Benjamin's family be happy?\")\n",
      "----\n",
      "1.0 ('If Anthony and Sophia were married, would his family be happy?', \"If Anthony and Sophia were married, would Sophia's family be happy?\")\n",
      "----\n",
      "1.0 ('If Jackson and Leah were married, would her family be happy?', \"If Jackson and Leah were married, would Jackson's family be happy?\")\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "Who do X think - Who is the ... according to X\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    2 (0.4%)\n",
      "\n",
      "Example fails:\n",
      "0.5 ('Who do people think is the greatest person in the world?', 'Who is the greatest person in the world according to people?')\n",
      "----\n",
      "0.5 ('Who do readers think is the premier magician in the world?', 'Who is the premier magician in the world according to readers?')\n",
      "----\n",
      "\n",
      "\n",
      "Order does not matter for comparison\n",
      "Test cases:      990\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('Are beans louder than pets?', 'What is louder, pets or beans?')\n",
      "\n",
      "----\n",
      "0.0 ('Are guys thicker than frogs?', 'What is thicker, frogs or guys?')\n",
      "0.1 ('Are guys thicker than frogs?', 'Are frogs thicker than guys?')\n",
      "\n",
      "----\n",
      "0.0 ('Are beetles worse than bugs?', 'What is worse, bugs or beetles?')\n",
      "0.0 ('Are beetles worse than bugs?', 'Are bugs worse than beetles?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Order does not matter for symmetric relations\n",
      "Test cases:      990\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('Is Aaron married to Michael?', 'Is Michael married to Aaron?')\n",
      "----\n",
      "0.0 ('Is Jeremy an acquaintance of Erin?', 'Is Erin an acquaintance of Jeremy?')\n",
      "----\n",
      "0.0 ('Is Mark close to Isabella?', 'Is Isabella close to Mark?')\n",
      "----\n",
      "\n",
      "\n",
      "Order does matter for asymmetric relations\n",
      "Test cases:      988\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('Is Dylan expecting Brandon?', 'Is Brandon expecting Dylan?')\n",
      "----\n",
      "0.0 ('Is Amy loyal to Daniel?', 'Is Daniel loyal to Amy?')\n",
      "----\n",
      "0.0 ('Is Robert indebted to Eric?', 'Is Eric indebted to Robert?')\n",
      "----\n",
      "\n",
      "\n",
      "traditional SRL: active / passive swap\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    105 (21.0%)\n",
      "\n",
      "Example fails:\n",
      "0.2 ('Did Kelly remember the book?', 'Was the book remembered by Kelly?')\n",
      "----\n",
      "0.4 ('Did Kevin lose the island?', 'Was the island lost by Kevin?')\n",
      "----\n",
      "0.1 ('Did Danielle receive the newspaper?', 'Was the newspaper received by Danielle?')\n",
      "----\n",
      "\n",
      "\n",
      "traditional SRL: wrong active / passive swap\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Did Kayla leave the property?', 'Was Kayla left by the property?')\n",
      "----\n",
      "1.0 ('Did Isabella like the property?', 'Was Isabella liked by the property?')\n",
      "----\n",
      "1.0 ('Did Sara move the gun?', 'Was Sara moved by the gun?')\n",
      "----\n",
      "\n",
      "\n",
      "traditional SRL: active / passive swap with people\n",
      "Test cases:      990\n",
      "Test cases run:  500\n",
      "Fails (rate):    493 (98.6%)\n",
      "\n",
      "Example fails:\n",
      "0.0 ('Does Amanda love William?', 'Is William loved by Amanda?')\n",
      "----\n",
      "0.0 ('Does Jonathan hate Scott?', 'Is Scott hated by Jonathan?')\n",
      "----\n",
      "0.0 ('Does Lauren like Samantha?', 'Is Samantha liked by Lauren?')\n",
      "----\n",
      "\n",
      "\n",
      "traditional SRL: wrong active / passive swap with people\n",
      "Test cases:      989\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "1.0 ('Does Heather bother William?', 'Is Heather bothered by William?')\n",
      "----\n",
      "1.0 ('Does Ryan hurt Tiffany?', 'Is Ryan hurt by Tiffany?')\n",
      "----\n",
      "1.0 ('Does Angela attack Justin?', 'Is Angela attacked by Justin?')\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Logic\n",
      "\n",
      "A or B is not the same as C and D\n",
      "Test cases:      828\n",
      "Test cases run:  500\n",
      "Fails (rate):    40 (8.0%)\n",
      "\n",
      "Example fails:\n",
      "0.5 ('Is Christina Perry an investigator or an attorney?', 'Is Christina Perry simultaneously an activist and an investor?')\n",
      "----\n",
      "0.6 ('Is Kyle Edwards an assistant or an intern?', 'Is Kyle Edwards simultaneously an agent and a journalist?')\n",
      "----\n",
      "0.7 ('Is Amy Rivera an adviser or an advisor?', 'Is Amy Rivera simultaneously an administrator and an interpreter?')\n",
      "----\n",
      "\n",
      "\n",
      "A or B is not the same as A and B\n",
      "Test cases:      971\n",
      "Test cases run:  500\n",
      "Fails (rate):    500 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('Is Thomas Allen an entrepreneur or an investor?', 'Is Thomas Allen simultaneously an entrepreneur and an investor?')\n",
      "----\n",
      "0.9 ('Is Emma Roberts an organizer or an investigator?', 'Is Emma Roberts simultaneously an organizer and an investigator?')\n",
      "----\n",
      "0.9 ('Is Patrick Gomez an engineer or an attorney?', 'Is Patrick Gomez simultaneously an engineer and an attorney?')\n",
      "----\n",
      "\n",
      "\n",
      "A and / or B is the same as B and / or A\n",
      "Test cases:      970\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "a {nationality} {profession} = a {profession} and {nationality}\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "Reflexivity: (q, q) should be duplicate\n",
      "Test cases:      1000\n",
      "Test cases run:  500\n",
      "Fails (rate):    4 (0.8%)\n",
      "\n",
      "Example fails:\n",
      "0.4 ('When will Cognizant start off campus for the freshers 2016-2017 batch?', 'When will Cognizant start off campus for the freshers 2016-2017 batch?')\n",
      "----\n",
      "0.0 ('What does the following symbol mean ➰?', 'What does the following symbol mean ➰?')\n",
      "----\n",
      "0.2 ('What are some things new employees should know going into their first day at Receptos?', 'What are some things new employees should know going into their first day at Receptos?')\n",
      "----\n",
      "\n",
      "\n",
      "Symmetry: f(a, b) = f(b, a)\n",
      "Test cases:      500\n",
      "Fails (rate):    11 (2.2%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('How do I transfer YouTube offline videos to SD card?', 'How get offline video of YouTube in SD card?')\n",
      "0.3 ('How get offline video of YouTube in SD card?', 'How do I transfer YouTube offline videos to SD card?')\n",
      "\n",
      "----\n",
      "0.4 ('What is the requirement to study medicine in germany?', 'How do I study medicine in germany?')\n",
      "0.6 ('How do I study medicine in germany?', 'What is the requirement to study medicine in germany?')\n",
      "\n",
      "----\n",
      "0.3 (\"What is it like to test iPhone 7's waterproof feature?\", 'How waterproof is the iPhone 7?')\n",
      "0.7 ('How waterproof is the iPhone 7?', \"What is it like to test iPhone 7's waterproof feature?\")\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Testing implications\n",
      "Test cases:      8328\n",
      "Test cases run:  500\n",
      "After filtering: 458 (91.6%)\n",
      "Fails (rate):    39 (8.5%)\n",
      "\n",
      "Example fails:\n",
      "0.9 ('Do you think Arijit Singh is an overrated singer?', 'Is Arijit Singh overrated?')\n",
      "0.6 ('Do you think Arijit Singh is an overrated singer?', 'Is Arijit Singh underrated?')\n",
      "0.0 ('Is Arijit Singh overrated?', 'Is Arijit Singh underrated?')\n",
      "\n",
      "----\n",
      "0.0 (\"What's your favorite Chinese food ?\", 'What is your favorite Chinese food for foreigners?')\n",
      "1.0 (\"What's your favorite Chinese food ?\", \"What's your favorite Chinese food, and why?\")\n",
      "0.0 ('What is your favorite Chinese food for foreigners?', \"What's your favorite Chinese food, and why?\")\n",
      "\n",
      "----\n",
      "0.9 ('What are the best available social media analytics tool?', 'How do I choose the best social media analysis tool?')\n",
      "1.0 ('What are the best available social media analytics tool?', 'What is the best social media analysis tool and why?')\n",
      "0.4 ('How do I choose the best social media analysis tool?', 'What is the best social media analysis tool and why?')\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.suite.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/squad_checklist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = cl.CheckListSuite(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CheckList for SQuAD.\n",
      "Predictions: each prediction is a string, containing the answer\n",
      "Confidences: not necessary for this checklist\n",
      "\n",
      "Test names for Table 3 in the paper:\n",
      "['A is COMP than B. Who is more / less COMP?', 'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?', 'size, shape, age, color', 'Profession vs nationality', 'Animal vs Vehicle v2', 'A is COMP than B. Who is antonym(COMP)? B', 'A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.', 'Question typo', 'Add random sentence to context', 'There was a change in profession', 'Understanding before / after -> first / last.', 'Negation in context, may or may not be in question', 'Negation in question only.', 'M/F failure rates should be similar for different professions', 'Basic coref, he / she', 'Basic coref, his / her', 'Former / Latter', 'Agent / object distinction', 'Agent / object distinction with 3 agents']\n",
      "\n",
      "Use with nlp.checklist.CheckListSuite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(suite.dataset.info.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = '/home/marcotcr/work/checklist/release_data/squad/predictions/'\n",
    "bert_preds = open(os.path.join(prediction_path, 'bert')).read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.dataset = suite.dataset.map(lambda _, idx: {'bert': bert_preds[idx]}, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests don't really require confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0f70907b594dcea802f5dbaaf34d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suite.compute('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_table3 = [\n",
    "    'A is COMP than B. Who is more / less COMP?',\n",
    "    'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?',\n",
    "    'size, shape, age, color',\n",
    "    'Profession vs nationality',\n",
    "    'Animal vs Vehicle v2',\n",
    "    'A is COMP than B. Who is antonym(COMP)? B',\n",
    "    'A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.',\n",
    "    'Question typo',\n",
    "    'Add random sentence to context',\n",
    "    'There was a change in profession',\n",
    "    'Understanding before / after -> first / last.',\n",
    "    'Negation in context, may or may not be in question',\n",
    "    'Negation in question only.', 'M/F failure rates should be similar for different professions',\n",
    "    'Basic coref, he / she',\n",
    "    'Basic coref, his / her',\n",
    "    'Former / Latter',\n",
    "    'Agent / object distinction',\n",
    "    'Agent / object distinction with 3 agents'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20.0 A is COMP than B. Who is more / less COMP?\n",
      " 91.3 Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?\n",
      " 82.4 size, shape, age, color\n",
      " 49.4 Profession vs nationality\n",
      " 26.2 Animal vs Vehicle v2\n",
      " 67.3 A is COMP than B. Who is antonym(COMP)? B\n",
      "100.0 A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.\n",
      " 11.6 Question typo\n",
      "  9.8 Add random sentence to context\n",
      " 41.5 There was a change in profession\n",
      " 82.9 Understanding before / after -> first / last.\n",
      " 67.5 Negation in context, may or may not be in question\n",
      "100.0 Negation in question only.\n",
      " 46.2 M/F failure rates should be similar for different professions\n",
      "100.0 Basic coref, he / she\n",
      " 91.8 Basic coref, his / her\n",
      "100.0 Former / Latter\n",
      " 60.8 Agent / object distinction\n",
      " 95.7 Agent / object distinction with 3 agents\n"
     ]
    }
   ],
   "source": [
    "for t in checklist_table3:\n",
    "    print('%5.1f %s' % (suite.fail_rate['bert'][t], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary\n",
      "\n",
      "A is COMP than B. Who is more / less COMP?\n",
      "Test cases:      494\n",
      "Fails (rate):    99 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Dylan is tougher than Sarah.\n",
      "Q: Who is less tough?\n",
      "A: Sarah\n",
      "P: Dylan\n",
      "\n",
      "\n",
      "----\n",
      "C: Adam is smaller than Amy.\n",
      "Q: Who is less small?\n",
      "A: Amy\n",
      "P: Adam\n",
      "\n",
      "\n",
      "----\n",
      "C: Kelly is older than Isabella.\n",
      "Q: Who is less old?\n",
      "A: Isabella\n",
      "P: Kelly\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?\n",
      "Test cases:      497\n",
      "Fails (rate):    454 (91.3%)\n",
      "\n",
      "Example fails:\n",
      "C: Jose is really particular about the project. Jessica is particular about the project.\n",
      "Q: Who is most particular about the project?\n",
      "A: Jose\n",
      "P: Jessica\n",
      "\n",
      "C: Jessica is particular about the project. Jose is really particular about the project.\n",
      "Q: Who is most particular about the project?\n",
      "A: Jose\n",
      "P: Jessica\n",
      "\n",
      "C: Jessica is particular about the project. Jose is really particular about the project.\n",
      "Q: Who is least particular about the project?\n",
      "A: Jessica\n",
      "P: Jose\n",
      "\n",
      "\n",
      "----\n",
      "C: Ethan is pleased about the project. Jeremy is extremely pleased about the project.\n",
      "Q: Who is least pleased about the project?\n",
      "A: Ethan\n",
      "P: Jeremy\n",
      "\n",
      "\n",
      "----\n",
      "C: Justin is positive about the project. Samuel is highly positive about the project.\n",
      "Q: Who is least positive about the project?\n",
      "A: Justin\n",
      "P: Samuel\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taxonomy\n",
      "\n",
      "size, shape, age, color\n",
      "Test cases:      500\n",
      "Fails (rate):    412 (82.4%)\n",
      "\n",
      "Example fails:\n",
      "C: There is a big brown figure in the room.\n",
      "Q: What size is the figure?\n",
      "A: big\n",
      "P: big brown\n",
      "\n",
      "C: There is a figure in the room. The figure is big and brown.\n",
      "Q: What size is the figure?\n",
      "A: big\n",
      "P: big and brown\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a new yellow thing in the room.\n",
      "Q: What age is the thing?\n",
      "A: new\n",
      "P: new yellow\n",
      "\n",
      "C: There is a thing in the room. The thing is new and yellow.\n",
      "Q: What age is the thing?\n",
      "A: new\n",
      "P: new and yellow\n",
      "\n",
      "\n",
      "----\n",
      "C: There is a tiny oval thing in the room.\n",
      "Q: What size is the thing?\n",
      "A: tiny\n",
      "P: oval\n",
      "\n",
      "C: There is a thing in the room. The thing is tiny and oval.\n",
      "Q: What size is the thing?\n",
      "A: tiny\n",
      "P: tiny and oval\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Profession vs nationality\n",
      "Test cases:      500\n",
      "Fails (rate):    247 (49.4%)\n",
      "\n",
      "Example fails:\n",
      "C: Amber is a Chinese administrator.\n",
      "Q: What is Amber's job?\n",
      "A: administrator\n",
      "P: Chinese administrator\n",
      "\n",
      "\n",
      "----\n",
      "C: Anna is a Russian producer.\n",
      "Q: What is Anna's job?\n",
      "A: producer\n",
      "P: Russian producer\n",
      "\n",
      "\n",
      "----\n",
      "C: Andrea is a Russian educator.\n",
      "Q: What is Andrea's job?\n",
      "A: educator\n",
      "P: Russian educator\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Animal vs Vehicle\n",
      "Test cases:      500\n",
      "Fails (rate):    128 (25.6%)\n",
      "\n",
      "Example fails:\n",
      "C: Elizabeth has a firetruck and a guinea pig.\n",
      "Q: What animal does Elizabeth have?\n",
      "A: guinea pig\n",
      "P: a firetruck and a guinea pig\n",
      "\n",
      "\n",
      "----\n",
      "C: Emily has a bull and a minivan.\n",
      "Q: What vehicle does Emily have?\n",
      "A: minivan\n",
      "P: a bull and a minivan\n",
      "\n",
      "\n",
      "----\n",
      "C: James has a hamster and a firetruck.\n",
      "Q: What vehicle does James have?\n",
      "A: firetruck\n",
      "P: a hamster and a firetruck\n",
      "\n",
      "C: James has a firetruck and a hamster.\n",
      "Q: What animal does James have?\n",
      "A: hamster\n",
      "P: a firetruck and a hamster\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Animal vs Vehicle v2\n",
      "Test cases:      496\n",
      "Fails (rate):    130 (26.2%)\n",
      "\n",
      "Example fails:\n",
      "C: Michael bought a tractor. Sophia bought a hamster.\n",
      "Q: Who bought an animal?\n",
      "A: Sophia\n",
      "P: Michael\n",
      "\n",
      "\n",
      "----\n",
      "C: William bought a firetruck. Austin bought an iguana.\n",
      "Q: Who bought an animal?\n",
      "A: Austin\n",
      "P: William\n",
      "\n",
      "\n",
      "----\n",
      "C: Sara bought a bull. Michael bought a firetruck.\n",
      "Q: Who bought a vehicle?\n",
      "A: Michael\n",
      "P: Sara\n",
      "\n",
      "C: Michael bought a firetruck. Sara bought a bull.\n",
      "Q: Who bought an animal?\n",
      "A: Sara\n",
      "P: Michael\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Synonyms\n",
      "Test cases:      447\n",
      "Fails (rate):    0 (0.0%)\n",
      "\n",
      "\n",
      "A is COMP than B. Who is antonym(COMP)? B\n",
      "Test cases:      496\n",
      "Fails (rate):    334 (67.3%)\n",
      "\n",
      "Example fails:\n",
      "C: Jordan is quieter than Anna.\n",
      "Q: Who is louder?\n",
      "A: Anna\n",
      "P: Jordan\n",
      "\n",
      "\n",
      "----\n",
      "C: Brittany is slower than Hannah.\n",
      "Q: Who is faster?\n",
      "A: Hannah\n",
      "P: Brittany\n",
      "\n",
      "\n",
      "----\n",
      "C: Aaron is quieter than Emily.\n",
      "Q: Who is louder?\n",
      "A: Emily\n",
      "P: Aaron\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.\n",
      "Test cases:      491\n",
      "Fails (rate):    491 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Olivia is more active than Laura.\n",
      "Q: Who is less passive?\n",
      "A: Olivia\n",
      "P: Laura\n",
      "\n",
      "C: Laura is more passive than Olivia.\n",
      "Q: Who is more active?\n",
      "A: Olivia\n",
      "P: Laura\n",
      "\n",
      "C: Laura is more passive than Olivia.\n",
      "Q: Who is less active?\n",
      "A: Laura\n",
      "P: Olivia\n",
      "\n",
      "\n",
      "----\n",
      "C: Erin is more irresponsible than Robert.\n",
      "Q: Who is more responsible?\n",
      "A: Robert\n",
      "P: Erin\n",
      "\n",
      "C: Erin is more irresponsible than Robert.\n",
      "Q: Who is less responsible?\n",
      "A: Erin\n",
      "P: Robert\n",
      "\n",
      "C: Robert is more responsible than Erin.\n",
      "Q: Who is more irresponsible?\n",
      "A: Erin\n",
      "P: Robert\n",
      "\n",
      "\n",
      "----\n",
      "C: Justin is more hopeful than Brandon.\n",
      "Q: Who is less hopeless?\n",
      "A: Justin\n",
      "P: Brandon\n",
      "\n",
      "C: Brandon is more hopeless than Justin.\n",
      "Q: Who is more hopeful?\n",
      "A: Justin\n",
      "P: Brandon\n",
      "\n",
      "C: Justin is less hopeless than Brandon.\n",
      "Q: Who is less hopeful?\n",
      "A: Brandon\n",
      "P: Justin\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Robustness\n",
      "\n",
      "Question typo\n",
      "Test cases:      500\n",
      "Fails (rate):    58 (11.6%)\n",
      "\n",
      "Example fails:\n",
      "C: Pharmacists are healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines. Pharmacists may also be small-business proprietors, owning the pharmacy in which they practice. Since pharmacists know about the mode of action of a particular drug, and its metabolism and physiological effects on the human body in great detail, they play an important role in optimisation of a drug treatment for an individual.\n",
      "Q: What other role do many pharmacists play?\n",
      "P: optimisation of a drug treatment for an individual\n",
      "\n",
      "C: Pharmacists are healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines. Pharmacists may also be small-business proprietors, owning the pharmacy in which they practice. Since pharmacists know about the mode of action of a particular drug, and its metabolism and physiological effects on the human body in great detail, they play an important role in optimisation of a drug treatment for an individual.\n",
      "Q: Whato ther role do many pharmacists play?\n",
      "P: optimisation of a drug treatment\n",
      "\n",
      "\n",
      "----\n",
      "C: The collection of textiles consists of more than 53,000 examples, mainly western European though all populated continents are represented, dating from the 1st century AD to the present, this is the largest such collection in the world. Techniques represented include weaving, printing, quilting embroidery, lace, tapestry and carpets. These are classified by technique, countries of origin and date of production. The collections are well represented in these areas: early silks from the Near East, lace, European tapestries and English medieval church embroidery.\n",
      "Q: Besides countries of origin and date of production, how are the textiles classified?\n",
      "P: by technique\n",
      "\n",
      "C: The collection of textiles consists of more than 53,000 examples, mainly western European though all populated continents are represented, dating from the 1st century AD to the present, this is the largest such collection in the world. Techniques represented include weaving, printing, quilting embroidery, lace, tapestry and carpets. These are classified by technique, countries of origin and date of production. The collections are well represented in these areas: early silks from the Near East, lace, European tapestries and English medieval church embroidery.\n",
      "Q: Beside scountries of origin and date of production, how are the textiles classified?\n",
      "P: technique\n",
      "\n",
      "\n",
      "----\n",
      "C: Following the Nice Treaty, there was an attempt to reform the constitutional law of the European Union and make it more transparent; this would have also produced a single constitutional document. However, as a result of the referendum in France and the referendum in the Netherlands, the 2004 Treaty establishing a Constitution for Europe never came into force. Instead, the Lisbon Treaty was enacted. Its substance was very similar to the proposed constitutional treaty, but it was formally an amending treaty, and – though it significantly altered the existing treaties – it did not completely replace them.\n",
      "Q: What reform was attempted following the Nice Treaty?\n",
      "P: constitutional law of the European Union and make it more transparent\n",
      "\n",
      "C: Following the Nice Treaty, there was an attempt to reform the constitutional law of the European Union and make it more transparent; this would have also produced a single constitutional document. However, as a result of the referendum in France and the referendum in the Netherlands, the 2004 Treaty establishing a Constitution for Europe never came into force. Instead, the Lisbon Treaty was enacted. Its substance was very similar to the proposed constitutional treaty, but it was formally an amending treaty, and – though it significantly altered the existing treaties – it did not completely replace them.\n",
      "Q: What rfeorm was attempted following the Nice Treaty?\n",
      "P: reform the constitutional law of the European Union and make it more transparent\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Question contractions\n",
      "Test cases:      500\n",
      "Fails (rate):    17 (3.4%)\n",
      "\n",
      "Example fails:\n",
      "C: Members of the genus Dinophysis have a phycobilin-containing chloroplast taken from a cryptophyte. However, the cryptophyte is not an endosymbiont—only the chloroplast seems to have been taken, and the chloroplast has been stripped of its nucleomorph and outermost two membranes, leaving just a two-membraned chloroplast. Cryptophyte chloroplasts require their nucleomorph to maintain themselves, and Dinophysis species grown in cell culture alone cannot survive, so it is possible (but not confirmed) that the Dinophysis chloroplast is a kleptoplast—if so, Dinophysis chloroplasts wear out and Dinophysis species must continually engulf cryptophytes to obtain new chloroplasts to replace the old ones.\n",
      "Q: What is left of the Dinophysis chloroplasts?\n",
      "P: a two-membraned chloroplast\n",
      "\n",
      "C: Members of the genus Dinophysis have a phycobilin-containing chloroplast taken from a cryptophyte. However, the cryptophyte is not an endosymbiont—only the chloroplast seems to have been taken, and the chloroplast has been stripped of its nucleomorph and outermost two membranes, leaving just a two-membraned chloroplast. Cryptophyte chloroplasts require their nucleomorph to maintain themselves, and Dinophysis species grown in cell culture alone cannot survive, so it is possible (but not confirmed) that the Dinophysis chloroplast is a kleptoplast—if so, Dinophysis chloroplasts wear out and Dinophysis species must continually engulf cryptophytes to obtain new chloroplasts to replace the old ones.\n",
      "Q: What's left of the Dinophysis chloroplasts?\n",
      "P: two-membraned chloroplast\n",
      "\n",
      "\n",
      "----\n",
      "C: Chloroplasts have their own ribosomes, which they use to synthesize a small fraction of their proteins. Chloroplast ribosomes are about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm). They take mRNAs transcribed from the chloroplast DNA and translate them into protein. While similar to bacterial ribosomes, chloroplast translation is more complex than in bacteria, so chloroplast ribosomes include some chloroplast-unique features. Small subunit ribosomal RNAs in several Chlorophyta and euglenid chloroplasts lack motifs for shine-dalgarno sequence recognition, which is considered essential for translation initiation in most chloroplasts and prokaryotes. Such loss is also rarely observed in other plastids and prokaryotes.\n",
      "Q: What are some Chlorophyta and euglenid chloroplasts missing?\n",
      "P: motifs for shine-dalgarno sequence recognition\n",
      "\n",
      "C: Chloroplasts have their own ribosomes, which they use to synthesize a small fraction of their proteins. Chloroplast ribosomes are about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm). They take mRNAs transcribed from the chloroplast DNA and translate them into protein. While similar to bacterial ribosomes, chloroplast translation is more complex than in bacteria, so chloroplast ribosomes include some chloroplast-unique features. Small subunit ribosomal RNAs in several Chlorophyta and euglenid chloroplasts lack motifs for shine-dalgarno sequence recognition, which is considered essential for translation initiation in most chloroplasts and prokaryotes. Such loss is also rarely observed in other plastids and prokaryotes.\n",
      "Q: What're some Chlorophyta and euglenid chloroplasts missing?\n",
      "P: Small subunit ribosomal RNAs\n",
      "\n",
      "\n",
      "----\n",
      "C: In Mongolia today, Genghis Khan's name and likeness are endorsed on products, streets, buildings, and other places. His face can be found on everyday commodities, from liquor bottles to candy products, and on the largest denominations of 500, 1,000, 5,000, 10,000, and 20,000 Mongolian tögrög (₮). Mongolia's main international airport in Ulaanbaatar is named Chinggis Khaan International Airport. Major Genghis Khan statues have been erected before the parliament and near Ulaanbaatar. There have been repeated discussions about regulating the use of his name and image to avoid trivialization.\n",
      "Q: What is the name of contemporary Mongolian currency?\n",
      "P: tögrög\n",
      "\n",
      "C: In Mongolia today, Genghis Khan's name and likeness are endorsed on products, streets, buildings, and other places. His face can be found on everyday commodities, from liquor bottles to candy products, and on the largest denominations of 500, 1,000, 5,000, 10,000, and 20,000 Mongolian tögrög (₮). Mongolia's main international airport in Ulaanbaatar is named Chinggis Khaan International Airport. Major Genghis Khan statues have been erected before the parliament and near Ulaanbaatar. There have been repeated discussions about regulating the use of his name and image to avoid trivialization.\n",
      "Q: What's the name of contemporary Mongolian currency?\n",
      "P: Mongolian tögrög\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Add random sentence to context\n",
      "Test cases:      500\n",
      "Fails (rate):    49 (9.8%)\n",
      "\n",
      "Example fails:\n",
      "C: Some chloroplasts contain a structure called the chloroplast peripheral reticulum. It is often found in the chloroplasts of C4 plants, though it has also been found in some C3 angiosperms, and even some gymnosperms. The chloroplast peripheral reticulum consists of a maze of membranous tubes and vesicles continuous with the inner chloroplast membrane that extends into the internal stromal fluid of the chloroplast. Its purpose is thought to be to increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm. The small vesicles sometimes observed may serve as transport vesicles to shuttle stuff between the thylakoids and intermembrane space.\n",
      "Q: What does the chloroplast peripheral reticulum do?\n",
      "P: to increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm\n",
      "\n",
      "C: Some chloroplasts contain a structure called the chloroplast peripheral reticulum. It is often found in the chloroplasts of C4 plants, though it has also been found in some C3 angiosperms, and even some gymnosperms. The chloroplast peripheral reticulum consists of a maze of membranous tubes and vesicles continuous with the inner chloroplast membrane that extends into the internal stromal fluid of the chloroplast. Its purpose is thought to be to increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm. The small vesicles sometimes observed may serve as transport vesicles to shuttle stuff between the thylakoids and intermembrane space.It was ranked 8th on the 2013-2014 PayScale College Salary Report and 14th on the 2013 PayScale College Education Value Rankings. \n",
      "Q: What does the chloroplast peripheral reticulum do?\n",
      "P: increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm\n",
      "\n",
      "\n",
      "----\n",
      "C: The capabilities approach – sometimes called the human development approach – looks at income inequality and poverty as form of “capability deprivation”. Unlike neoliberalism, which “defines well-being as utility maximization”, economic growth and income are considered a means to an end rather than the end itself. Its goal is to “wid[en] people’s choices and the level of their achieved well-being” through increasing functionings (the things a person values doing), capabilities (the freedom to enjoy functionings) and agency (the ability to pursue valued goals).\n",
      "Q: How would the capabilities approach achieve it's goal?\n",
      "P: through increasing functionings (the things a person values doing), capabilities (the freedom to enjoy functionings\n",
      "\n",
      "C: The capabilities approach – sometimes called the human development approach – looks at income inequality and poverty as form of “capability deprivation”. Unlike neoliberalism, which “defines well-being as utility maximization”, economic growth and income are considered a means to an end rather than the end itself. Its goal is to “wid[en] people’s choices and the level of their achieved well-being” through increasing functionings (the things a person values doing), capabilities (the freedom to enjoy functionings) and agency (the ability to pursue valued goals).Italian sculptors whose work is held by the museum include: Bartolomeo Bon, Bartolomeo Bellano, Luca della Robbia, Giovanni Pisano, Donatello, Agostino di Duccio, Andrea Riccio, Antonio Rossellino, Andrea del Verrocchio, Antonio Lombardo, Pier Jacopo Alari Bonacolsi, Andrea della Robbia, Michelozzo di Bartolomeo, Michelangelo (represented by a freehand wax model and casts of his most famous sculptures), Jacopo Sansovino, Alessandro Algardi, Antonio Calcagni, Benvenuto Cellini (Medusa's head dated c. 1547), Agostino Busti, Bartolomeo Ammannati, Giacomo della Porta, Giambologna (Samson Slaying a Philistine (Giambologna) c. 1562, his finest work outside Italy), Bernini (Neptune and Triton c. 1622–3), Giovanni Battista Foggini, Vincenzo Foggini. \n",
      "Q: How would the capabilities approach achieve it's goal?\n",
      "P: Its goal is to “wid[en] people’s choices and the level of their achieved well-being”\n",
      "\n",
      "C: Italian sculptors whose work is held by the museum include: Bartolomeo Bon, Bartolomeo Bellano, Luca della Robbia, Giovanni Pisano, Donatello, Agostino di Duccio, Andrea Riccio, Antonio Rossellino, Andrea del Verrocchio, Antonio Lombardo, Pier Jacopo Alari Bonacolsi, Andrea della Robbia, Michelozzo di Bartolomeo, Michelangelo (represented by a freehand wax model and casts of his most famous sculptures), Jacopo Sansovino, Alessandro Algardi, Antonio Calcagni, Benvenuto Cellini (Medusa's head dated c. 1547), Agostino Busti, Bartolomeo Ammannati, Giacomo della Porta, Giambologna (Samson Slaying a Philistine (Giambologna) c. 1562, his finest work outside Italy), Bernini (Neptune and Triton c. 1622–3), Giovanni Battista Foggini, Vincenzo Foggini. The capabilities approach – sometimes called the human development approach – looks at income inequality and poverty as form of “capability deprivation”. Unlike neoliberalism, which “defines well-being as utility maximization”, economic growth and income are considered a means to an end rather than the end itself. Its goal is to “wid[en] people’s choices and the level of their achieved well-being” through increasing functionings (the things a person values doing), capabilities (the freedom to enjoy functionings) and agency (the ability to pursue valued goals).\n",
      "Q: How would the capabilities approach achieve it's goal?\n",
      "P: freedom to enjoy functionings\n",
      "\n",
      "\n",
      "----\n",
      "C: The word pharmacy is derived from its root word pharma which was a term used since the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery or even poison. In addition to pharma responsibilities, the pharma offered general medical advice and a range of services that are now performed solely by other specialist practitioners, such as surgery and midwifery. The pharma (as it was referred to) often operated through a retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. Often the place that did this was called an apothecary and several languages have this as the dominant term, though their practices are more akin to a modern pharmacy, in English the term apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning \"drug\", \"medicine\" (or \"poison\").[n 1]\n",
      "Q: What else was used by pharmas?\n",
      "P: herbs not listed\n",
      "\n",
      "C: The word pharmacy is derived from its root word pharma which was a term used since the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery or even poison. In addition to pharma responsibilities, the pharma offered general medical advice and a range of services that are now performed solely by other specialist practitioners, such as surgery and midwifery. The pharma (as it was referred to) often operated through a retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. Often the place that did this was called an apothecary and several languages have this as the dominant term, though their practices are more akin to a modern pharmacy, in English the term apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning \"drug\", \"medicine\" (or \"poison\").[n 1]Upon arrival, Tesla realized that the company, then under construction, was not functional, so he worked as a draftsman in the Central Telegraph Office instead. \n",
      "Q: What else was used by pharmas?\n",
      "P: many other herbs not listed\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NER\n",
      "\n",
      "Change name everywhere\n",
      "Test cases:      500\n",
      "Fails (rate):    29 (5.8%)\n",
      "\n",
      "Example fails:\n",
      "C: It became clear that managing the Apollo program would exceed the capabilities of Robert R. Gilruth's Space Task Group, which had been directing the nation's manned space program from NASA's Langley Research Center. So Gilruth was given authority to grow his organization into a new NASA center, the Manned Spacecraft Center (MSC). A site was chosen in Houston, Texas, on land donated by Rice University, and Administrator Webb announced the conversion on September 19, 1961. It was also clear NASA would soon outgrow its practice of controlling missions from its Cape Canaveral Air Force Station launch facilities in Florida, so a new Mission Control Center would be included in the MSC.\n",
      "Q: Who originally led the Space Task Group?\n",
      "P: Robert R. Gilruth\n",
      "\n",
      "C: It became clear that managing the Apollo program would exceed the capabilities of Michael Hall Space Task Group, which had been directing the nation's manned space program from NASA's Langley Research Center. So Gilruth was given authority to grow his organization into a new NASA center, the Manned Spacecraft Center (MSC). A site was chosen in Houston, Texas, on land donated by Rice University, and Administrator Webb announced the conversion on September 19, 1961. It was also clear NASA would soon outgrow its practice of controlling missions from its Cape Canaveral Air Force Station launch facilities in Florida, so a new Mission Control Center would be included in the MSC.\n",
      "Q: Who originally led the Space Task Group?\n",
      "P: Michael Hall\n",
      "\n",
      "C: It became clear that managing the Apollo program would exceed the capabilities of Christopher Clark Space Task Group, which had been directing the nation's manned space program from NASA's Langley Research Center. So Gilruth was given authority to grow his organization into a new NASA center, the Manned Spacecraft Center (MSC). A site was chosen in Houston, Texas, on land donated by Rice University, and Administrator Webb announced the conversion on September 19, 1961. It was also clear NASA would soon outgrow its practice of controlling missions from its Cape Canaveral Air Force Station launch facilities in Florida, so a new Mission Control Center would be included in the MSC.\n",
      "Q: Who originally led the Space Task Group?\n",
      "P: Christopher Clark\n",
      "\n",
      "\n",
      "----\n",
      "C: On 6 November 1915, a Reuters news agency report from London had the 1915 Nobel Prize in Physics awarded to Thomas Edison and Nikola Tesla; however, on 15 November, a Reuters story from Stockholm stated the prize that year was being awarded to Sir William Henry Bragg and William Lawrence Bragg \"for their services in the analysis of crystal structure by means of X-rays.\":245 There were unsubstantiated rumors at the time that Tesla and/or Edison had refused the prize.:245 The Nobel Foundation said, \"Any rumor that a person has not been given a Nobel Prize because he has made known his intention to refuse the reward is ridiculous\"; a recipient could only decline a Nobel Prize after he is announced a winner.:245\n",
      "Q: What was the rumored reason Edison and Tesla were not awarded the prize?\n",
      "P: refuse the reward\n",
      "\n",
      "C: On 6 November 1915, a Reuters news agency report from London had the 1915 Nobel Prize in Physics awarded to Thomas Christian and Nikola Tesla; however, on 15 November, a Reuters story from Stockholm stated the prize that year was being awarded to Sir William Henry Bragg and William Lawrence Bragg \"for their services in the analysis of crystal structure by means of X-rays.\":245 There were unsubstantiated rumors at the time that Tesla and/or Christian had refused the prize.:245 The Nobel Foundation said, \"Any rumor that a person has not been given a Nobel Prize because he has made known his intention to refuse the reward is ridiculous\"; a recipient could only decline a Nobel Prize after he is announced a winner.:245\n",
      "Q: What was the rumored reason Christian and Tesla were not awarded the prize?\n",
      "P: he has made known his intention to refuse the reward\n",
      "\n",
      "\n",
      "----\n",
      "C: The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England across the English Channel. This relationship eventually produced closer ties of blood through the marriage of Emma, sister of Duke Richard II of Normandy, and King Ethelred II of England. Because of this, Ethelred fled to Normandy in 1013, when he was forced from his kingdom by Sweyn Forkbeard. His stay in Normandy (until 1016) influenced him and his sons by Emma, who stayed in Normandy after Cnut the Great's conquest of the isle.\n",
      "Q: Who was Emma's brother?\n",
      "P: Duke Richard II of Normandy\n",
      "\n",
      "C: The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England across the English Channel. This relationship eventually produced closer ties of blood through the marriage of Emma, sister of Duke Daniel Campbell of Normandy, and King Ethelred II of England. Because of this, Ethelred fled to Normandy in 1013, when he was forced from his kingdom by Sweyn Forkbeard. His stay in Normandy (until 1016) influenced him and his sons by Emma, who stayed in Normandy after Cnut the Great's conquest of the isle.\n",
      "Q: Who was Emma's brother?\n",
      "P: Duke Daniel Campbell\n",
      "\n",
      "C: The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England across the English Channel. This relationship eventually produced closer ties of blood through the marriage of Emma, sister of Duke David Perez of Normandy, and King Ethelred II of England. Because of this, Ethelred fled to Normandy in 1013, when he was forced from his kingdom by Sweyn Forkbeard. His stay in Normandy (until 1016) influenced him and his sons by Emma, who stayed in Normandy after Cnut the Great's conquest of the isle.\n",
      "Q: Who was Emma's brother?\n",
      "P: Duke David Perez\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Change location everywhere\n",
      "Test cases:      500\n",
      "Fails (rate):    53 (10.6%)\n",
      "\n",
      "Example fails:\n",
      "C: The league announced on October 16, 2012, that the two finalists were Sun Life Stadium and Levi's Stadium. The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010. The San Francisco Bay Area last hosted in 1985 (Super Bowl XIX), held at Stanford Stadium in Stanford, California, won by the home team 49ers. The Miami bid depended on whether the stadium underwent renovations. However, on May 3, 2013, the Florida legislature refused to approve the funding plan to pay for the renovations, dealing a significant blow to Miami's chances.\n",
      "Q: How many times has the South Florida/Miami area hosted the Super Bowl?\n",
      "P: 10 times\n",
      "\n",
      "C: The league announced on October 16, 2012, that the two finalists were Sun Life Stadium and Levi's Stadium. The South Florida/Jersey City area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010. The San Francisco Bay Area last hosted in 1985 (Super Bowl XIX), held at Stanford Stadium in Stanford, California, won by the home team 49ers. The Jersey City bid depended on whether the stadium underwent renovations. However, on May 3, 2013, the Florida legislature refused to approve the funding plan to pay for the renovations, dealing a significant blow to Jersey City's chances.\n",
      "Q: How many times has the South Florida/Jersey City area hosted the Super Bowl?\n",
      "P: 10\n",
      "\n",
      "C: The league announced on October 16, 2012, that the two finalists were Sun Life Stadium and Levi's Stadium. The South Florida/Miami area has previously hosted the event 10 times (tied for most with Lancaster), with the most recent one being Super Bowl XLIV in 2010. The San Francisco Bay Area last hosted in 1985 (Super Bowl XIX), held at Stanford Stadium in Stanford, California, won by the home team 49ers. The Miami bid depended on whether the stadium underwent renovations. However, on May 3, 2013, the Florida legislature refused to approve the funding plan to pay for the renovations, dealing a significant blow to Miami's chances.\n",
      "Q: How many times has the South Florida/Miami area hosted the Super Bowl?\n",
      "P: 10\n",
      "\n",
      "\n",
      "----\n",
      "C: Published comments on Kenya's Capital FM website by Liu Guangyuan, China's ambassador to Kenya, at the time of President Kenyatta's 2013 trip to Beijing, said, \"Chinese investment in Kenya ... reached $474 million, representing Kenya's largest source of foreign direct investment, and ... bilateral trade ... reached $2.84 billion\" in 2012. Kenyatta was \"[a]ccompanied by 60 Kenyan business people [and hoped to] ... gain support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\", according to a statement from the president's office also at the time of the trip. Base Titanium, a subsidiary of Base resources of Australia, shipped its first major consignment of minerals to China. About 25,000 tonnes of ilmenite was flagged off the Kenyan coastal town of Kilifi. The first shipment was expected to earn Kenya about Shs15 – Shs20 Billion in earnings. China has been causing environmental and social problems that include the recent suspension of the railway project.\n",
      "Q: What did the Kenyan business people hope for when meeting with the Chinese?\n",
      "P: gain support\n",
      "\n",
      "C: Published comments on Kenya's Capital FM website by Liu Guangyuan, Croatia's ambassador to Kenya, at the time of President Kenyatta's 2013 trip to Beijing, said, \"Chinese investment in Kenya ... reached $474 million, representing Kenya's largest source of foreign direct investment, and ... bilateral trade ... reached $2.84 billion\" in 2012. Kenyatta was \"[a]ccompanied by 60 Kenyan business people [and hoped to] ... gain support from Croatia for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\", according to a statement from the president's office also at the time of the trip. Base Titanium, a subsidiary of Base resources of Australia, shipped its first major consignment of minerals to Croatia. About 25,000 tonnes of ilmenite was flagged off the Kenyan coastal town of Kilifi. The first shipment was expected to earn Kenya about Shs15 – Shs20 Billion in earnings. Croatia has been causing environmental and social problems that include the recent suspension of the railway project.\n",
      "Q: What did the Kenyan business people hope for when meeting with the Chinese?\n",
      "P: gain support from Croatia for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda\n",
      "\n",
      "C: Published comments on Kenya's Capital FM website by Liu Guangyuan, Comoros's ambassador to Kenya, at the time of President Kenyatta's 2013 trip to Beijing, said, \"Chinese investment in Kenya ... reached $474 million, representing Kenya's largest source of foreign direct investment, and ... bilateral trade ... reached $2.84 billion\" in 2012. Kenyatta was \"[a]ccompanied by 60 Kenyan business people [and hoped to] ... gain support from Comoros for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\", according to a statement from the president's office also at the time of the trip. Base Titanium, a subsidiary of Base resources of Australia, shipped its first major consignment of minerals to Comoros. About 25,000 tonnes of ilmenite was flagged off the Kenyan coastal town of Kilifi. The first shipment was expected to earn Kenya about Shs15 – Shs20 Billion in earnings. Comoros has been causing environmental and social problems that include the recent suspension of the railway project.\n",
      "Q: What did the Kenyan business people hope for when meeting with the Chinese?\n",
      "P: gain support from Comoros for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda\n",
      "\n",
      "\n",
      "----\n",
      "C: Harbor improvements since the late 19th century have made Jacksonville a major military and civilian deep-water port. Its riverine location facilitates two United States Navy bases and the Port of Jacksonville, Florida's third largest seaport. The two US Navy bases, Blount Island Command and the nearby Naval Submarine Base Kings Bay form the third largest military presence in the United States. Significant factors in the local economy include services such as banking, insurance, healthcare and logistics. As with much of Florida, tourism is also important to the Jacksonville area, particularly tourism related to golf. People from Jacksonville may be called \"Jacksonvillians\" or \"Jaxsons\" (also spelled \"Jaxons\").\n",
      "Q: What are those from Jacksonville sometimes called?\n",
      "P: Jacksonvillians\" or \"Jaxsons\n",
      "\n",
      "C: Harbor improvements since the late 19th century have made Kissimmee a major military and civilian deep-water port. Its riverine location facilitates two United States Navy bases and the Port of Kissimmee, Florida's third largest seaport. The two US Navy bases, Blount Island Command and the nearby Naval Submarine Base Kings Bay form the third largest military presence in the United States. Significant factors in the local economy include services such as banking, insurance, healthcare and logistics. As with much of Florida, tourism is also important to the Kissimmee area, particularly tourism related to golf. People from Kissimmee may be called \"Jacksonvillians\" or \"Jaxsons\" (also spelled \"Jaxons\").\n",
      "Q: What are those from Kissimmee sometimes called?\n",
      "P: Jacksonvillians\n",
      "\n",
      "C: Harbor improvements since the late 19th century have made Overland Park a major military and civilian deep-water port. Its riverine location facilitates two United States Navy bases and the Port of Overland Park, Florida's third largest seaport. The two US Navy bases, Blount Island Command and the nearby Naval Submarine Base Kings Bay form the third largest military presence in the United States. Significant factors in the local economy include services such as banking, insurance, healthcare and logistics. As with much of Florida, tourism is also important to the Overland Park area, particularly tourism related to golf. People from Overland Park may be called \"Jacksonvillians\" or \"Jaxsons\" (also spelled \"Jaxons\").\n",
      "Q: What are those from Overland Park sometimes called?\n",
      "P: Jacksonvillians\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fairness\n",
      "\n",
      "M/F failure rates should be similar for different professions\n",
      "Test cases:      2000\n",
      "Fails (rate):    925 (46.2%)\n",
      "\n",
      "Example fails:\n",
      "C: Liam is not a doctor, Tiffany is.\n",
      "Q: Who is a doctor?\n",
      "A: Tiffany\n",
      "P: Liam\n",
      "\n",
      "----\n",
      "C: Henry is not a CEO, Ashley is.\n",
      "Q: Who is a CEO?\n",
      "A: Ashley\n",
      "P: Henry\n",
      "\n",
      "----\n",
      "C: Andrea is not a secretary, Isaac is.\n",
      "Q: Who is a secretary?\n",
      "A: Isaac\n",
      "P: Andrea\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporal\n",
      "\n",
      "There was a change in profession\n",
      "Test cases:      482\n",
      "Fails (rate):    200 (41.5%)\n",
      "\n",
      "Example fails:\n",
      "C: Both Victoria and William were architects, but there was a change in William, who is now a photographer.\n",
      "Q: Who is a photographer?\n",
      "A: William\n",
      "P: William were architects, but there was a change in William\n",
      "\n",
      "\n",
      "----\n",
      "C: Both Amber and Thomas were agents, but there was a change in Thomas, who is now an actress.\n",
      "Q: Who is an actress?\n",
      "A: Thomas\n",
      "P: Thomas were agents, but there was a change in Thomas\n",
      "\n",
      "\n",
      "----\n",
      "C: Both Charles and Lisa were journalists, but there was a change in Charles, who is now an actor.\n",
      "Q: Who is an actor?\n",
      "A: Charles\n",
      "P: Charles and Lisa were journalists, but there was a change in Charles\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Understanding before / after -> first / last.\n",
      "Test cases:      496\n",
      "Fails (rate):    411 (82.9%)\n",
      "\n",
      "Example fails:\n",
      "C: Ethan became a administrator before Danielle did.\n",
      "Q: Who became a administrator last?\n",
      "A: Danielle\n",
      "P: Ethan\n",
      "\n",
      "\n",
      "----\n",
      "C: Tiffany became a assistant before Andrew did.\n",
      "Q: Who became a assistant last?\n",
      "A: Andrew\n",
      "P: Tiffany\n",
      "\n",
      "C: Andrew became a assistant after Tiffany did.\n",
      "Q: Who became a assistant first?\n",
      "A: Tiffany\n",
      "P: Andrew\n",
      "\n",
      "\n",
      "----\n",
      "C: Jeffrey became a assistant before Tiffany did.\n",
      "Q: Who became a assistant last?\n",
      "A: Tiffany\n",
      "P: Jeffrey\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negation\n",
      "\n",
      "Negation in context, may or may not be in question\n",
      "Test cases:      499\n",
      "Fails (rate):    337 (67.5%)\n",
      "\n",
      "Example fails:\n",
      "C: Kevin is not an agent. Jamie is.\n",
      "Q: Who is an agent?\n",
      "A: Jamie\n",
      "P: Kevin\n",
      "\n",
      "\n",
      "----\n",
      "C: Kevin is not an engineer. Tyler is.\n",
      "Q: Who is an engineer?\n",
      "A: Tyler\n",
      "P: Kevin\n",
      "\n",
      "\n",
      "----\n",
      "C: Sophia is not a photographer. Rachel is.\n",
      "Q: Who is a photographer?\n",
      "A: Rachel\n",
      "P: Sophia\n",
      "\n",
      "C: Rachel is a photographer. Sophia is not.\n",
      "Q: Who is not a photographer?\n",
      "A: Sophia\n",
      "P: Rachel\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Negation in question only.\n",
      "Test cases:      481\n",
      "Fails (rate):    481 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Stephanie is an editor. Amy is an academic.\n",
      "Q: Who is not an academic?\n",
      "A: Stephanie\n",
      "P: Amy\n",
      "\n",
      "C: Amy is an academic. Stephanie is an editor.\n",
      "Q: Who is not an editor?\n",
      "A: Amy\n",
      "P: Stephanie\n",
      "\n",
      "C: Amy is an academic. Stephanie is an editor.\n",
      "Q: Who is not an academic?\n",
      "A: Stephanie\n",
      "P: Amy\n",
      "\n",
      "\n",
      "----\n",
      "C: Aaron is an entrepreneur. Samantha is an engineer.\n",
      "Q: Who is not an entrepreneur?\n",
      "A: Samantha\n",
      "P: Aaron\n",
      "\n",
      "C: Aaron is an entrepreneur. Samantha is an engineer.\n",
      "Q: Who is not an engineer?\n",
      "A: Aaron\n",
      "P: Samantha\n",
      "\n",
      "C: Samantha is an engineer. Aaron is an entrepreneur.\n",
      "Q: Who is not an entrepreneur?\n",
      "A: Samantha\n",
      "P: Aaron\n",
      "\n",
      "\n",
      "----\n",
      "C: Brian is a nurse. Laura is a photographer.\n",
      "Q: Who is not a nurse?\n",
      "A: Laura\n",
      "P: Brian\n",
      "\n",
      "C: Brian is a nurse. Laura is a photographer.\n",
      "Q: Who is not a photographer?\n",
      "A: Brian\n",
      "P: Laura\n",
      "\n",
      "C: Laura is a photographer. Brian is a nurse.\n",
      "Q: Who is not a nurse?\n",
      "A: Laura\n",
      "P: Brian\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Coref\n",
      "\n",
      "Basic coref, he / she\n",
      "Test cases:      477\n",
      "Fails (rate):    477 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Adam and Kristin are friends. He is an interpreter, and she is a photographer.\n",
      "Q: Who is an interpreter?\n",
      "A: Adam\n",
      "P: Kristin\n",
      "\n",
      "C: Kristin and Adam are friends. He is an interpreter, and she is a photographer.\n",
      "Q: Who is a photographer?\n",
      "A: Kristin\n",
      "P: Adam\n",
      "\n",
      "C: Adam and Kristin are friends. She is a photographer, and he is an interpreter.\n",
      "Q: Who is an interpreter?\n",
      "A: Adam\n",
      "P: Kristin\n",
      "\n",
      "\n",
      "----\n",
      "C: Destiny and Jason are friends. He is an economist, and she is an auditor.\n",
      "Q: Who is an auditor?\n",
      "A: Destiny\n",
      "P: Jason\n",
      "\n",
      "C: Jason and Destiny are friends. She is an auditor, and he is an economist.\n",
      "Q: Who is an economist?\n",
      "A: Jason\n",
      "P: Destiny\n",
      "\n",
      "C: Destiny and Jason are friends. She is an auditor, and he is an economist.\n",
      "Q: Who is an auditor?\n",
      "A: Destiny\n",
      "P: Jason\n",
      "\n",
      "\n",
      "----\n",
      "C: April and Jacob are friends. He is an escort, and she is an auditor.\n",
      "Q: Who is an auditor?\n",
      "A: April\n",
      "P: Jacob\n",
      "\n",
      "C: Jacob and April are friends. She is an auditor, and he is an escort.\n",
      "Q: Who is an escort?\n",
      "A: Jacob\n",
      "P: April\n",
      "\n",
      "C: Jacob and April are friends. She is an auditor, and he is an escort.\n",
      "Q: Who is an auditor?\n",
      "A: April\n",
      "P: Jacob\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Basic coref, his / her\n",
      "Test cases:      500\n",
      "Fails (rate):    459 (91.8%)\n",
      "\n",
      "Example fails:\n",
      "C: Alexander and Angela are friends. His mom is a nurse.\n",
      "Q: Whose mom is a nurse?\n",
      "A: Alexander\n",
      "P: Alexander and Angela\n",
      "\n",
      "\n",
      "----\n",
      "C: Jared and Abigail are friends. His mom is an auditor.\n",
      "Q: Whose mom is an auditor?\n",
      "A: Jared\n",
      "P: Jared and Abigail\n",
      "\n",
      "\n",
      "----\n",
      "C: Juan and Grace are friends. His mom is an architect.\n",
      "Q: Whose mom is an architect?\n",
      "A: Juan\n",
      "P: Juan and Grace\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Former / Latter\n",
      "Test cases:      475\n",
      "Fails (rate):    475 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "C: Lisa and Mary are friends. The former is an executive.\n",
      "Q: Who is an executive?\n",
      "A: Lisa\n",
      "P: Mary\n",
      "\n",
      "C: Lisa and Mary are friends. The former is an executive and the latter is an assistant.\n",
      "Q: Who is an executive?\n",
      "A: Lisa\n",
      "P: Mary\n",
      "\n",
      "\n",
      "----\n",
      "C: Jonathan and William are friends. The former is a producer.\n",
      "Q: Who is a producer?\n",
      "A: Jonathan\n",
      "P: William\n",
      "\n",
      "C: Jonathan and William are friends. The former is a producer and the latter is an interpreter.\n",
      "Q: Who is a producer?\n",
      "A: Jonathan\n",
      "P: Jonathan and William\n",
      "\n",
      "\n",
      "----\n",
      "C: Ashley and Anthony are friends. The former is an interpreter.\n",
      "Q: Who is an interpreter?\n",
      "A: Ashley\n",
      "P: Anthony\n",
      "\n",
      "C: Ashley and Anthony are friends. The former is an interpreter and the latter is an assistant.\n",
      "Q: Who is an interpreter?\n",
      "A: Ashley\n",
      "P: Anthony\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SRL\n",
      "\n",
      "Agent / object distinction\n",
      "Test cases:      497\n",
      "Fails (rate):    302 (60.8%)\n",
      "\n",
      "Example fails:\n",
      "C: James accepts Natalie.\n",
      "Q: Who is accepted?\n",
      "A: Natalie\n",
      "P: James\n",
      "\n",
      "\n",
      "----\n",
      "C: Charles dislikes Mary.\n",
      "Q: Who is disliked?\n",
      "A: Mary\n",
      "P: Charles\n",
      "\n",
      "\n",
      "----\n",
      "C: Jamie hurts Maria.\n",
      "Q: Who is hurt?\n",
      "A: Maria\n",
      "P: Jamie\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Agent / object distinction with 3 agents\n",
      "Test cases:      483\n",
      "Fails (rate):    462 (95.7%)\n",
      "\n",
      "Example fails:\n",
      "C: Eric likes Alexander. James is liked by Alexander.\n",
      "Q: Who likes James?\n",
      "A: Alexander\n",
      "P: Eric\n",
      "\n",
      "C: Alexander is liked by Eric. Alexander likes James.\n",
      "Q: Who is liked by Alexander?\n",
      "A: James\n",
      "P: Eric\n",
      "\n",
      "C: Alexander is liked by Eric. James is liked by Alexander.\n",
      "Q: Who is liked by Alexander?\n",
      "A: James\n",
      "P: Eric\n",
      "\n",
      "\n",
      "----\n",
      "C: Andrea is supported by Christina. Andrea supports Austin.\n",
      "Q: Who is supported by Andrea?\n",
      "A: Austin\n",
      "P: Christina\n",
      "\n",
      "\n",
      "----\n",
      "C: Thomas notices Michelle. Michelle notices Jeremy.\n",
      "Q: Who notices Jeremy?\n",
      "A: Michelle\n",
      "P: Thomas\n",
      "\n",
      "C: Thomas notices Michelle. Jeremy is noticed by Michelle.\n",
      "Q: Who notices Jeremy?\n",
      "A: Michelle\n",
      "P: Thomas\n",
      "\n",
      "C: Thomas notices Michelle. Jeremy is noticed by Michelle.\n",
      "Q: Who is noticed by Thomas?\n",
      "A: Michelle\n",
      "P: Jeremy\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.suite.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
