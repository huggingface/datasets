{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import checklist as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_path = '/home/marcotcr/work/checklist/release_data/sentiment/sentiment_suite.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tell `nlp` how to turn the data in a checklist into a map, which we do in the second argument.  \n",
    "I'll just add this function to the checklist pickle file for my test suites, making the argument optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = cl.CheckListSuite(suite_path, lambda x: {'tweet': x })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading predictions from other models (which I have saved).  \n",
    "I'm assuming that people would want to add predictions to `suite.dataset`, similar to the examples in [here](https://huggingface.co/nlp/processing.html#processing-data-row-by-row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c470b614dc1407d889e355f7e2064c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c31991c8e3d40f7acbc433611d684b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c762c84c7c7c40249f55f3fd1e00d3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fd22bac1e34671aa548db7f8c2648e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b524ce13968b4fb2afb21791501a2209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = ['microsoft', 'google', 'amazon', 'bert', 'roberta']\n",
    "for model in models:\n",
    "    preds = open('/home/marcotcr/work/checklist/release_data/sentiment/predictions/%s' % model).read().splitlines()\n",
    "    confs = [list(map(float, (x.split()[1:]))) for x in preds]\n",
    "    preds = [int(x.split()[0]) for x in preds]\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.dataset = suite.dataset.map(lambda _, idx: {model: preds[idx], conf_key: confs[idx]}, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the predictions, we call `suite.compute` to compute test results for each model.  \n",
    "The second argument is optional, but many tests depend on having a confidence score to check for monotonicity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486df3afeeb94cdcb7cec85b7556fb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700fd4e3734f42508ecc89aa077ee204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e519a4abfb7d42818ec10d032ff34b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4efa334b92641be8dd4ae24974bd534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a81a7607f0455b8c982153cd6e1bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    conf_key = '%s_conf' % model\n",
    "    suite.compute(model, conf_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tests in this suite than I had in the paper, but let's pretend I want to replicate table 1 of the [checklist paper](https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf).  \n",
    "What I would do is look at `suite.fail_rate` (I can change this name to `suite.results`, or whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_table1 =  [\n",
    "'neutral words in context',\n",
    "'Sentiment-laden words in context',\n",
    "'change neutral words with BERT',\n",
    "'add positive phrases',\n",
    "'add negative phrases',\n",
    "'add random urls and handles',\n",
    "'typos',\n",
    "'change locations',\n",
    "'change names',\n",
    "'used to, but now',\n",
    "'simple negations: not negative',\n",
    "'simple negations: not neutral is still neutral',\n",
    "'simple negations: I thought x was negative, but it was not (should be neutral or positive)',\n",
    "'Hard: Negation of positive with neutral stuff in the middle (should be negative)',\n",
    "'my opinion is what matters',\n",
    "'Q & A: yes',\n",
    "'Q & A: no',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro googl amazo bert rober hf_pi\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'microsoft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-eacd362aab69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecklist_table1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%5.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-243-eacd362aab69>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecklist_table1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%5.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'microsoft'"
     ]
    }
   ],
   "source": [
    "print (' '.join([x[:5] for x in models]))\n",
    "for t in checklist_table1:\n",
    "    r = ' '.join(['%5.1f' % (suite.fail_rate[m][t]) for m in models])\n",
    "    print('%s %s' % (r, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose I want to compare the pipeline in transformers to these models.  \n",
    "This test suite assumes the labels are [negative, neutral, positive], so we have to do some converting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(\"sentiment-analysis\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pred_and_conf(data):\n",
    "    # change format to softmax, make everything in [0.33, 0.66] range be predicted as neutral\n",
    "    preds = model(data)\n",
    "    pr = np.array([x['score'] if x['label'] == 'POSITIVE' else 1 - x['score'] for x in preds])\n",
    "    pp = np.zeros((pr.shape[0], 3))\n",
    "    margin_neutral = 1/3.\n",
    "    mn = margin_neutral / 2.\n",
    "    neg = pr < 0.5 - mn\n",
    "    pp[neg, 0] = 1 - pr[neg]\n",
    "    pp[neg, 2] = pr[neg]\n",
    "    pos = pr > 0.5 + mn\n",
    "    pp[pos, 0] = 1 - pr[pos]\n",
    "    pp[pos, 2] = pr[pos]\n",
    "    neutral_pos = (pr >= 0.5) * (pr < 0.5 + mn)\n",
    "    pp[neutral_pos, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_pos] - 0.5)\n",
    "    pp[neutral_pos, 2] = 1 - pp[neutral_pos, 1]\n",
    "    neutral_neg = (pr < 0.5) * (pr > 0.5 - mn)\n",
    "    pp[neutral_neg, 1] = 1 - (1 / margin_neutral) * np.abs(pr[neutral_neg] - 0.5)\n",
    "    pp[neutral_neg, 0] = 1 - pp[neutral_neg, 1]\n",
    "    preds = np.argmax(pp, axis=1)\n",
    "    return preds, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddebc25dff046fdacd2533656df2208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_pipeline(x):\n",
    "    preds, confs = pred_and_conf(x['tweet'])\n",
    "    return {'hf_pipeline': preds, 'hf_pipeline_conf': confs}\n",
    "suite.dataset = suite.dataset.map(add_pipeline , batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85c21a88ece430983dd26e19e0d1174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suite.compute('hf_pipeline', 'hf_pipeline_conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append('hf_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro googl amazo bert rober hf_pi\n",
      "  0.0   7.6   4.8  94.6  81.8  95.8 neutral words in context\n",
      "  4.0  15.0   2.8   0.0   0.2   0.8 Sentiment-laden words in context\n",
      "  9.4  16.2  12.4  10.2  10.2   9.8 change neutral words with BERT\n",
      " 12.6  12.4   1.4   0.2  10.2   0.0 add positive phrases\n",
      "  0.8  34.6   5.0   0.0  13.2   6.8 add negative phrases\n",
      "  9.6  13.4  24.8  11.4   7.4  15.4 add random urls and handles\n",
      "  5.6  10.2  10.4   5.2   3.8   6.6 typos\n",
      "  7.0  20.8  14.8   7.6   6.4  10.0 change locations\n",
      "  2.4  15.1   9.1   6.6   2.4   5.1 change names\n",
      " 41.0  36.6  42.2  18.8  11.0  32.6 used to, but now\n",
      " 18.8  54.2  29.4  13.2   2.6  12.8 simple negations: not negative\n",
      " 40.4  39.6  74.2  98.4  95.4  97.4 simple negations: not neutral is still neutral\n",
      "100.0  90.4 100.0  84.8   7.2 100.0 simple negations: I thought x was negative, but it was not (should be neutral or positive)\n",
      " 98.4 100.0 100.0  74.0  30.2  86.8 Hard: Negation of positive with neutral stuff in the middle (should be negative)\n",
      " 45.4  62.4  68.0  38.8  30.0  44.2 my opinion is what matters\n",
      "  9.0  57.6  20.8   3.6   3.0   0.8 Q & A: yes\n",
      " 96.8  90.8  81.6  55.4  54.8  85.8 Q & A: no\n"
     ]
    }
   ],
   "source": [
    "print (' '.join([x[:5] for x in models]))\n",
    "for t in checklist_table1:\n",
    "    r = ' '.join(['%5.1f' % (suite.fail_rate[m][t]) for m in models])\n",
    "    print('%s %s' % (r, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using marcotcr/checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can access my package's object if they want to use it, with the caveat that it doesn't really allow for model comparison (it only keeps the state of the last model we called `compute` on).  \n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checklist.test_suite.TestSuite"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(suite.suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No results. Run run() first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-d7206b97afd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment-laden words in context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/checklist/checklist/abstract_test.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, n, print_fn, format_example_fn, n_per_testcase)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mMaximum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0mto\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/checklist/checklist/abstract_test.py\u001b[0m in \u001b[0;36mprint_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test cases:      %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestcases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'testcases_run'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/checklist/checklist/abstract_test.py\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mn_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_idxs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/checklist/checklist/abstract_test.py\u001b[0m in \u001b[0;36m_check_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No results. Run run() first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_create_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No results. Run run() first"
     ]
    }
   ],
   "source": [
    "suite.suite.tests['Sentiment-laden words in context'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.summary() # calls suite.suite.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sugar on nlp.checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of a certain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e30b969994f14b17d6af9d2d9f62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "simple = suite.get_test('Sentiment-laden words in context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I valued the flight.',\n",
       " 'That is a sad customer service.',\n",
       " 'We like the flight.',\n",
       " 'This was a nice crew.',\n",
       " 'We abhor that flight.',\n",
       " 'This staff is difficult.',\n",
       " 'I valued that aircraft.',\n",
       " 'This was a fantastic flight.',\n",
       " 'I hate the food.',\n",
       " 'I despised the food.']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple['tweet'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering by examples where google fails and hf does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598692dd8f474770bb078215da5e0046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "google_fails_hf_doesnt = simple.filter(lambda x:x['fail']['hf_pipeline'] == 0 and x['fail']['google'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF:negative GOOGLE:neutral  That is a lame service.\n",
      "HF:negative GOOGLE:positive That is a difficult pilot.\n",
      "HF:negative GOOGLE:neutral  The pilot is creepy.\n",
      "HF:positive GOOGLE:neutral  We like this company.\n",
      "HF:negative GOOGLE:neutral  The seat is hard.\n"
     ]
    }
   ],
   "source": [
    "mapz = ['negative', 'neutral', 'positive']\n",
    "for x in np.random.choice(google_fails_hf_doesnt.shape[0], 5):\n",
    "    x = google_fails_hf_doesnt[int(x)]\n",
    "    print('HF:%-8s GOOGLE:%-8s %s' % (mapz[x['hf_pipeline']], mapz[x['google']], x['tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbation tests combine multiple examples, so looking at a single row in the dataset would not give us a good picture.  \n",
    "Instead, we want to aggregate each testcase into a row of examples (data goes into the `data` key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613b0c3b14754409b0589f27dcc0afeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "perturbation = suite.get_test('change locations', aggregate_testcases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a418c9574ac49b9a6ee9266d715e1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "any_fails = lambda x, key: any([y['fail'][key] for y in x['data']])\n",
    "google_fails_hf_doesnt = perturbation.filter(lambda x: not any_fails(x, 'hf_pipeline') and any_fails(x, 'google'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF:negative GOOGLE:positive @SouthwestAir if only you could control the weather in Las Vegas ðŸ˜‰\n",
      "HF:negative GOOGLE:neutral  @SouthwestAir if only you could control the weather in Farmington Hills ðŸ˜‰\n",
      "\n",
      "HF:positive GOOGLE:positive @united stay warm - I will be passing through Chicago next week\n",
      "HF:positive GOOGLE:neutral  @united stay warm - I will be passing through Manchester next week\n",
      "\n",
      "HF:negative GOOGLE:neutral  @AmericanAir you really need some customer service training for your unhappy EEs in the morning in Chicago. Gate K20 at 430 chking her schd\n",
      "HF:negative GOOGLE:negative @AmericanAir you really need some customer service training for your unhappy EEs in the morning in Plainfield. Gate K20 at 430 chking her schd\n",
      "\n",
      "HF:negative GOOGLE:neutral  @JetBlue Are there really no flights from the Bay Area to Chicago anymore? Lame. So lame.\n",
      "HF:negative GOOGLE:negative @JetBlue Are there really no flights from the Bay Area to Urbana anymore? Lame. So lame.\n",
      "\n",
      "HF:positive GOOGLE:neutral  @USAirways  has me on my toes whether I'm going to make my flight back to Boston tonight\n",
      "HF:positive GOOGLE:positive @USAirways  has me on my toes whether I'm going to make my flight back to Rancho Cordova tonight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapz = ['negative', 'neutral', 'positive']\n",
    "for x in np.random.choice(google_fails_hf_doesnt.shape[0], 5):\n",
    "    x = google_fails_hf_doesnt[int(x)]\n",
    "    orig = x['data'][0]\n",
    "    fail = [y for y in x['data'] if y['fail']['google']][0]\n",
    "    print('HF:%-8s GOOGLE:%-8s %s' % (mapz[orig['hf_pipeline']], mapz[orig['google']], orig['tweet']))\n",
    "    print('HF:%-8s GOOGLE:%-8s %s' % (mapz[fail['hf_pipeline']], mapz[fail['google']], fail['tweet']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_path = '/home/marcotcr/work/checklist/release_data/squad/squad_suite.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = cl.CheckListSuite(suite_path, lambda x: {'context': x[0], 'question': x[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preds = open('/home/marcotcr/work/checklist/release_data/squad/predictions/bert').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc0f92dda194610a87ab45d1a08b4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suite.dataset = suite.dataset.map(lambda _, idx: {'bert': bert_preds[idx]}, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9608f244dc94bb58913c23847d0a2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suite.compute('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_table3 = [\n",
    "    'A is COMP than B. Who is more / less COMP?',\n",
    "    'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?',\n",
    "    'size, shape, age, color',\n",
    "    'Profession vs nationality',\n",
    "    'Animal vs Vehicle v2',\n",
    "    'A is COMP than B. Who is antonym(COMP)? B',\n",
    "    'A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.',\n",
    "    'Question typo',\n",
    "    'Add random sentence to context',\n",
    "    'There was a change in profession',\n",
    "    'Understanding before / after -> first / last.',\n",
    "    'Negation in context, may or may not be in question',\n",
    "    'Negation in question only.', 'M/F failure rates should be similar for different professions',\n",
    "    'Basic coref, he / she',\n",
    "    'Basic coref, his / her',\n",
    "    'Former / Latter',\n",
    "    'Agent / object distinction',\n",
    "    'Agent / object distinction with 3 agents'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0 A is COMP than B. Who is more / less COMP?\n",
      "91.3 Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?\n",
      "82.4 size, shape, age, color\n",
      "49.4 Profession vs nationality\n",
      "26.2 Animal vs Vehicle v2\n",
      "67.3 A is COMP than B. Who is antonym(COMP)? B\n",
      "100.0 A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.\n",
      "11.6 Question typo\n",
      "9.8 Add random sentence to context\n",
      "41.5 There was a change in profession\n",
      "82.9 Understanding before / after -> first / last.\n",
      "67.5 Negation in context, may or may not be in question\n",
      "100.0 Negation in question only.\n",
      "46.2 M/F failure rates should be similar for different professions\n",
      "100.0 Basic coref, he / she\n",
      "91.8 Basic coref, his / her\n",
      "100.0 Former / Latter\n",
      "60.8 Agent / object distinction\n",
      "95.7 Agent / object distinction with 3 agents\n"
     ]
    }
   ],
   "source": [
    "for t in checklist_table3:\n",
    "    print('%.1f %s' % (suite.fail_rate['bert'][t], t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Put some data into `suite.dataset.info`, so people know what the predictions and confidences should look like (e.g. [0, 1, 2] for sentiment, or string for SQuAD)\n",
    "- Warn people that CheckList suites contained pickled functions, which may not be safe\n",
    "- Write some documentation for `nlp.checklist`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checklist",
   "language": "python",
   "name": "checklist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
