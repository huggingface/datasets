# Depth estimation

Depth estimation datasets are used to train a model to approximate the relative distance of every pixel in an
image from the camera aka depth. The applications enabled by these datasets mostly lie in areas like visual machine
perception and perception in robotics. This guide will show you how to apply transformations
to a depth estimation dataset.

Before you start, make sure you have up-to-date versions of `albumentations` installed:

```bash
pip install -U albumentations 
```

[Albumentations](https://albumentations.ai/) is a Python library for performing data augmentation
for computer vision. It supports various computer vision tasks such as image classification, object
detection, segmentation, and keypoint estimation.

This guide uses the [NYU Depth V2](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2) dataset which is 
comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras
from the Microsoft [Kinect](http://www.xbox.com/kinect). The dataset consists of scenes from 3 cities and provides images along with
their depth maps as labels.

Load the `train` split of the dataset and take a look at an example:

```py
>>> from datasets import load_dataset

>>> train_dataset = load_dataset("sayakpaul/nyu_depth_v2", split="train")
>>> index = 17
>>> dataset[index]
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=640x480>,
 'depth_map': <PIL.PngImagePlugin.PngImageFile image mode=L size=640x480>}
```

The dataset has three fields:

* `image`: a PIL image object.
* `depth_map`: depth map of the image.

Next, check out an image with:

```py
>>> train_dataset[index]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_sample.png">
</div>

Similarly, you can check out the respective depth map:

```py
>>> train_dataset[index]["depth_map"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target.png">
</div>

It's all black! You need to apply some preprocessing to the depth map to properly visualize it. 

```py 
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> cmap = plt.cm.viridis

>>> def colored_depthmap(depth, d_min=None, d_max=None):
...     if d_min is None:
...         d_min = np.min(depth)
...     if d_max is None:
...         d_max = np.max(depth)
...     depth_relative = (depth - d_min) / (d_max - d_min)
...     return 255 * cmap(depth_relative)[:,:,:3]
```

**Note** that the above utility is taken from the [FastDepth repository](https://github.com/dwofk/fast-depth/blob/master/utils.py). You can now visualize the depth map.

```py 
>>> depth_target = train_dataset[index]["depth_map"]
>>> depth_target = np.squeeze(np.array(depth_target))

>>> d_min = np.min(depth_target)
>>> d_max = np.max(depth_target)
>>> depth_target = colored_depthmap(depth_target, d_min, d_max)

>>> plt.imshow(depth_target.astype("uint8"))
>>> plt.axis("off")
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target_viz.png">
</div>

Now apply some augmentations with `albumentations`. The augmentation transformations include:

* Random horizontal flipping
* Random cropping 
* Random brightness and contrast 
* Random gamma correction 
* Random hue saturation

```py 
>>> import albumentations as A

>>> crop_size = (448, 576)
>>> transforms = [
...     A.HorizontalFlip(),
...     A.RandomCrop(crop_size[0], crop_size[1]),
...     A.RandomBrightnessContrast(),
...     A.RandomGamma(),
...     A.HueSaturationValue()
... ]
```

Since these transformations (such as flipping, cropping) also impact the depth maps, you need to define a mapping
while applying these transformations:

```py 
>>> additional_targets = {"depth": "mask"}
>>> aug = A.Compose(transforms=transforms, additional_targets=additional_targets)
```

This will ensure that you can pass an image and its corresponding depth map to `aug`.

Create a function to apply the transformation to the images as well as their depth maps:

```py 
>>> def transforms(examples):
...     transformed_images, transformed_maps = [], []
...     for image, depth_map in zip(examples["image"], examples["depth_map"]):
...         image, depth_map = np.array(image), np.array(depth_map)
...         transformed = aug(image=image, depth=depth_map)
...         transformed_images.append(transformed["image"])
...         transformed_maps.append(transformed["depth"])
...
...     examples["pixel_values"] = transformed_images
...     examples["labels"] = transformed_maps
...     return examples
```

Use the [`~Dataset.set_transform`] function to apply the transformation on-the-fly to batches of the dataset to consume less disk space:

```py
>>> train_dataset.set_transform(transforms)
```

You can verify the transformation worked by indexing into the `pixel_values` and `labels` of an example:

```py
>>> image = np.array(train_dataset[index]["pixel_values"])
>>> depth_map = np.array(train_dataset[index]["labels"])
```

```py
>>> plt.imshow(image)
>>> plt.axis("off")
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_sample_aug.png">
</div>

```py 
>>> d_min = np.min(depth_map)
>>> d_max = np.max(depth_map)
>>> depth_map = colored_depthmap(depth_map, d_min, d_max)

>>> plt.imshow(depth_map.astype("uint8"))
>>> plt.axis("off")
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target_aug.png">
</div>

You can also visualize multiple training samples: 

```py 
>>> random_indices = np.random.choice(len(train_dataset), 9).tolist()

>>> plt.figure(figsize=(15, 6))

>>> for i, idx in enumerate(random_indices):
...     ax = plt.subplot(3, 3, i + 1)
...     image_viz = merge_into_row(
...         train_dataset[idx]["pixel_values"], train_dataset[idx]["labels"]
...     )
...     plt.imshow(image_viz.astype("uint8"))
...     plt.axis("off")
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_collage.png">
</div>