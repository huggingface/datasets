# Share an audio dataset

You can share a dataset with your team or with anyone in the community by creating a dataset repository on the Hugging Face Hub:

```py
from datasets import load_dataset

dataset = load_dataset("<username>/my_dataset")
```

There are several methods for sharing an audio dataset:

1. Share an audio dataset from local files in python with [`Dataset.push_to_hub`]. This the recommended way and requires only a few steps in python.

1. Share an audio dataset repository with the `AudioFolder` format with metadata. This is a no-code solution for quickly creating small dataset to experiment with.

1. Share an audio dataset by writing a loading script. This method is for advanced users and requires more effort and coding, but you have greater flexibility over how a dataset is defined, downloaded, and generated.


<Tip>

You can control access to your dataset by requiring users to share their contact information first. Check out the [Gated datasets](https://huggingface.co/docs/hub/datasets-gated) guide for more information about how to enable this feature on the Hub.

</Tip>

## Local files

You can load your own dataset using the paths to your audio files. Use the [`~Dataset.cast_column`] function to take a column of audio file paths, and cast it to the [`Audio`] feature:

```py
>>> audio_dataset = Dataset.from_dict({"audio": ["path/to/audio_1", "path/to/audio_2", ..., "path/to/audio_n"]}).cast_column("audio", Audio())
>>> audio_dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': 'path/to/audio_1',
 'sampling_rate': 16000}
```

Then upload the dataset to the Hugging Face Hub using [`Dataset.push_to_hub`]:

```py
audio_dataset.push_to_hub("<username>/my_dataset")
```

This will create a dataset repository containing your audio dataset:

```
my_dataset/
 README.md
 data/
     train-00000-of-00001.parquet
```

## AudioFolder

The `AudioFolder` is a dataset builder designed to quickly load an audio dataset without requiring you to write any code.
Any additional information about your dataset - such as transcription, speaker accent, or speaker intent - is automatically loaded by `AudioFolder` as long as you include this information in a metadata file (`metadata.csv`/`metadata.jsonl`). 

Create a dataset repository on the Hugging Face Hub and upload your dataset directory following the `AudioFolder` structure:

```
my_dataset/
 README.md
 metadata.csv
 data/
```

The `data` folder can be any name you want, it doesn't have to be `data`.


<Tip>

It can be helpful to store your metadata as a `jsonl` file if the data columns contain a more complex format (like a list of floats) to avoid parsing errors or reading complex values as strings.

</Tip>

The metadata file should include a `file_name` column to link an audio file to it's metadata:

```csv
file_name,transcription
first_audio_file.mp3,znowu si duch z ciaem zronie w modocianej wstaniesz wiosnie i mo偶esz skutkiem tych lek贸w umiera wstawa wiek wiek贸w dalej tam byy przestrogi jak sieka gow jak nogi
second_audio_file.mp3,ju偶 u 藕wierzyca podwoj贸w kr贸l zasiada przy nim ksi偶ta i panowie rada a gdzie wzniosy kr偶y ganek rycerze obok kochanek kr贸l skin palcem zaczto igrzysko
third_audio_file.mp3,pewnie kdy w obdzie ubite miny szlaki zaczekajmy dzie jaki polemy szuka wszdzie dzi jutro pewnie bdzie posali wszdzie sugi czekali dzie i drugi gdy nic nie doczekali z paczem chc jecha dali
```

Then you can store your dataset in a directory structure like this:

```
data/train/metadata.csv
data/train/first_audio_file.mp3
data/train/second_audio_file.mp3
data/train/third_audio_file.mp3
```

Users can now load your dataset and the associated metadata by specifying `audiofolder` in [`load_dataset`] and the dataset directory in `data_dir`:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("audiofolder", data_dir="/path/to/folder")
>>> dataset["train"][0]
{'audio':
    {'path': '/path/to/extracted/audio/first_audio_file.mp3',
    'array': array([ 0.00088501,  0.0012207 ,  0.00131226, ..., -0.00045776, -0.00054932, -0.00054932], dtype=float32),
    'sampling_rate': 16000},
 'transcription': 'znowu si duch z ciaem zronie w modocianej wstaniesz wiosnie i mo偶esz skutkiem tych lek贸w umiera wstawa wiek wiek贸w dalej tam byy przestrogi jak sieka gow jak nogi'
}
```

For audio datasets that don't have any associated metadata, `AudioFolder` automatically infers the class labels of the dataset based on the directory name. In this case, your dataset directory might look like:

```
data/train/en_us/14223464699748513050.wav
data/train/en_us/363601479976099198.wav
data/train/en_us/9644097927007796370.wav

data/train/fr_fr/14945328524472190788.wav
```

### Language identification

Language identification datasets have audio recordings of speech in multiple languages:

```
data/train/ar/0197_720_0207_190.wav
data/train/ar/0179_830_0185_540.mp3
data/train/ar/0179_830_0185_540.mp3

data/train/zh/0442_690_0454_380.mp3
```

Load the dataset with `AudioFolder`, and it will create a `label` column from the directory name (language id):

```py
>>> dataset = load_dataset("audiofolder", data_dir="/path/to/folder", drop_labels=False)
>>> dataset["train"][0]
{'audio':
    {'path': '/path/to/extracted/audio/0197_720_0207_190.mp3',
    'array': array([-3.6621094e-04, -6.1035156e-05,  6.1035156e-05, ..., -5.1879883e-04, -1.0070801e-03, -7.6293945e-04],
    'sampling_rate': 16000}
 'label': 0  # "ar"
}
```

## (Advanced) Loading script

Write a dataset loading script to manually create a dataset.
It defines a dataset's splits and configurations, and handles downloading and generating the dataset examples.
The script should have the same name as your dataset folder or repository:

```
my_dataset/
 README.md
 my_dataset.py
 data/
```

<Tip>

The `data` folder can be any name you want, it doesn't have to be `data`! This folder is optional, unless you're hosting your dataset on the Hub.

</Tip>

This directory structure allows your dataset to be loaded in one line:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("path/to/my_dataset")
```

This guide will show you how to create a dataset loading script for audio datasets, which is a bit different from <a class="underline decoration-green-400 decoration-2 font-semibold" href="./dataset_script">creating a loading script for text datasets</a>. Audio datasets are commonly stored in `tar.gz` archives which requires a separate approach to support streaming mode. While streaming is not required, we highly encourage enabling streaming support in your audio dataset because:

1. Users without a lot of disk space can use your dataset without waiting for the entire dataset to be downloaded. Learn more about streaming in the [Stream](./stream) guide!
2. Users can preview a dataset in the dataset viewer.

In addition to learning how to create a streamable dataset, you'll also learn how to:

* Create a dataset builder class.
* Create dataset configurations.
* Add dataset metadata.
* Download and define the dataset splits.
* Generate the dataset.
* Generate the dataset metadata (optional).
* Upload the dataset to the Hub.

The best way to learn is to open up an existing audio dataset loading script, like [LibriVox Indonesia](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia), and follow along!

<Tip>

To help you get started, we created a loading script [template](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py) you can copy and use as a starting point!

</Tip>

### Create a dataset builder class

[`GeneratorBasedBuilder`] is the base class for datasets generated from a dictionary generator. Within this class, there are three methods to help create your dataset:

* `info` stores information about your dataset like its description, license, and features.
* `split_generators` downloads the dataset and defines its splits.
* `generate_examples` generates the dataset's samples containing the audio data and other features specified in `info` for each split.

Start by creating your dataset class as a subclass of [`GeneratorBasedBuilder`] and add the three methods. Don't worry about filling in each of these methods yet, you'll develop those over the next few sections:

```py
class LibriVoxIndonesia(datasets.GeneratorBasedBuilder):

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, local_extracted_archive, audio_files, metadata_path, path_to_clips,):
```

#### Multiple configurations

In some cases, a dataset may have more than one configuration. The Vivos dataset has several configurations corresponding to different languages.

To create different configurations, use the [`BuilderConfig`] class to create a subclass of your dataset. The only required parameter is the `name` of the configuration, which must be passed to the configuration's superclass `__init__()`. Otherwise, you can specify any custom parameters you want in your configuration class.

```py
class LibriVoxIndonesiaConfig(datasets.BuilderConfig):
    """BuilderConfig for LibriVoxIndonesia."""

    def __init__(self, name, version, **kwargs):
        self.language = kwargs.pop("language", None)
        self.release_date = kwargs.pop("release_date", None)
        self.num_clips = kwargs.pop("num_clips", None)
        self.num_speakers = kwargs.pop("num_speakers", None)
        self.validated_hr = kwargs.pop("validated_hr", None)
        self.total_hr = kwargs.pop("total_hr", None)
        self.size_bytes = kwargs.pop("size_bytes", None)
        self.size_human = size_str(self.size_bytes)
        description = (
            f"LibriVox-Indonesia speech to text dataset in {self.language} released on {self.release_date}. "
            f"The dataset comprises {self.validated_hr} hours of transcribed speech data"
        )
        super(LibriVoxIndonesiaConfig, self).__init__(
            name=name,
            version=datasets.Version(version),
            description=description,
            **kwargs,
        )
```

Define your configurations in the `BUILDER_CONFIGS` class variable inside [`GeneratorBasedBuilder`]. In this example, the author imports the languages from a separate `release_stats.py` [file](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/blob/main/release_stats.py) from their repository, and then loops through each language to create a configuration:

```py
class LibriVoxIndonesiaConfig(datasets.GeneratorBasedBuilder):
    DEFAULT_CONFIG_NAME = "all"

    BUILDER_CONFIGS = [
        LibriVoxIndonesiaConfig(
            name=lang,
            version=STATS["version"],
            language=LANGUAGES[lang],
            release_date=STATS["date"],
            num_clips=lang_stats["clips"],
            num_speakers=lang_stats["users"],
            total_hr=float(lang_stats["totalHrs"]) if lang_stats["totalHrs"] else None,
            size_bytes=int(lang_stats["size"]) if lang_stats["size"] else None,
        )
        for lang, lang_stats in STATS["locales"].items()
    ]
```

<Tip>

Typically, users need to specify a configuration to load in [`load_dataset`], otherwise a `ValueError` is raised. You can avoid this by setting a default dataset configuration to load in `DEFAULT_CONFIG_NAME`. 

</Tip>

Now if users want to load the Balinese (`bal`) configuration, they can use the configuration name:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("indonesian-nlp/librivox-indonesia", "bal", split="train")
```

### Add dataset metadata

Adding information about your dataset helps users learn more about it. This information is stored in the [`DatasetInfo`] class which is returned by the `info` method. Users can access this information by:

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder("indonesian-nlp/librivox-indonesia")
>>> ds_builder.info
```

There is a lot of information you can include about your dataset, but some important ones are:

1. `description` provides a concise description of the dataset.
2. `features` specify the dataset column types. Since you're creating an audio loading script, you'll need to include the [`Audio`] feature and the `sampling_rate` of the dataset.
3. `homepage` provides a link to the dataset homepage.
4. `license` specify the permissions for using a dataset as defined by the license type.
5. `citation` is a BibTeX citation of the dataset.

<Tip>

You'll notice a lot of the dataset information is defined earlier in the loading script which can make it easier to read. There are also other [`~Datasets.Features`] you can input, so be sure to check out the full list for more details.

</Tip>

```py
def _info(self):
    total_languages = len(STATS["locales"])
    total_hours = self.config.total_hr
    description = (
        "LibriVox-Indonesia is a speech dataset generated from LibriVox with only languages from Indonesia."
        f"The dataset currently consists of {total_hours} hours of speech "
        f" in {total_languages} languages, but more voices and languages are always added."
    )
    features = datasets.Features(
        {
            "path": datasets.Value("string"),
            "language": datasets.Value("string"),
            "reader": datasets.Value("string"),
            "sentence": datasets.Value("string"),
            "audio": datasets.features.Audio(sampling_rate=44100)
        }
    )

    return datasets.DatasetInfo(
        description=description,
        features=features,
        supervised_keys=None,
        homepage=_HOMEPAGE,
        license=_LICENSE,
        citation=_CITATION,
        version=self.config.version,
    )
```

### Download and define the dataset splits

Now that you've added some information about your dataset, the next step is to download the dataset and define the splits.

1. Use the [`~DownloadManager.download`] method to download the audio data specified in `_AUDIO_URL`. If you're not enabling streaming, this method returns a path to a local archive. However, if you're planning on enabling streaming, this method opens the data at the URL remotely. This method accepts:

    * a relative path to a file inside a Hub dataset repository (for example, in the `data/` folder)
    * a URL to a file hosted somewhere else
    * a (nested) list or dictionary of file names or URLs

2. If you aren't planning on making your dataset streamable, then you can use the [`~DownloadManager.extract`] method to extract the TAR archive locally. This returns a local path to the extracted archive directory:

   ```py
   local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
   ```

3. But if you want your dataset to be streamable, then you'll need to use the [`~DownloadManager.iter_archive`] method to iterate over the `audio_path` after it's downloaded. This allows  Datasets to iterate over the files inside the TAR archive when you generate examples from your dataset. However, [`~DownloadManager.iter_archive`] doesn't provide any information about the full local paths even if they exist locally. As a result, you need to pass the `local_extracted_archive` path to the next step inside `gen_kwargs`, in order to preserve information about where the locally extracted files are. This is required to construct the correct paths to the local files when you generate the examples.

<Tip>

The reason you need to use a combination of [`~DownloadManager.download`] and [`~DownloadManager.iter_archive`] is because data in TAR archives can't be accessed directly from their paths. Instead, you'll need to download it first and then sequentially iterate over the files within the archive!

</Tip>

4. Use the [`~DownloadManager.download_and_extract`] method to download the metadata file specified in `_METADATA_URL`. If you're not enabling streaming, this method returns a path to a local file. However, if you're planning on enabling streaming, this method opens the file at the URL remotely. 

    While you can use [`~DownloadManager.download_and_extract`] to extract TAR archives, this method on its own does not support streaming. To use  [`~DownloadManager.download_and_extract`] for streaming, you also need to use [`~DownloadManager.iter_archive`].

5. Now use the [`SplitGenerator`] to organize the audio files and metadata in each split. Name each split with a standard name like: `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`.

    In the `gen_kwargs` parameter, specify the file paths to `local_extracted_archive`, `audio_files`, `metadata_path`, and `path_to_clips`. Remember, for `audio_files`, you need to use [`~DownloadManager.iter_archive`] to iterate over the audio files in the TAR archives. This enables streaming for your dataset! All of these file paths are passed onto the next step where the dataset samples are generated.

For a quick and handy reference, check out the table below to help you decide which download method (or combination of download methods) to use:

| Method                               | File type                     | Streamable |
|--------------------------------------|-------------------------------|------------|
| download_and_extract                 | tar                           | no         |
| download_and_extract                 | zip, gzip, xz, zstd, bz2, lz4 | yes        |
| download + iter_archives             | tar, rar, 7z                  | yes        |
| download_and_extract + iter_archives | any                           | yes        |

```py
def _split_generators(self, dl_manager):
    """Returns SplitGenerators."""
    dl_manager.download_config.ignore_url_params = True

    audio_path = dl_manager.download(_AUDIO_URL)
    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
    path_to_clips = "librivox-indonesia"

    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_train.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.TEST,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_test.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
    ]
```

### Generate the dataset

The last method in the [`GeneratorBasedBuilder`] class actually generates the audio data and metadata in the dataset. It yields a dataset according to the structure specified in `features` from the `info` method. As you can see, `generate_examples` accepts `local_extracted_archive`, `audio_files`, `metadata_path`, and `path_to_clips` from the previous method as arguments.

1. To stream a TAR archive, it needs to be opened and read first. TAR files are accessed and yielded sequentially. This means you need to have the metadata in `metadata_path` associated with the audio files in the TAR file in hand first so you can yield it with its corresponding audio file:

   ```py
   with open(metadata_path, "r", encoding="utf-8") as f:
       reader = csv.DictReader(f)
       for row in reader:
           if self.config.name == "all" or self.config.name == row["language"]:
               row["path"] = os.path.join(path_to_clips, row["path"])
               # if data is incomplete, fill with empty values
               for field in data_fields:
                   if field not in row:
                       row[field] = ""
               metadata[row["path"]] = row
   ```

2. Now you can open the files stored in `audio_files`. When you used [`~DownloadManager.iter_archive`] earlier, it yielded a tuple of a *relative* path to a file inside the archive, and the file itself. To get the *full path* to the locally extracted file, join the path of the directory (`local_extracted_path`) where the archive is extracted to and the relative audio file path (`path`):

   ```py
   for path, f in audio_files:
       if path in metadata:
           result = dict(metadata[path])
           # set the audio feature and the path to the extracted file
           path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
           result["audio"] = {"path": path, "bytes": f.read()}
           result["path"] = path
           yield id_, result
           id_ += 1
    ````

Put both of these steps together, and the whole `_generate_examples` method should look like:

```py
def _generate_examples(
        self,
        local_extracted_archive,
        audio_files,
        metadata_path,
        path_to_clips,
    ):
        """Yields examples."""
        data_fields = list(self._info().features.keys())
        metadata = {}
        with open(metadata_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if self.config.name == "all" or self.config.name == row["language"]:
                    row["path"] = os.path.join(path_to_clips, row["path"])
                    # if data is incomplete, fill with empty values
                    for field in data_fields:
                        if field not in row:
                            row[field] = ""
                    metadata[row["path"]] = row
        id_ = 0
        for path, f in audio_files:
            if path in metadata:
                result = dict(metadata[path])
                # set the audio feature and the path to the extracted file
                path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
                result["audio"] = {"path": path, "bytes": f.read()}
                result["path"] = path
                yield id_, result
                id_ += 1
```

### Upload the dataset to the Hub

Once your script is ready, [create a dataset card](./dataset_card) and [upload it to the Hub](./share).

Congratulations, you can now load your dataset from the Hub! コ

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```
