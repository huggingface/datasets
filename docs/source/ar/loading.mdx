## ุงูุชุญููู
ูููู ุชุฎุฒูู ุจูุงูุงุชู ูู ุฃูุงูู ูุฎุชููุฉุ ูููู ุฃู ุชููู ุนูู ุงููุฑุต ุงูููุฌูุฏ ุนูู ุฌูุงุฒู ุงููุญููุ ุฃู ูู ูุณุชูุฏุน GitHubุ ุฃู ูู ููุงูู ุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ ูุซู ุงูููุงููุณ ู DataFrames ุงูุฎุงุตุฉ ุจู Python. ุฃูููุง ุชู ุชุฎุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูููู ูู ๐ค Datasets ูุณุงุนุฏุชู ูู ุชุญููููุง.

ุณููุถุญ ูู ูุฐุง ุงูุฏููู ููููุฉ ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูู:

- Hub Hugging Face
- ูููุงุช ูุญููุฉ
- ุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ
- ุบูุฑ ูุชุตู
- ุดุฑูุญุฉ ูุญุฏุฏุฉ ูู ุชูุณูู
- ูุต ุจุฑูุฌู ูุญูู ููุชุญููู (ูุฏูู)

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู ุงููุญุฏุฏุฉ ุญูู ุชุญููู ุทุฑุงุฆู ุจูุงูุงุช ุฃุฎุฑูุ ุงุทูุน ุนูู ุฏููู ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ุงูุตูุชุ ุฃู ุฏููู ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ุงูุตูุฑุ ุฃู ุฏููู ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช NLP.

## Hub Hugging Face

ูุชู ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ูุต ุจุฑูุฌู ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูููู ุจุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฅูุดุงุฆูุง. ููุน ุฐููุ ููููู ุฃูุถูุง ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูู ุฃู ูุณุชูุฏุน ููุฌููุนุฉ ุจูุงูุงุช ุนูู Hub ุฏูู ูุต ุจุฑูุฌู ููุชุญููู! ุงุจุฏุฃ ุจู [ุฅูุดุงุก ูุณุชูุฏุน ูุฌููุนุฉ ุจูุงูุงุช](share#create-the-repository) ููู ุจุชุญููู ูููุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู. ุงูุขู ููููู ุงุณุชุฎุฏุงู ูุธููุฉ [`load_dataset`] ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช.

ุนูู ุณุจูู ุงููุซุงูุ ุฌุฑูุจ ุชุญููู ุงููููุงุช ูู ูุฐุง [ูุณุชูุฏุน ุงูุนุฑุถ ุงูุชูุถูุญู](https://huggingface.co/datasets/lhoestq/demo1) ุนู ุทุฑูู ุชูููุฑ ูุณุงุญุฉ ุงุณู ุงููุณุชูุฏุน ูุงุณู ูุฌููุนุฉ ุงูุจูุงูุงุช. ูุญุชูู ูุณุชูุฏุน ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐุง ุนูู ูููุงุช CSVุ ููููู ุงูููุฏ ุฃุฏูุงู ุจุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ูููุงุช CSV:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("lhoestq/demo1")
```

ูุฏ ุชุญุชูู ุจุนุถ ูุฌููุนุงุช ุงูุจูุงูุงุช ุนูู ุฃูุซุฑ ูู ุฅุตุฏุงุฑ ูุงุญุฏ ุจูุงุกู ุนูู ุนูุงูุงุช Git ุฃู ุงููุฑูุน ุฃู ุงูุงูุชุฒุงูุงุช. ุงุณุชุฎุฏู ูุนููุฉ `revision` ูุชุญุฏูุฏ ุฅุตุฏุงุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฐู ุชุฑูุฏ ุชุญูููู:

```py
>>> dataset = load_dataset(
...   "lhoestq/custom_squad",
...   revision="main"  # ุงุณู ุงูุนูุงูุฉุ ุฃู ุงุณู ุงููุฑุนุ ุฃู ุงูุชุฌุฒุฆุฉ
... )
```

ุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู [ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ุฅูู Hub](./upload_dataset) ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ููููุฉ ุฅูุดุงุก ูุณุชูุฏุน ูุฌููุนุฉ ุจูุงูุงุช ุนูู Hubุ ูููููุฉ ุชุญููู ูููุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู.

ุชูุญูููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฏูู ูุต ุจุฑูุฌู ููุชุญููู ุจุดูู ุงูุชุฑุงุถู ุฌููุน ุงูุจูุงูุงุช ูู ุชูุณูู `train`. ุงุณุชุฎุฏู ูุนููุฉ `data_files` ูุชุนููู ูููุงุช ุงูุจูุงูุงุช ุฅูู ุชูุณููุงุช ูุซู `train` ู`validation` ู`test`:

```py
>>> data_files = {"train": "train.csv", "test": "test.csv"}
>>> dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)
```

ุฅุฐุง ูู ุชุญุฏุฏ ูููุงุช ุงูุจูุงูุงุช ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุงุ ูุณูุชู [`load_dataset`] ุฅุฑุฌุงุน ุฌููุน ูููุงุช ุงูุจูุงูุงุช. ูุฏ ูุณุชุบุฑู ูุฐุง ููุชูุง ุทูููุงู ุฅุฐุง ููุช ุจุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ ูุซู C4ุ ูุงูุชู ุชุจูุบ ุญูุงูู 13 ุชูุฑุงุจุงูุช ูู ุงูุจูุงูุงุช.

ููููู ุฃูุถูุง ุชุญููู ุฌุฒุก ูุฑุนู ูุญุฏุฏ ูู ุงููููุงุช ุจุงุณุชุฎุฏุงู ูุนููุฉ `data_files` ุฃู `data_dir`. ูููู ููุฐู ุงููุนููุงุช ูุจูู ูุณุงุฑ ูุณุจู ูุญู ูุญู ูุณุงุฑ ุงูุฃุณุงุณ ุงูููุงุจู ููููุงู ุงูุฐู ูุชู ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ููู.

```py
>>> from datasets import load_dataset

# ุชุญููู ุงููููุงุช ุงูุชู ุชุชุทุงุจู ูุน ููุท grep
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")

# ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ุฏููู en ุนูู Hub
>>> c4_subset = load_dataset("allenai/c4", data_dir="en")
```

ูููู ุฃูุถูุง ููุนููุฉ `split` ุชุนููู ููู ุจูุงูุงุช ุฅูู ุชูุณูู ูุญุฏุฏ:

```py
>>> data_files = {"validation": "en/c4-validation.*.json.gz"}
>>> c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")
```

## ุงููููุงุช ุงููุญููุฉ ูุงูุจุนูุฏุฉ

ูููู ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ุงููููุงุช ุงููุญููุฉ ุงููุฎุฒูุฉ ุนูู ุฌูุงุฒ ุงูููุจููุชุฑ ุงูุฎุงุต ุจู ููู ุงููููุงุช ุงูุจุนูุฏุฉ. ูู ุงููุญุชูู ุฃู ุชููู ูุฌููุนุงุช ุงูุจูุงูุงุช ูุฎุฒูุฉ ุจุชูุณูู `csv` ุฃู `json` ุฃู `txt` ุฃู `parquet`. ูููู ููุธููุฉ [`load_dataset`] ุชุญููู ูู ููุน ูู ูุฐู ุงูุฃููุงุน ูู ุงููููุงุช.

### CSV

ูููู ูู ๐ค Datasets ูุฑุงุกุฉ ูุฌููุนุฉ ุจูุงูุงุช ุชุชููู ูู ููู CSV ูุงุญุฏ ุฃู ุฃูุซุฑ (ูู ูุฐู ุงูุญุงูุฉุ ูู ุจุชูุฑูุฑ ูููุงุช CSV ุงูุฎุงุตุฉ ุจู ููุงุฆูุฉ):

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("csv", data_files="my_file.csv")
```

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตููุ ุฑุงุฌุน ุงูุฏููู [ููููุฉ ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฌุฏูููุฉ ูู ูููุงุช CSV](tabular_load#csv-files).

### JSON

ูุชู ุชุญููู ูููุงุช JSON ูุจุงุดุฑุฉ ุจุงุณุชุฎุฏุงู [`load_dataset`] ููุง ูู ููุถุญ ุฃุฏูุงู:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json")
```

ุชุชููุน ุชูุณููุงุช JSONุ ูููููุง ูุนุชูุฏ ุฃู ุฃูุซุฑ ุงูุชูุณููุงุช ููุงุกุฉ ูู ูุฌูุฏ ุนุฏุฉ ูุงุฆูุงุช JSONุ ููุซู ูู ุณุทุฑ ุตููุง ูุฑุฏููุง ูู ุงูุจูุงูุงุช. ุนูู ุณุจูู ุงููุซุงู:

```json
{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}
```

ุชูุณูู JSON ุขุฎุฑ ูุฏ ุชุตุงุฏูู ูู ุญูู ูุชุฏุงุฎูุ ููู ูุฐู ุงูุญุงูุฉ ุณุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุญุฌุฉ `field` ููุง ูู ููุถุญ ูููุง ููู:

```py
{"version": "0.1.0",
"data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
{"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json", field="data")
```

ูุชุญููู ูููุงุช JSON ุงูุจุนูุฏุฉ ุนุจุฑ HTTPุ ูุฑุฑ ุนูุงููู URL ุจุฏูุงู ูู ุฐูู:

```py
>>> base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
>>> dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")
```

ุจูููุง ูุฐู ูู ุชูุณููุงุช JSON ุงูุฃูุซุฑ ุดููุนูุงุ ูุณุชุดุงูุฏ ูุฌููุนุงุช ุจูุงูุงุช ุฃุฎุฑู ุจุชูุณูู ูุฎุชูู. ูุชุนุฑู ๐ค Datasets ุนูู ูุฐู ุงูุชูุณููุงุช ุงูุฃุฎุฑู ูุณูุชู ุงูุชุฑุงุฌุน ููููุง ูุฐูู ุนูู ุฃุณุงููุจ ุชุญููู JSON ูู Python ููุชุนุงูู ูุนูุง.

### Parquet

ุชูุฎุฒููู ูููุงุช Parquet ุจุชูุณูู ุนููุฏูุ ุนูู ุนูุณ ุงููููุงุช ุงููุณุชูุฏุฉ ุฅูู ุงูุตู ูุซู CSV. ูุฏ ูุชู ุชุฎุฒูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููุจูุฑุฉ ูู ููู Parquet ูุฃูู ุฃูุซุฑ ููุงุกุฉ ูุฃุณุฑุน ูู ุฅุฑุฌุงุน ุงุณุชุนูุงูู.

ูุชุญููู ููู Parquet:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})
```

ูุชุญููู ูููุงุช Parquet ุงูุจุนูุฏุฉ ุนุจุฑ HTTPุ ูุฑุฑ ุนูุงููู URL ุจุฏูุงู ูู ุฐูู:

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.parquet"}
>>> wiki = load_dataset("parquet", data_files=data_files, split="train")
```

### Arrow

ุชูุฎุฒููู ูููุงุช Arrow ุจุชูุณูู ุนููุฏู ูู ุงูุฐุงูุฑุฉุ ุนูู ุนูุณ ุงูุชูุณููุงุช ุงููุณุชูุฏุฉ ุฅูู ุงูุตู ูุซู CSV ูุงูุชูุณููุงุช ุบูุฑ ุงููุถุบูุทุฉ ูุซู Parquet.

ูุชุญููู ููู Arrow:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("arrow", data_files={'train': 'train.arrow', 'test': 'test.arrow'})
```

ูุชุญููู ูููุงุช Arrow ุงูุจุนูุฏุฉ ุนุจุฑ HTTPุ ูุฑุฑ ุนูุงููู URL ุจุฏูุงู ูู ุฐูู:

```py
>>> base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/2Multiplier0200501.en/1.0.0/"
>>> data_files = {"train": base_url + "wikipedia-train.arrow"}
>>> wiki = load_dataset("arrow", data_files=data_files, split="train")
```

Arrow ูู ุชูุณูู ุงูููู ุงูุฐู ูุณุชุฎุฏูู ๐ค Datasets ุชุญุช ุงูุบุทุงุกุ ูุฐูู ููููู ุชุญููู ููู Arrow ูุญูู ุจุงุณุชุฎุฏุงู [`Dataset.from_file`] ูุจุงุดุฑุฉ:

```py
>>> from datasets import Dataset
>>> dataset = Dataset.from_file("data.arrow")
```

ุนูู ุนูุณ [`load_dataset`]`[Dataset.from_file`]ุ ูููู ุจุชุนููู ููู Arrow ูู ุงูุฐุงูุฑุฉ ุฏูู ุฅุนุฏุงุฏ ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉุ ููุง ูููุฑ ูู ูุณุงุญุฉ ุนูู ุงููุฑุต.

ุณูููู ุฏููู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ูุชุฎุฒูู ูุชุงุฆุฌ ุงููุนุงูุฌุฉ ุงููุชูุณุทุฉ ูู ุฏููู ููู Arrow ูู ูุฐู ุงูุญุงูุฉ.

ูุชู ุฏุนู ุชูุณูู Arrow ููุจุซ ููุท ูู ุงูููุช ุงูุญุงูู. ุชูุณูู IPC Arrow (ุงููุนุฑูู ุฃูุถูุง ุจุงุณู Feather V2) ุบูุฑ ูุฏุนูู.

### SQL

ุงูุฑุฃ ูุญุชููุงุช ูุงุนุฏุฉ ุงูุจูุงูุงุช ุจุงุณุชุฎุฏุงู [`~datasets.Dataset.from_sql`] ุนู ุทุฑูู ุชุญุฏูุฏ ุนููุงู URL ููุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู. ููููู ูุฑุงุกุฉ ุฃุณูุงุก ุงูุฌุฏุงูู ูุงูุงุณุชุนูุงูุงุช:

```py
>>> from datasets import Dataset
# ุชุญููู ุงูุฌุฏูู ุจุงููุงูู
>>> dataset = Dataset.from_sql("data_table_name", con="sqlite:///sqlite_file.db")
# ุงูุชุญููู ูู ุงูุงุณุชุนูุงู
>>> dataset = Dataset.from_sql("SELECT text FROM table WHERE length(text) > 100 LIMIT 10", con="sqlite:///sqlite_file.db")
```

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตููุ ุฑุงุฌุน ุงูุฏููู [ููููุฉ ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฌุฏูููุฉ ูู ููุงุนุฏ ุจูุงูุงุช SQL](tabular_load#databases).

### WebDataset

ูุณุชูุฏ ุชูุณูู [WebDataset](https://github.com/webdataset/webdataset) ุฅูู ุฃุฑุดููุงุช TAR ููู ููุงุณุจ ููุฌููุนุงุช ุงูุจูุงูุงุช ุงูุตูุฑูุฉ ุงููุจูุฑุฉ.

ุจุณุจุจ ุญุฌููุงุ ูุชู ุชุญููู WebDatasets ุจุดูู ุนุงู ูู ูุถุน ุงูุจุซ (ุจุงุณุชุฎุฏุงู `streaming=True`).

ููููู ุชุญููู WebDataset ุนูู ุงููุญู ุงูุชุงูู:

```python
>>> from datasets import load_dataset
>>>
>>> path = "path/to/train/*.tar"
>>> dataset = load_dataset("webdataset", data_files={"train": path}, split="train", streaming=True)
```

ูุชุญููู WebDatasets ุงูุจุนูุฏุฉ ุนุจุฑ HTTPุ ูุฑุฑ ุนูุงููู URL ุจุฏูุงู ูู ุฐูู:

```python
>>> from datasets import load_dataset
>>>
>>> base_url = "https://huggingface.co/datasets/lhoestq/small-publaynet-wds/resolve/main/publaynet-train-{i:06d}.tar"
>>> urls = [base_url.format(i=i) for i in range(4)]
>>> dataset = load_dataset("webdataset", data_files={"train": urls}, split="train", streaming=True)
```

## ุชุนุฏุฏ ุงููุนุงูุฌุฉ

ุนูุฏูุง ุชุชููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ุนุฏุฉ ูููุงุช (ูุงูุชู ูุณูููุง "ุงูุดุธุงูุง")ุ ูููู ุชุณุฑูุน ุฎุทูุฉ ุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฅุนุฏุงุฏูุง ุจุดูู ูุจูุฑ.

ููููู ุงุฎุชูุงุฑ ุนุฏุฏ ุงูุนูููุงุช ุงูุชู ุชุฑูุฏ ุงุณุชุฎุฏุงููุง ูุฅุนุฏุงุฏ ูุฌููุนุฉ ุจูุงูุงุช ุจุงูุชูุงุฒู ุจุงุณุชุฎุฏุงู `num_proc`.

ูู ูุฐู ุงูุญุงูุฉุ ูุชู ููุญ ูู ุนูููุฉ ูุฌููุนุฉ ูุฑุนูุฉ ูู ุงูุดุธุงูุง ูุฅุนุฏุงุฏูุง:

```python
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", num_proc=8)
ml_librispeech_spanish = load_dataset("facebook/multilingual_librispeech", "spanish", num_proc=8)
```

## ุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ

ุณููุชูุญ ูู ๐ค Datasets ุฃูุถูุง ุฅูุดุงุก [`Dataset`] ูุจุงุดุฑุฉู ูู ููุงูู ุงูุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ ูุซู ุงูููุงููุณ ูDataFrames ุงูุฎุงุตุฉ ุจู Python.

### ูุงููุณ Python

ูู ุจุชุญููู ุงูููุงููุณ ุงูุฎุงุตุฉ ุจู Python ุจุงุณุชุฎุฏุงู [`~Dataset.from_dict`]:

```py
>>> from datasets import Dataset
>>> my_dict = {"a": [1, 2, 3]}
>>> dataset = Dataset.from_dict(my_dict)
```

### ูุงุฆูุฉ Python ูู ุงูููุงููุณ

ูู ุจุชุญููู ูุงุฆูุฉ ูู ุงูููุงููุณ ุงูุฎุงุตุฉ ุจู Python ุจุงุณุชุฎุฏุงู [`~Dataset.from_list`]:

```py
>>> from datasets import Dataset
>>> my_list = [{"a": 1}, {"a": 2}, {"a": 3}]
>>> dataset = Dataset.from_list(my_list)
```

### ูููุฏ Python

ูู ุจุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ูู ูููุฏ Python ุจุงุณุชุฎุฏุงู [`~Dataset.from_generator`]:

```py
>>> from datasets import Dataset
>>> def my_gen():
...     for i in range(1, 4):
...         yield {"a": i}
...
>>> dataset = Dataset.from_generator(my_gen)
```

ูุฏุนู ูุฐุง ุงูููุฌ ุชุญููู ุงูุจูุงูุงุช ุงูุฃูุจุฑ ูู ุงูุฐุงูุฑุฉ ุงููุชููุฑุฉ.

ููููู ุฃูุถูุง ุชุญุฏูุฏ ูุฌููุนุฉ ุจูุงูุงุช ูุฌุฒุฃุฉ ุนู ุทุฑูู ุชูุฑูุฑ ุงูููุงุฆู ุฅูู `gen_kwargs`:

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {"line": line}
...
>>> shards = [f"data{i}.txt" for i in range(32)]
>>> ds = IterableDataset.from_generator(gen, gen_kwargs={"shards": shards})
>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # ุชุฑุชูุจ ุงูุดุธุงูุง + ุงุณุชุฎุฏุงู ูุฎุฒู ูุคูุช ููุฎูุท
>>> from torch.utils.data import DataLoader
>>> dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # ุฅุนุทุงุก ูู ุนุงูู ูุฌููุนุฉ ูุฑุนูุฉ ูู ุงูุดุธุงูุง 32/4=8
```

### Pandas DataFrame

ูู ุจุชุญููู Pandas DataFrames ุจุงุณุชุฎุฏุงู [`~Dataset.from_pandas`]:

```py
>>> from datasets import Dataset
>>> import pandas as pd
>>> df = pd.DataFrame({"a": [1, 2, 3]})
>>> dataset = Dataset.from_pandas(df)
```

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตููุ ุฑุงุฌุน ุงูุฏููู [ููููุฉ ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฌุฏูููุฉ ูู Pandas DataFrames](tabular_load#pandas-dataframes).

## ุบูุฑ ูุชุตู

ุญุชู ุฅุฐุง ูู ููู ูุฏูู ุงุชุตุงู ุจุงูุฅูุชุฑูุชุ ููุง ูุฒุงู ูู ุงููููู ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช. ุทุงููุง ููุช ุจุชูุฒูู ูุฌููุนุฉ ุจูุงูุงุช ูู ูุณุชูุฏุน Hub ูู ูุจูุ ูุฌุจ ุฃู ุชููู ูุฎุฒูุฉ ูุคูุชูุง. ููุฐุง ูุนูู ุฃูู ููููู ุฅุนุงุฏุฉ ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ูุงุณุชุฎุฏุงููุง ุฏูู ุงุชุตุงู.

ุฅุฐุง ููุช ุชุนูู ุฃูู ูู ูููู ูุฏูู ุงุชุตุงู ุจุงูุฅูุชุฑูุชุ ูููููู ุชุดุบูู ๐ค Datasets ูู ูุถุน ุนุฏู ุงูุงุชุตุงู ุงููุงูู. ูููุฑ ูุฐุง ุงูููุช ูุฃูู ุจุฏูุงู ูู ุงูุชุธุงุฑ ุงูุชูุงุก ุตูุงุญูุฉ ุชูุฒูู ุจุฑูุงูุฌ ุจูุงุก ูุฌููุนุฉ ุงูุจูุงูุงุชุ ุณููุธุฑ ๐ค Datasets ูุจุงุดุฑุฉ ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ. ูู ุจุชุนููู ูุชุบูุฑ ุงูุจูุฆุฉ `HF_HUB_OFFLINE` ุฅูู `1` ูุชูููู ูุถุน ุนุฏู ุงูุงุชุตุงู ุงููุงูู.
## ุชูุณูู ุงูุดุฑุงุฆุญ

ููููู ุฃูุถูุง ุงุฎุชูุงุฑ ุชุญููู ุดุฑุงุฆุญ ูุญุฏุฏุฉ ููุท ูู ุชูุณูู. ููุงู ุฎูุงุฑุงู ูุชูุณูู ุงูุดุฑูุญุฉ: ุจุงุณุชุฎุฏุงู ุงูุณูุงุณู ุฃู ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช [`ReadInstruction`]. ุงูุณูุงุณู ุฃูุซุฑ ุฅุญูุงูุง ููุงุจููุฉ ูููุฑุงุกุฉ ููุญุงูุงุช ุงูุจุณูุทุฉุ ูู ุญูู ุฃู [`ReadInstruction`] ุฃุณูู ูู ุงูุงุณุชุฎุฏุงู ูุน ูุนููุงุช ุชูุณูู ุงูุดุฑุงุฆุญ ุงููุชุบูุฑุฉ.

ูู ุจุฏูุฌ ุชูุณูู `train` ู`test` ุจุงูุทุฑููุฉ ุงูุชุงููุฉ:

```py
>>> train_test_ds = datasets.load_dataset("bookcorpus", split="train+test")
===STRINGAPI-READINSTRUCTION-SPLIT===
>>> ri = datasets.ReadInstruction("train") + datasets.ReadInstruction("test")
>>> train_test_ds = datasets.load_dataset("bookcorpus", split=ri)
```

ุญุฏุฏ ุตููููุง ูุญุฏุฏุฉ ูู ุชูุณูู "ุงูุชุฏุฑูุจ":

```py
>>> train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")
===STRINGAPI-READINSTRUCTION-SPLIT===
>>> train_10_20_ds = datasets.load_dataset("bookcorpu"ุ split=datasets.ReadInstruction("train"ุ from_=10ุ to=20ุ unit="abs"))
```

ุฃู ุญุฏุฏ ูุณุจุฉ ูุฆููุฉ ูู ุงูุชูุณูู ุจุงุณุชุฎุฏุงู:

```py
>>> train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]")
===STRINGAPI-READINSTRUCTION-SPLIT===
>>> train_10_20_ds = datasets.load_dataset("bookcorpus"ุ split=datasets.ReadInstruction("train"ุ to=10ุ unit="ูช"))
```

ุญุฏุฏ ูุฌููุนุฉ ูู ุงููุณุจ ุงููุฆููุฉ ูู ูู ุชูุณูู:

```py
>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")
===STRINGAPI-READINSTRUCTION-SPLIT===
>>> ri = (datasets.ReadInstruction("train"ุ to=10ุ unit="ูช") + datasets.ReadInstruction("train"ุ from_=-80ุ unit="ูช"))
>>> train_10_80pct_ds = datasets.load_dataset("bookcorpus"ุ split=ri)
```

ุฃุฎูุฑูุงุ ููููู ุญุชู ุฅูุดุงุก ุชูุณููุงุช ูุตุฏูุฉ. ูููู ุงููุซุงู ุฃุฏูุงู ุจุฅูุดุงุก ุชูุณููุงุช ูุตุฏูุฉ 10 ุฃุถุนุงู. ุชุญุชูู ูู ูุฌููุนุฉ ุจูุงูุงุช ุชุญูู ูู ุงูุตุญุฉ ุนูู ุฌุฒุก 10ูชุ ูุชุดูู ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฏุฑูุจ ุงูุฌุฒุก ุงูุชููููู ุงููุชุจูู ุจูุณุจุฉ 90ูช:

```py
>>> val_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])
>>> train_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])
===STRINGAPI-READINSTRUCTION-SPLIT===
>>> val_ds = datasets.load_dataset("bookcorpus"ุ [datasets.ReadInstruction("train"ุ from_=kุ to=k+10ุ unit="ูช") for k in range(0ุ 100ุ 10)])
>>> train_ds = datasets.load_dataset("bookcorpus"ุ [(datasets.ReadInstruction("train"ุ to=kุ unit="ูช") + datasets.ReadInstruction("train"ุ from_=k+10ุ unit="ูช")) for k in range(0ุ 100ุ 10)])
```

### ุชูุณูู ุงููุณุจ ุงููุฆููุฉ ูุงูุชูุฑูุจ

ุงูุณููู ุงูุงูุชุฑุงุถู ูู ุชูุฑูุจ ุงูุญุฏูุฏ ุฅูู ุฃูุฑุจ ุฑูู ุตุญูุญ ููุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชู ูุง ุชูุณู ูููุง ุญุฏูุฏ ุงูุดุฑุงุฆุญ ุงููุทููุจุฉ ุจุงูุชุณุงูู ุนู ุทุฑูู 100. ููุง ูู ููุถุญ ุฃุฏูุงูุ ูุฏ ุชุญุชูู ุจุนุถ ุงูุดุฑุงุฆุญ ุนูู ุฃูุซูุฉ ุฃูุซุฑ ูู ุบูุฑูุง. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงู ุชูุณูู ุงูุชุฏุฑูุจ ุงูุชุงูู ูุญุชูู ุนูู 999 ุณุฌูุ ุซู:

```py
# 19 ุณุฌููุงุ ูู 500 (ูุถูู) ุฅูู 519 (ูุณุชุจุนุฏ).
>>> train_50_52_ds = datasets.load_dataset("bookcorpus"ุ split="train[50%:52%]")
# 20 ุณุฌููุงุ ูู 519 (ูุถูู) ุฅูู 539 (ูุณุชุจุนุฏ).
>>> train_52_54_ds = datasets.load_dataset("bookcorpus"ุ split="train[52%:54%]")
```

ุฅุฐุง ููุช ุชุฑูุฏ ุชูุณููุงุช ุฐุงุช ุญุฌู ูุชุณุงููุ ูุงุณุชุฎุฏู ุชูุฑูุจ `pct1_dropremainder` ุจุฏูุงู ูู ุฐูู. ุชุนุงูู ูุฐู ุงูุทุฑููุฉ ุญุฏูุฏ ุงููุณุจ ุงููุฆููุฉ ุงููุญุฏุฏุฉ ูุฃุถุนุงู 1ูช.

```py
# 18 ุณุฌููุงุ ูู 450 (ูุถูู) ุฅูู 468 (ูุณุชุจุนุฏ).
>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus"ุ split=datasets.ReadInstruction("train"ุ from_=50ุ to=52ุ unit="ูช"ุ rounding="pct1_dropremainder"))
# 18 ุณุฌููุงุ ูู 468 (ูุถูู) ุฅูู 486 (ูุณุชุจุนุฏ).
>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus"ุ split=datasets.ReadInstruction("train"ุ from_=52ุ to=54ุ unit="ูช"ุ rounding="pct1_dropremainder"))
# ุฃู ูุง ูุนุงุฏููุง:
>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus"ุ split="train[50%:52%](pct1_dropremainder)")
>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus"ุ split="train[52%:54%](pct1_dropremainder)")
```

<Tip warning={true}>
ูุฏ ูุคุฏู ุชูุฑูุจ `pct1_dropremainder` ุฅูู ุงูุชุทุงุน ุงูุฃูุซูุฉ ุงูุฃุฎูุฑุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅุฐุง ูู ููู ุนุฏุฏ ุงูุฃูุซูุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ููุณูููุง ุจุงูุชุณุงูู ุนูู 100.
</Tip>

<a id='troubleshoot'></a>

## ุงุณุชูุดุงู ุงูุฃุฎุทุงุก ูุฅุตูุงุญูุง

ูู ุจุนุถ ุงูุฃุญูุงูุ ูุฏ ุชุญุตู ุนูู ูุชุงุฆุฌ ุบูุฑ ูุชููุนุฉ ุนูุฏ ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช. ุงุซูุงู ูู ุฃูุซุฑ ุงููุถุงูุง ุดููุนูุง ุงูุชู ูุฏ ุชูุงุฌููุง ููุง ุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฏูููุง ูุชุญุฏูุฏ ููุฒุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช.

### ุงูุชูุฒูู ุงููุฏูู

ุชุชุทูุจ ุจุนุถ ูุฌููุนุงุช ุงูุจูุงูุงุช ููู ุชูุฒูู ูููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฏูููุง ุจุณุจุจ ุนุฏู ุชูุงูู ุงูุชุฑุฎูุต ุฃู ุฅุฐุง ูุงูุช ุงููููุงุช ูุฎููุฉ ุฎูู ุตูุญุฉ ุชุณุฌูู ุงูุฏุฎูู. ูุชุณุจุจ ูุฐุง ูู ููุงู [`load_dataset`] ุจุฅููุงุก `AssertionError`. ูููู ูููุฑ ๐ค Datasets ุชุนูููุงุช ููุตูุฉ ูุชูุฒูู ุงููููุงุช ุงูููููุฏุฉ. ุจุนุฏ ุชูุฒูู ุงููููุงุชุ ุงุณุชุฎุฏู ูุณูุท `data_dir` ูุชุญุฏูุฏ ุงููุณุงุฑ ุฅูู ุงููููุงุช ุงูุชู ููุช ุจุชูุฒูููุง ููุชู.

ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ุญุงููุช ุชูุฒูู ุชูููู ูู ูุฌููุนุฉ ุจูุงูุงุช [MATINF](https://huggingface.co/datasets/matinf):

```py
>>> dataset = load_dataset("matinf", "summarization")
ุชูุฒูู ูุฅุนุฏุงุฏ ูุฌููุนุฉ ุงูุจูุงูุงุช matinf/summarization (ุงูุชูุฒูู: ุงูุญุฌู ุบูุฑ ูุนุฑููุ ุงููููุดุฃ: 246.89 ููุบุงุจุงูุชุ ูุง ุจุนุฏ ุงููุนุงูุฌุฉ: ุงูุญุฌู ุบูุฑ ูุนุฑููุ ุงูุฅุฌูุงูู: 246.89 ููุบุงุจุงูุช) ุฅูู /root/.cache/huggingface/datasets/matinf/summarization/1.0.0/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: ุชุชุทูุจ ูุฌููุนุฉ ุงูุจูุงูุงุช matinf ุจุงูุชูููู ุชูุฎูุต ุชูุฒูู ุงูุจูุงูุงุช ูุฏูููุง.
ูุฑุฌู ุงุชุจุงุน ุชุนูููุงุช ุงูุชูุฒูู ุงููุฏูู: ูุงุณุชุฎุฏุงู MATINFุ ูุฌุจ ุชูุฒููู ูุฏูููุง. ูุฑุฌู ููุก ูููุฐุฌ Google ูุฐุง (https://forms.gle/nkH4LVE4iNQeDzsc9). ุณุชุชููู ุฑุงุจุท ุชูุฒูู ููููุฉ ูุฑูุฑ ุจูุฌุฑุฏ ุฅููุงู ุงููููุฐุฌ. ูุฑุฌู ุงุณุชุฎุฑุงุฌ ุฌููุน ุงููููุงุช ูู ูุฌูุฏ ูุงุญุฏ ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุงุณุชุฎุฏุงู: *datasets.load_dataset('matinf'ุ data_dir='path/to/folder/folder_name')*.
ูููู ุชุญููู ุงูุจูุงูุงุช ุงููุฏููุฉ ุจุงุณุชุฎุฏุงู `datasets.load_dataset(matinfุ data_dir='<path/to/manual/data>')
```

ุฅุฐุง ููุช ุจุงููุนู ุจุชูุฒูู ูุฌููุนุฉ ุจูุงูุงุช ูู *Hub ุจุงุณุชุฎุฏุงู ูุต ุจุฑูุฌู ููุชุญููู* ุฅูู ุฌูุงุฒ ุงูููุจููุชุฑ ุงูุฎุงุต ุจูุ ููุฌุจ ุนููู ุชูุฑูุฑ ูุณุงุฑ ูุทูู ุฅูู ูุนููุฉ `data_dir` ุฃู `data_files` ูุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุชูู. ูุฅูุงุ ุฅุฐุง ููุช ุจุชูุฑูุฑ ูุณุงุฑ ูุณุจูุ ูุณูููู [`load_dataset`] ุจุชุญููู ุงูุฏููู ูู ุงููุณุชูุฏุน ุนูู Hub ุจุฏูุงู ูู ุงูุฏููู ุงููุญูู.

### ุชุญุฏูุฏ ุงูููุฒุงุช

ุนูุฏูุง ุชููู ุจุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ูู ูููุงุช ูุญููุฉุ ูุชู ุงุณุชูุชุงุฌ [`Features`] ุชููุงุฆููุง ุจูุงุณุทุฉ [Apache Arrow](https://arrow.apache.org/docs/). ููุน ุฐููุ ูุฏ ูุง ุชุชูุงูู ููุฒุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุฏุงุฆููุง ูุน ุชููุนุงุชูุ ุฃู ูุฏ ุชุฑุบุจ ูู ุชุญุฏูุฏ ุงูููุฒุงุช ุจููุณู. ููุถุญ ุงููุซุงู ุงูุชุงูู ููููุฉ ุฅุถุงูุฉ ุชุณููุงุช ูุฎุตุตุฉ ุจุงุณุชุฎุฏุงู ููุฒุฉ [`ClassLabel`].

ุงุจุฏุฃ ุจุชุญุฏูุฏ ุงูุชุณููุงุช ุงูุฎุงุตุฉ ุจู ุจุงุณุชุฎุฏุงู ูุฆุฉ [`Features`]:

```py

>>> class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
>>> emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})
```

ุจุนุฏ ุฐููุ ุญุฏุฏ ูุนููุฉ `features` ูู [`load_dataset`] ุจุงูููุฒุงุช ุงูุชู ููุช ุจุฅูุดุงุฆูุง ููุชู:

```py
>>> dataset = load_dataset('csv'ุ data_files=file_dictุ delimiter=';'ุ column_names=['text'ุ 'label']ุ features=emotion_features)
```

ุงูุขู ุนูุฏ ุงููุธุฑ ุฅูู ููุฒุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุ ููููู ุฑุคูุฉ ุงุณุชุฎุฏุงูู ุชุณููุงุช ูุฎุตุตุฉ ุงูุชู ููุช ุจุชุญุฏูุฏูุง:

```py
>>> dataset['train'].features
{'text': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}
```

## (ูุฏูู) ูุต ุจุฑูุฌู ูุญูู ููุชุญููู

ูุฏ ูููู ูุฏูู ูุต ุจุฑูุฌู ูุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ๐ค ูุญูููุง ุนูู ุฌูุงุฒ ุงูููุจููุชุฑ ุงูุฎุงุต ุจู. ูู ูุฐู ุงูุญุงูุฉุ ูู ุจุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุนู ุทุฑูู ุชูุฑูุฑ ุฃุญุฏ ุงููุณุงุฑุงุช ุงูุชุงููุฉ ุฅูู [`load_dataset`]:

- ุงููุณุงุฑ ุงููุญูู ุฅูู ููู ูุต ุจุฑูุฌู ููุชุญููู.
- ุงููุณุงุฑ ุงููุญูู ุฅูู ุงูุฏููู ุงูุฐู ูุญุชูู ุนูู ููู ูุต ุจุฑูุฌู ููุชุญููู (ููุท ุฅุฐุง ูุงู ุงุณู ุงูููู ูุทุงุจููุง ูุงุณู ุงูุฏููู).

ูุฑุฑ `trust_remote_code=True` ููุณูุงุญ ูู ๐ค Datasets ุจุชูููุฐ ูุต ุงูุจุฑูุฌุฉ:

```py
>>> dataset = load_dataset("path/to/local/loading_script/loading_script.py"ุ split="train"ุ trust_remote_code=True)
>>> dataset = load_dataset("path/to/local/loading_script"ุ split="train"ุ trust_remote_code=True) # ููุงูุฆ ูุฃู ุงุณู ุงูููู ูุทุงุจู ูุงุณู ุงูุฏููู
```