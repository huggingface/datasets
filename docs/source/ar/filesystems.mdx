# التخزين السحابي
🤗 Datasets تدعم الوصول إلى مزودي التخزين السحابي من خلال تطبيقات FileSystem الخاصة بـ `fsspec`.
يمكنك حفظ وتحميل مجموعات البيانات من أي تخزين سحابي بطريقة Pythonic.
الق نظرة على الجدول التالي لبعض أمثلة مزودي التخزين السحابي المدعومين:

| مزود التخزين | تطبيق نظام الملفات |
| ------------ | ------------------- |
| Amazon S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |
| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |
| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |
| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs) |
| Google Drive | [gdrivefs](https://github.com/intake/gdrivefs) |
| Oracle Cloud Storage | [ocifs](https://ocifs.readthedocs.io/en/latest/) |

سيوضح لك هذا الدليل كيفية حفظ وتحميل مجموعات البيانات باستخدام أي تخزين سحابي.
فيما يلي أمثلة على S3 و Google Cloud Storage و Azure Blob Storage و Oracle Cloud Object Storage.

## قم بإعداد نظام ملفات التخزين السحابي الخاص بك

### Amazon S3

1. قم بتثبيت تطبيق نظام ملفات S3:

```
>>> pip install s3fs
```

2. حدد بيانات اعتمادك

لاستخدام اتصال مجهول، استخدم `anon=True`.
وإلا، قم بتضمين `aws_access_key_id` و `aws_secret_access_key` كلما تفاعلت مع دلو S3 خاص.

```py
>>> storage_options = {"anon": True} # للاتصال المجهول
# أو استخدم بيانات اعتمادك
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key} # للدلوات الخاصة
# أو استخدم جلسة botocore
>>> import aiobotocore.session
>>> s3_session = aiobotocore.session.AioSession(profile="my_profile_name")
>>> storage_options = {"session": s3_session}
```

3. قم بإنشاء مثيل نظام الملفات الخاص بك

```py
>>> import s3fs
>>> fs = s3fs.S3FileSystem(**storage_options)
```

### Google Cloud Storage

1. قم بتثبيت تطبيق Google Cloud Storage:

```
>>> conda install -c conda-forge gcsfs
# أو قم بالتثبيت باستخدام pip
>>> pip install gcsfs
```

2. حدد بيانات اعتمادك

```py
>>> storage_options={"token": "anon"} # للاتصال المجهول
# أو استخدم بيانات اعتمادك الافتراضية لـ gcloud أو من خدمة بيانات Google
>>> storage_options={"project": "my-google-project"}
# أو استخدم بيانات اعتمادك من مكان آخر، راجع الوثائق على https://gcsfs.readthedocs.io/
>>> storage_options={"project": "my-google-project", "token": TOKEN}
```

3. قم بإنشاء مثيل نظام الملفات الخاص بك

```py
>>> import gcsfs
>>> fs = gcsfs.GCSFileSystem(**storage_options)
```

### Azure Blob Storage

1. قم بتثبيت تطبيق Azure Blob Storage:

```
>>> conda install -c conda-forge adlfs
# أو قم بالتثبيت باستخدام pip
>>> pip install adlfs
```

2. حدد بيانات اعتمادك

```py
>>> storage_options = {"anon": True} # للاتصال المجهول
# أو استخدم بيانات اعتمادك
>>> storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY} # لنظام الملفات من الجيل الثاني
# أو استخدم بيانات اعتمادك مع نظام الملفات من الجيل الأول
>>> storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
```

3. قم بإنشاء مثيل نظام الملفات الخاص بك

```py
>>> import adlfs
>>> fs = adlfs.AzureBlobFileSystem(**storage_options)
```

### Oracle Cloud Object Storage

1. قم بتثبيت تطبيق نظام ملفات OCI:

```
>>> pip install ocifs
```

2. حدد بيانات اعتمادك

```py
>>> storage_options = {"config": "~/.oci/config", "region": "us-ashburn-1"}
```

3. قم بإنشاء مثيل نظام الملفات الخاص بك

```py
>>> import ocifs
>>> fs = ocifs.OCIFileSystem(**storage_options)
```

## قم بتحميل وحفظ مجموعات البيانات الخاصة بك باستخدام نظام ملفات التخزين السحابي الخاص بك

### قم بتنزيل وإعداد مجموعة بيانات في التخزين السحابي

يمكنك تنزيل وإعداد مجموعة بيانات في التخزين السحابي الخاص بك من خلال تحديد `output_dir` عن بعد في `download_and_prepare`.
لا تنس استخدام `storage_options` المحددة مسبقًا والتي تحتوي على بيانات اعتمادك للكتابة في تخزين سحابي خاص.

تعمل طريقة `download_and_prepare` في خطوتين:

1. أولاً، يقوم بتنزيل ملفات البيانات الخام (إن وجدت) في ذاكرة التخزين المؤقت المحلية الخاصة بك. يمكنك تعيين دليل ذاكرة التخزين المؤقت الخاصة بك من خلال تمرير `cache_dir` إلى [`load_dataset_builder`]

2. ثم يقوم بتوليد مجموعة البيانات بتنسيق Arrow أو Parquet في التخزين السحابي الخاص بك من خلال التكرار فوق ملفات البيانات الخام.

قم بتحميل برنامج بناء مجموعة بيانات من Hugging Face Hub (راجع [كيفية التحميل من Hugging Face Hub](./loading#hugging-face-hub)):

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("imdb")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

استخدم ملفات البيانات الخاصة بك (راجع [كيفية تحميل الملفات المحلية والبعيدة](./loading#local-and-remote-files)):

```py
>>> data_files = {"train": ["path/to/train.csv"]}
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("csv", data_files=data_files)
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

من المستحسن بشدة حفظ الملفات بتنسيق Parquet المضغوط لتحسين عمليات الإدخال/الإخراج من خلال تحديد `file_format="parquet"`.
وإلا، يتم حفظ مجموعة البيانات كملف Arrow غير مضغوط.

يمكنك أيضًا تحديد حجم الشرائح باستخدام `max_shard_size` (الحجم الافتراضي هو 500 ميجابايت):

```py
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet", max_shard_size="1GB")
```

#### Dask

Dask هي مكتبة حوسبة متوازية ولديها واجهة برمجة تطبيقات تشبه pandas للعمل مع مجموعات بيانات Parquet الأكبر من ذاكرة الوصول العشوائي بشكل متوازي.
يمكن لـ Dask استخدام عدة خيوط أو عمليات على جهاز واحد، أو مجموعة من الأجهزة لمعالجة البيانات بشكل متواز.
يدعم Dask البيانات المحلية ولكن أيضًا البيانات من التخزين السحابي.

لذلك، يمكنك تحميل مجموعة بيانات محفوظة كملفات Parquet مجزأة في Dask باستخدام:

```py
import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# أو إذا كانت مجموعة البيانات الخاصة بك مقسمة إلى train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)
```

يمكنك معرفة المزيد حول أطر بيانات dask في [وثائقهم](https://docs.dask.org/en/stable/dataframe.html).

## حفظ مجموعات البيانات المسلسلة

بعد معالجة مجموعة البيانات الخاصة بك، يمكنك حفظها في التخزين السحابي الخاص بك باستخدام [`Dataset.save_to_disk`]:

```py
# حفظ encoded_dataset إلى Amazon S3
>>> encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", storage_options=storage_options)
# حفظ encoded_dataset إلى Google Cloud Storage
>>> encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", storage_options=storage_options)
# حفظ encoded_dataset إلى Microsoft Azure Blob/DataLake
>>> encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", storage_options=storage_options)
```

<Tip>
تذكر تحديد بيانات اعتمادك في مثيل [نظام الملفات](#set-up-your-cloud-storage-filesystem) `fs` كلما تفاعلت مع تخزين سحابي خاص.
</Tip>

## إدراج مجموعات البيانات المسلسلة

قم بإدراج الملفات من التخزين السحابي باستخدام مثيل نظام الملفات `fs` الخاص بك، باستخدام `fs.ls`:

```py
>>> fs.ls("my-private-datasets/imdb/train", detail=False)
["dataset_info.json.json","dataset.arrow","state.json"]
```

### تحميل مجموعات البيانات المسلسلة

عندما تكون مستعدًا لاستخدام مجموعة البيانات الخاصة بك مرة أخرى، قم بتحميلها مرة أخرى باستخدام [`Dataset.load_from_disk`]:

```py
>>> from datasets import load_from_disk
# تحميل encoded_dataset من التخزين السحابي
>>> dataset = load_from_disk("s3://a-public-datasets/imdb/train", storage_options=storage_options)
>>> print(len(dataset))
25000
```