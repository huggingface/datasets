# Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ
ğŸ¤— Datasets ØªØ¯Ø¹Ù… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø²ÙˆØ¯ÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ·Ø¨ÙŠÙ‚Ø§Øª FileSystem Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ `fsspec`.
ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ ÙˆØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ ØªØ®Ø²ÙŠÙ† Ø³Ø­Ø§Ø¨ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Pythonic.
Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¨Ø¹Ø¶ Ø£Ù…Ø«Ù„Ø© Ù…Ø²ÙˆØ¯ÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…ÙŠÙ†:

| Ù…Ø²ÙˆØ¯ Ø§Ù„ØªØ®Ø²ÙŠÙ† | ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª |
| ------------ | ------------------- |
| Amazon S3 | [s3fs](https://s3fs.readthedocs.io/en/latest/) |
| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/) |
| Azure Blob/DataLake | [adlfs](https://github.com/fsspec/adlfs) |
| Dropbox | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs) |
| Google Drive | [gdrivefs](https://github.com/intake/gdrivefs) |
| Oracle Cloud Storage | [ocifs](https://ocifs.readthedocs.io/en/latest/) |

Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø­ÙØ¸ ÙˆØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ ØªØ®Ø²ÙŠÙ† Ø³Ø­Ø§Ø¨ÙŠ.
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ S3 Ùˆ Google Cloud Storage Ùˆ Azure Blob Storage Ùˆ Oracle Cloud Object Storage.

## Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

### Amazon S3

1. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ù…Ù„ÙØ§Øª S3:

```
>>> pip install s3fs
```

2. Ø­Ø¯Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ

Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§ØªØµØ§Ù„ Ù…Ø¬Ù‡ÙˆÙ„ØŒ Ø§Ø³ØªØ®Ø¯Ù… `anon=True`.
ÙˆØ¥Ù„Ø§ØŒ Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† `aws_access_key_id` Ùˆ `aws_secret_access_key` ÙƒÙ„Ù…Ø§ ØªÙØ§Ø¹Ù„Øª Ù…Ø¹ Ø¯Ù„Ùˆ S3 Ø®Ø§Øµ.

```py
>>> storage_options = {"anon": True} # Ù„Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key} # Ù„Ù„Ø¯Ù„ÙˆØ§Øª Ø§Ù„Ø®Ø§ØµØ©
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù„Ø³Ø© botocore
>>> import aiobotocore.session
>>> s3_session = aiobotocore.session.AioSession(profile="my_profile_name")
>>> storage_options = {"session": s3_session}
```

3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

```py
>>> import s3fs
>>> fs = s3fs.S3FileSystem(**storage_options)
```

### Google Cloud Storage

1. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª ØªØ·Ø¨ÙŠÙ‚ Google Cloud Storage:

```
>>> conda install -c conda-forge gcsfs
# Ø£Ùˆ Ù‚Ù… Ø¨Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pip
>>> pip install gcsfs
```

2. Ø­Ø¯Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ

```py
>>> storage_options={"token": "anon"} # Ù„Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù€ gcloud Ø£Ùˆ Ù…Ù† Ø®Ø¯Ù…Ø© Ø¨ÙŠØ§Ù†Ø§Øª Google
>>> storage_options={"project": "my-google-project"}
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ù…Ù† Ù…ÙƒØ§Ù† Ø¢Ø®Ø±ØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¹Ù„Ù‰ https://gcsfs.readthedocs.io/
>>> storage_options={"project": "my-google-project", "token": TOKEN}
```

3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

```py
>>> import gcsfs
>>> fs = gcsfs.GCSFileSystem(**storage_options)
```

### Azure Blob Storage

1. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª ØªØ·Ø¨ÙŠÙ‚ Azure Blob Storage:

```
>>> conda install -c conda-forge adlfs
# Ø£Ùˆ Ù‚Ù… Ø¨Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pip
>>> pip install adlfs
```

2. Ø­Ø¯Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ

```py
>>> storage_options = {"anon": True} # Ù„Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ
>>> storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY} # Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø«Ø§Ù†ÙŠ
# Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„
>>> storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}
```

3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

```py
>>> import adlfs
>>> fs = adlfs.AzureBlobFileSystem(**storage_options)
```

### Oracle Cloud Object Storage

1. Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ù…Ù„ÙØ§Øª OCI:

```
>>> pip install ocifs
```

2. Ø­Ø¯Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ

```py
>>> storage_options = {"config": "~/.oci/config", "region": "us-ashburn-1"}
```

3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

```py
>>> import ocifs
>>> fs = ocifs.OCIFileSystem(**storage_options)
```

## Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØ­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

### Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ

ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ†Ø²ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ `output_dir` Ø¹Ù† Ø¨Ø¹Ø¯ ÙÙŠ `download_and_prepare`.
Ù„Ø§ ØªÙ†Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… `storage_options` Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙˆØ§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ Ù„Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ ØªØ®Ø²ÙŠÙ† Ø³Ø­Ø§Ø¨ÙŠ Ø®Ø§Øµ.

ØªØ¹Ù…Ù„ Ø·Ø±ÙŠÙ‚Ø© `download_and_prepare` ÙÙŠ Ø®Ø·ÙˆØªÙŠÙ†:

1. Ø£ÙˆÙ„Ø§Ù‹ØŒ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (Ø¥Ù† ÙˆØ¬Ø¯Øª) ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ…Ø±ÙŠØ± `cache_dir` Ø¥Ù„Ù‰ [`load_dataset_builder`]

2. Ø«Ù… ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØªÙ†Ø³ÙŠÙ‚ Arrow Ø£Ùˆ Parquet ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙˆÙ‚ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù….

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Hugging Face Hub (Ø±Ø§Ø¬Ø¹ [ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Hugging Face Hub](./loading#hugging-face-hub)):

```py
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("imdb")
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

Ø§Ø³ØªØ®Ø¯Ù… Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ (Ø±Ø§Ø¬Ø¹ [ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¨Ø¹ÙŠØ¯Ø©](./loading#local-and-remote-files)):

```py
>>> data_files = {"train": ["path/to/train.csv"]}
>>> output_dir = "s3://my-bucket/imdb"
>>> builder = load_dataset_builder("csv", data_files=data_files)
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")
```

Ù…Ù† Ø§Ù„Ù…Ø³ØªØ­Ø³Ù† Ø¨Ø´Ø¯Ø© Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨ØªÙ†Ø³ÙŠÙ‚ Parquet Ø§Ù„Ù…Ø¶ØºÙˆØ· Ù„ØªØ­Ø³ÙŠÙ† Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„/Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ø¯ÙŠØ¯ `file_format="parquet"`.
ÙˆØ¥Ù„Ø§ØŒ ÙŠØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ…Ù„Ù Arrow ØºÙŠØ± Ù…Ø¶ØºÙˆØ·.

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `max_shard_size` (Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ 500 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª):

```py
>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet", max_shard_size="1GB")
```

#### Dask

Dask Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© Ø­ÙˆØ³Ø¨Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© ÙˆÙ„Ø¯ÙŠÙ‡Ø§ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª ØªØ´Ø¨Ù‡ pandas Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Parquet Ø§Ù„Ø£ÙƒØ¨Ø± Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ.
ÙŠÙ…ÙƒÙ† Ù„Ù€ Dask Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¯Ø© Ø®ÙŠÙˆØ· Ø£Ùˆ Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² ÙˆØ§Ø­Ø¯ØŒ Ø£Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø².
ÙŠØ¯Ø¹Ù… Dask Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆÙ„ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ.

Ù„Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙƒÙ…Ù„ÙØ§Øª Parquet Ù…Ø¬Ø²Ø£Ø© ÙÙŠ Dask Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…:

```py
import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…Ù‚Ø³Ù…Ø© Ø¥Ù„Ù‰ train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)
```

ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ø£Ø·Ø± Ø¨ÙŠØ§Ù†Ø§Øª dask ÙÙŠ [ÙˆØ«Ø§Ø¦Ù‚Ù‡Ù…](https://docs.dask.org/en/stable/dataframe.html).

## Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø©

Ø¨Ø¹Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡Ø§ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.save_to_disk`]:

```py
# Ø­ÙØ¸ encoded_dataset Ø¥Ù„Ù‰ Amazon S3
>>> encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", storage_options=storage_options)
# Ø­ÙØ¸ encoded_dataset Ø¥Ù„Ù‰ Google Cloud Storage
>>> encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", storage_options=storage_options)
# Ø­ÙØ¸ encoded_dataset Ø¥Ù„Ù‰ Microsoft Azure Blob/DataLake
>>> encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", storage_options=storage_options)
```

<Tip>
ØªØ°ÙƒØ± ØªØ­Ø¯ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø¹ØªÙ…Ø§Ø¯Ùƒ ÙÙŠ Ù…Ø«ÙŠÙ„ [Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª](#set-up-your-cloud-storage-filesystem) `fs` ÙƒÙ„Ù…Ø§ ØªÙØ§Ø¹Ù„Øª Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ø³Ø­Ø§Ø¨ÙŠ Ø®Ø§Øµ.
</Tip>

## Ø¥Ø¯Ø±Ø§Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø©

Ù‚Ù… Ø¨Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø«ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª `fs` Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `fs.ls`:

```py
>>> fs.ls("my-private-datasets/imdb/train", detail=False)
["dataset_info.json.json","dataset.arrow","state.json"]
```

### ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø©

Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø³ØªØ¹Ø¯Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.load_from_disk`]:

```py
>>> from datasets import load_from_disk
# ØªØ­Ù…ÙŠÙ„ encoded_dataset Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ
>>> dataset = load_from_disk("s3://a-public-datasets/imdb/train", storage_options=storage_options)
>>> print(len(dataset))
25000
```