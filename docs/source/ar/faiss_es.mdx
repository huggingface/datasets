# فهرس البحث

[FAISS](https://github.com/facebookresearch/faiss) و [Elasticsearch](https://www.elastic.co/elasticsearch/) يتيحان إمكانية البحث عن أمثلة في مجموعة بيانات. يمكن أن يكون هذا مفيدًا عندما تريد استرداد أمثلة محددة من مجموعة بيانات ذات صلة بمهمة معالجة اللغات الطبيعية الخاصة بك. على سبيل المثال، إذا كنت تعمل على مهمة "الأسئلة والأجوبة المفتوحة المجال"، فقد ترغب في إرجاع الأمثلة ذات الصلة فقط بالإجابة على سؤالك. 

سيوضح هذا الدليل كيفية إنشاء فهرس لمجموعة البيانات الخاصة بك يسمح بالبحث فيها. 

## FAISS 

يسترد FAISS المستندات بناءً على تشابه تمثيلاتها المتجهية. في هذا المثال، ستقوم بتوليد التمثيلات المتجهية باستخدام نموذج [DPR](https://huggingface.co/transformers/model_doc/dpr.html). 

1. قم بتنزيل نموذج DPR من 🤗 Transformers: 

   ```py
   >>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
   >>> import torch
   >>> torch.set_grad_enabled(False)
   >>> ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
   >>> ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
   ```

2. قم بتحميل مجموعة البيانات واحسب التمثيلات المتجهية: 

   ```py
   >>> from datasets import load_dataset
   >>> ds = load_dataset('crime_and_punish', split='train[:100]')
   >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example["line"], return_tensors="pt"))[0][0].numpy()})
   ```

3. قم بإنشاء الفهرس باستخدام [`Dataset.add_faiss_index`]: 

   ```py
   >>> ds_with_embeddings.add_faiss_index(column='embeddings')
   ```

4. الآن يمكنك استعلام مجموعة البيانات الخاصة بك باستخدام فهرس `embeddings`. قم بتحميل نموذج DPR Question Encoder، وابحث عن سؤال باستخدام [`Dataset.get_nearest_examples`]: 

   ```py
   >>> from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
   >>> q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
   >>> q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

   >>> question = "Is it serious ?"
   >>> question_embedding = q_encoder(**q_tokenizer(question, return_tensors="pt"))[0][0].numpy()
   >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)
   >>> retrieved_examples["line"][0]
   '_that_ serious? It is not serious at all. It’s simply a fantasy to amuse\r\n'
   ```

5. يمكنك الوصول إلى الفهرس باستخدام [`Dataset.get_index`] واستخدامه لعمليات خاصة، على سبيل المثال، استعلامه باستخدام `range_search`: 

   ```py
   >>> faiss_index = ds_with_embeddings.get_index('embeddings').faiss_index
   >>> limits, distances, indices = faiss_index.range_search(x=question_embedding.reshape(1, -1), thresh=0.95)
   ```

6. عندما تنتهي من الاستعلام، احفظ الفهرس على القرص باستخدام [`Dataset.save_faiss_index`]: 

   ```py
   >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')
   ```

7. أعد تحميله لاحقًا باستخدام [`Dataset.load_faiss_index`]: 

   ```py
   >>> ds = load_dataset('crime_and_punish', split='train[:100]')
   >>> ds.load_faiss_index('embeddings', 'my_index.faiss')
   ```

## Elasticsearch 

على عكس FAISS، يسترد Elasticsearch المستندات بناءً على تطابقات دقيقة. 

ابدأ تشغيل Elasticsearch على جهازك، أو راجع [دليل تثبيت Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html) إذا لم يكن لديك بالفعل. 

1. قم بتحميل مجموعة البيانات التي تريد إنشاء فهرس لها: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   ```

2. قم ببناء الفهرس باستخدام [`Dataset.add_elasticsearch_index`]: 

   ```py
   >>> squad.add_elasticsearch_index("context", host="localhost", port="9200")
   ```

3. بعد ذلك، يمكنك استعلام فهرس `context` باستخدام [`Dataset.get_nearest_examples`]: 

   ```py
   >>> query = "machine"
   >>> scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
   >>> retrieved_examples["title"][0]
   'Computational_complexity_theory'
   ```

4. إذا كنت تريد إعادة استخدام الفهرس، فقم بتعريف معلمة `es_index_name` عند بناء الفهرس: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   >>> squad.add_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
   >>> squad.get_index("context").es_index_name
   hf_squad_val_context
   ```

5. أعد تحميله لاحقًا باستخدام اسم الفهرس عند استدعاء [`Dataset.load_elasticsearch_index`]: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   >>> squad.load_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
   >>> query = "machine"
   >>> scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
   ```

لاستخدامات Elasticsearch المتقدمة، يمكنك تحديد تكوينك الخاص باستخدام إعدادات مخصصة: 

   ```py
   >>> import elasticsearch as es
   >>> import elasticsearch.helpers
   >>> from elasticsearch import Elasticsearch
   >>> es_client = Elasticsearch([{"host": "localhost", "port": "9200"}]) # العميل الافتراضي
   >>> es_config = {
   ...     "settings": {
   ...         "number_of_shards": 1,
   ...         "analysis": {"analyzer": {"stop_standard": {"type": "standard", "stopwords": "_english_"}}},
   ...     },
   ...     "mappings": {"properties": {"text": {"type": "text", "analyzer": "standard", "similarity": "BM25"}}},
   ... } # التهيئة الافتراضية
   >>> es_index_name = "hf_squad_context" # اسم الفهرس في Elasticsearch
   >>> squad.add_elasticsearch_index("context", es_client=es_client, es_config=es_config, es_index_name=es_index_name)
   ```