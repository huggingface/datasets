# ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«

[FAISS](https://github.com/facebookresearch/faiss) Ùˆ [Elasticsearch](https://www.elastic.co/elasticsearch/) ÙŠØªÙŠØ­Ø§Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù…Ø«Ù„Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ù…ÙÙŠØ¯Ù‹Ø§ Ø¹Ù†Ø¯Ù…Ø§ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø£Ù…Ø«Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ù…Ù‡Ù…Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ù‡Ù…Ø© "Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…Ø¬Ø§Ù„"ØŒ ÙÙ‚Ø¯ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙÙ‚Ø· Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ. 

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡Ø§. 

## FAISS 

ÙŠØ³ØªØ±Ø¯ FAISS Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡ ØªÙ…Ø«ÙŠÙ„Ø§ØªÙ‡Ø§ Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ØªÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ [DPR](https://huggingface.co/transformers/model_doc/dpr.html). 

1. Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ DPR Ù…Ù† ðŸ¤— Transformers: 

   ```py
   >>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
   >>> import torch
   >>> torch.set_grad_enabled(False)
   >>> ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
   >>> ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
   ```

2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø­Ø³Ø¨ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©: 

   ```py
   >>> from datasets import load_dataset
   >>> ds = load_dataset('crime_and_punish', split='train[:100]')
   >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example["line"], return_tensors="pt"))[0][0].numpy()})
   ```

3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.add_faiss_index`]: 

   ```py
   >>> ds_with_embeddings.add_faiss_index(column='embeddings')
   ```

4. Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ‡Ø±Ø³ `embeddings`. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ DPR Question EncoderØŒ ÙˆØ§Ø¨Ø­Ø« Ø¹Ù† Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.get_nearest_examples`]: 

   ```py
   >>> from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
   >>> q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
   >>> q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

   >>> question = "Is it serious ?"
   >>> question_embedding = q_encoder(**q_tokenizer(question, return_tensors="pt"))[0][0].numpy()
   >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)
   >>> retrieved_examples["line"][0]
   '_that_ serious? It is not serious at all. Itâ€™s simply a fantasy to amuse\r\n'
   ```

5. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.get_index`] ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø®Ø§ØµØ©ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `range_search`: 

   ```py
   >>> faiss_index = ds_with_embeddings.get_index('embeddings').faiss_index
   >>> limits, distances, indices = faiss_index.range_search(x=question_embedding.reshape(1, -1), thresh=0.95)
   ```

6. Ø¹Ù†Ø¯Ù…Ø§ ØªÙ†ØªÙ‡ÙŠ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ Ø§Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.save_faiss_index`]: 

   ```py
   >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')
   ```

7. Ø£Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.load_faiss_index`]: 

   ```py
   >>> ds = load_dataset('crime_and_punish', split='train[:100]')
   >>> ds.load_faiss_index('embeddings', 'my_index.faiss')
   ```

## Elasticsearch 

Ø¹Ù„Ù‰ Ø¹ÙƒØ³ FAISSØŒ ÙŠØ³ØªØ±Ø¯ Elasticsearch Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©. 

Ø§Ø¨Ø¯Ø£ ØªØ´ØºÙŠÙ„ Elasticsearch Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø²ÙƒØŒ Ø£Ùˆ Ø±Ø§Ø¬Ø¹ [Ø¯Ù„ÙŠÙ„ ØªØ«Ø¨ÙŠØª Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html) Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙƒ Ø¨Ø§Ù„ÙØ¹Ù„. 

1. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù‡Ø§: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   ```

2. Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.add_elasticsearch_index`]: 

   ```py
   >>> squad.add_elasticsearch_index("context", host="localhost", port="9200")
   ```

3. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‡Ø±Ø³ `context` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.get_nearest_examples`]: 

   ```py
   >>> query = "machine"
   >>> scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
   >>> retrieved_examples["title"][0]
   'Computational_complexity_theory'
   ```

4. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø±Ø³ØŒ ÙÙ‚Ù… Ø¨ØªØ¹Ø±ÙŠÙ Ù…Ø¹Ù„Ù…Ø© `es_index_name` Ø¹Ù†Ø¯ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   >>> squad.add_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
   >>> squad.get_index("context").es_index_name
   hf_squad_val_context
   ```

5. Ø£Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ø§Ù„ÙÙ‡Ø±Ø³ Ø¹Ù†Ø¯ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`Dataset.load_elasticsearch_index`]: 

   ```py
   >>> from datasets import load_dataset
   >>> squad = load_dataset('squad', split='validation')
   >>> squad.load_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
   >>> query = "machine"
   >>> scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
   ```

Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Elasticsearch Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ ØªÙƒÙˆÙŠÙ†Ùƒ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø®ØµØµØ©: 

   ```py
   >>> import elasticsearch as es
   >>> import elasticsearch.helpers
   >>> from elasticsearch import Elasticsearch
   >>> es_client = Elasticsearch([{"host": "localhost", "port": "9200"}]) # Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
   >>> es_config = {
   ...     "settings": {
   ...         "number_of_shards": 1,
   ...         "analysis": {"analyzer": {"stop_standard": {"type": "standard", "stopwords": "_english_"}}},
   ...     },
   ...     "mappings": {"properties": {"text": {"type": "text", "analyzer": "standard", "similarity": "BM25"}}},
   ... } # Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
   >>> es_index_name = "hf_squad_context" # Ø§Ø³Ù… Ø§Ù„ÙÙ‡Ø±Ø³ ÙÙŠ Elasticsearch
   >>> squad.add_elasticsearch_index("context", es_client=es_client, es_config=es_config, es_index_name=es_index_name)
   ```