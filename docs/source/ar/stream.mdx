# Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø±

ÙŠØªÙŠØ­ Ù„Ùƒ Ø¨Ø« Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙˆÙ† ØªÙ†Ø²ÙŠÙ„Ù‡Ø§. ÙŠØªÙ… Ø¨Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ù‚Ù„Ùƒ Ø¹Ø¨Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙˆÙ‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ø¹Ù†Ø¯Ù…Ø§:

- Ù„Ø§ ØªØ±ÙŠØ¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ù„Ù„ØºØ§ÙŠØ©.
- ÙŠØªØ¬Ø§ÙˆØ² Ø­Ø¬Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù‚Ø±Øµ Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.
- ØªØ±ÙŠØ¯ Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ ÙÙ‚Ø· Ù…Ù† Ø¹ÙŠÙ†Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø³Ø±Ø¹Ø©.

![ØµÙˆØ±Ø© Ù…ØªØ­Ø±ÙƒØ© ØªÙˆØ¶Ø­ Ø¨Ø« Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif)

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠØ¨Ù„Øº Ø­Ø¬Ù… Ù‚Ø³Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) 1.2 ØªÙŠØ±Ø§Ø¨Ø§ÙŠØªØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø«. Ù‚Ù… Ø¨Ø¨Ø« Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `streaming=True` ÙÙŠ [`load_dataset`] ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'ØªØ£Ø³Ø³Øª Golden Bees ÙÙŠ Ø¹Ø§Ù… 2015ØŒ ÙˆÙ‡ÙŠ Ù…Ù†ØµØ© ØªÙˆØ¸ÙŠÙ Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ø®ØµØµØ© Ù„Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù„ ÙˆÙˆÙƒØ§Ù„Ø§Øª Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙˆÙ…Ø¬Ø§Ù„Ø³ Ø§Ù„Ø¹Ù…Ù„. ÙˆÙ‚Ø¯ Ø·ÙˆØ±Øª Ø§Ù„Ø´Ø±ÙƒØ© ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø®ØµØµØ© Ù„Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ†Ø¨Ø¤ÙŠØ© ÙØ±ÙŠØ¯Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù„ÙØ±ØµØ© Ø¹Ù…Ù„ ÙˆØ¬Ø°Ø¨Ù‡Ù….'ØŒ ...
```

ÙƒÙ…Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø« Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØµÙ†ÙˆØ¹Ø© Ù…Ù† Ù…Ù„ÙØ§Øª Ù…Ø­Ù„ÙŠØ© Ø¯ÙˆÙ† Ø¥Ø¬Ø±Ø§Ø¡ Ø£ÙŠ ØªØ­ÙˆÙŠÙ„. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØªÙ… Ø¨Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ù‚Ù„Ùƒ Ø¹Ø¨Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙˆÙ‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ Ø¹Ù†Ø¯Ù…Ø§:

- Ù„Ø§ ØªØ±ÙŠØ¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠØ© ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Arrow.
- Ø³ÙŠØªØ¬Ø§ÙˆØ² Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙˆÙ„Ø© Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù‚Ø±Øµ Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.
- ØªØ±ÙŠØ¯ Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ ÙÙ‚Ø· Ù…Ù† Ø¹ÙŠÙ†Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø³Ø±Ø¹Ø©.

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø« Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠØ© Ù…ÙƒÙˆÙ†Ø© Ù…Ù† Ù…Ø¦Ø§Øª Ù…Ù„ÙØ§Øª JSONL Ø§Ù„Ù…Ø¶ØºÙˆØ·Ø© Ù…Ø«Ù„ [oscar-corpus/OSCAR-2201](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ±:

```py
>>> from datasets import load_dataset
>>> data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}
>>> dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)
>>> print(next(iter(dataset)))
{'id': 0, 'text': 'ØªØ£Ø³Ø³Øª Golden Bees ÙÙŠ Ø¹Ø§Ù… 2015ØŒ ÙˆÙ‡ÙŠ Ù…Ù†ØµØ© ØªÙˆØ¸ÙŠÙ Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ø®ØµØµØ© Ù„Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù„ ÙˆÙˆÙƒØ§Ù„Ø§Øª Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙˆÙ…Ø¬Ø§Ù„Ø³ Ø§Ù„Ø¹Ù…Ù„. ÙˆÙ‚Ø¯ Ø·ÙˆØ±Øª Ø§Ù„Ø´Ø±ÙƒØ© ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø®ØµØµØ© Ù„Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙ†Ø¨Ø¤ÙŠØ© ÙØ±ÙŠØ¯Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù„ÙØ±ØµØ© Ø¹Ù…Ù„ ÙˆØ¬Ø°Ø¨Ù‡Ù….'ØŒ ...
```

ÙŠØ¤Ø¯ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø« Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù†ÙˆØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© (Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒØ§Ø¦Ù† [`Dataset`] Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ)ØŒ Ø§Ù„Ù…Ø¹Ø±ÙˆÙ Ø¨Ø§Ø³Ù… [`IterableDataset`]. ÙŠØ­ØªÙˆÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ø®Ø§Øµ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø®Ø§ØµØ© Ø¨Ù‡ Ù…Ù† Ø·Ø±Ù‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ø£Ø¯Ù†Ø§Ù‡.

> Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ¹Ø¯ [`IterableDataset`] Ù…ÙÙŠØ¯Ø© Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ù…Ø«Ù„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬. Ù„Ø§ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`IterableDataset`] Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù„Ø£Ù†Ùƒ Ù…Ø¶Ø·Ø± Ù„Ù„ØªÙ†Ù‚Ù„ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø­Ù„Ù‚Ø© for. Ø³ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ± ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†Ù‚Ù„ Ù…Ù†Ùƒ Ø§Ù„ØªÙ†Ù‚Ù„ Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙÙŠ Ø¯Ù„ÙŠÙ„ [Dataset vs. IterableDataset](./about_mapstyle_vs_iterable).

## Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª

Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ ÙƒØ§Ø¦Ù† [`Dataset`] Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ [`IterableDataset`] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`~Dataset.to_iterable_dataset`]. ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ Ù‡Ø°Ø§ Ø£Ø³Ø±Ø¹ Ù…Ù† ØªØ¹ÙŠÙŠÙ† ÙˆØ³ÙŠØ· `streaming=True` ÙÙŠ [`load_dataset`] Ù„Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØªÙ… Ø¨Ø«Ù‡Ø§ Ù…Ù† Ù…Ù„ÙØ§Øª Ù…Ø­Ù„ÙŠØ©.

```py
>>> from datasets import load_dataset

# Ø£Ø³Ø±Ø¹ ğŸ‡
>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset()

# Ø£Ø¨Ø·Ø£ ğŸ¢
>>> iterable_dataset = load_dataset("food101", streaming=True)
```

ØªØ¯Ø¹Ù… Ø¯Ø§Ù„Ø© [`~Dataset.to_iterable_dataset`] Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ [`IterableDataset`]. Ù‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø©ØŒ ÙˆØªØ±ØºØ¨ ÙÙŠ Ø®Ù„Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ ØªÙ…ÙƒÙŠÙ† Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø²ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PyTorch DataLoader.

```py
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101")
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # ØªØ¬Ø²Ø¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # Ø®Ù„Ø· ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¬Ø²Ø¦Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚Øª Ù„Ù„Ø®Ù„Ø· Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„
dataloader = torch.utils.data.DataLoader(iterable_datasetØŒ num_workers=4)  # ØªØ¹ÙŠÙŠÙ† 64 / 4 = 16 Ø´Ø¸ÙŠØ© Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…Ø®Ù„ÙˆØ·Ø© Ù„ÙƒÙ„ Ø¹Ø§Ù…Ù„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„
```

## Ø®Ù„Ø·

Ù…Ø«Ù„ ÙƒØ§Ø¦Ù† [`Dataset`] Ø§Ù„Ø¹Ø§Ø¯ÙŠØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø®Ù„Ø· [`IterableDataset`] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`IterableDataset.shuffle`].

ÙŠØªØ­ÙƒÙ… ÙˆØ³ÙŠØ· `buffer_size` ÙÙŠ Ø­Ø¬Ù… Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ø§Ø®ØªÙŠØ§Ø± Ø£Ù…Ø«Ù„Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù†Ù‡. Ù„Ù†ÙØªØ±Ø¶ Ø£Ù† Ù„Ø¯ÙŠÙƒ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙŠÙˆÙ† Ù…Ø«Ø§Ù„ØŒ ÙˆØªØ­Ø¯Ø¯ Ø­Ø¬Ù… Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ù„Ù‰ Ø¹Ø´Ø±Ø© Ø¢Ù„Ø§Ù. Ø³ÙŠØ®ØªØ§Ø± [`IterableDataset.shuffle`] Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø£ÙˆÙ„ Ø¹Ø´Ø±Ø© Ø¢Ù„Ø§Ù Ù…Ø«Ø§Ù„ ÙÙŠ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª. ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø£Ù…Ø«Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©. ÙŠÙƒÙˆÙ† Ø­Ø¬Ù… Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ 1000.

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en"ØŒ split='train'ØŒ streaming=True)
>>> shuffled_dataset = dataset.shuffle(seed=42ØŒ buffer_size=10_000)
```

> ØªÙ„Ù…ÙŠØ­: Ø³ÙŠÙ‚ÙˆÙ… [`IterableDataset.shuffle`] Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø®Ù„Ø· ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø¥Ø°Ø§ ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©.

## Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø®Ù„Ø·

ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø­ÙŠØ§Ù†ØŒ Ù‚Ø¯ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø®Ù„Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ÙƒÙ„ ÙØªØ±Ø©. Ø³ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ Ù…Ù†Ùƒ ØªØ¹ÙŠÙŠÙ† Ø¨Ø°Ø±Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ ÙØªØ±Ø©. Ø§Ø³ØªØ®Ø¯Ù… [`IterableDataset.set_epoch`] Ø¨ÙŠÙ† Ø§Ù„ÙØªØ±Ø§Øª Ù„Ø¥Ø®Ø¨Ø§Ø± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ÙØªØ±Ø© Ø§Ù„ØªÙŠ Ø£Ù†Øª ÙÙŠÙ‡Ø§.

ØªØµØ¨Ø­ Ø§Ù„Ø¨Ø°Ø±Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„: `Ø§Ù„Ø¨Ø°Ø±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© + Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©`.

```py
>>> for epoch in range(epochs):
...     shuffled_dataset.set_epoch(epoch)
...     for example in shuffled_dataset:
...         ...
```

## ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø·Ø±ÙŠÙ‚ØªÙŠÙ†:

- [`IterableDataset.take`] ÙŠØ¹ÙŠØ¯ Ø£ÙˆÙ„ `n` Ø£Ù…Ø«Ù„Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> dataset = load_dataset('oscar', "unshuffled_deduplicated_en"ØŒ split='train'ØŒ streaming=True)
>>> dataset_head = dataset.take(2)
>>> list(dataset_head)
[{'id': 0ØŒ 'text': 'Ù‚Ø±ÙŠØ© Ù…ØªÙ†Ø¯ÙŠØ±ÙŠ Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø§Ù„Ø±Ø¤ÙŠØ© ...'}, {'id': 1ØŒ 'text': 'Ù„Ø§ ØªØ³ØªØ·ÙŠØ¹ Ù„ÙŠÙ„ÙŠ Ø¬ÙŠÙ…Ø³ Ù…Ø­Ø§Ø±Ø¨Ø© Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ ...'}]
```

- [`IterableDataset.skip`] ÙŠØªØ¬Ø§Ù‡Ù„ Ø£ÙˆÙ„ `n` Ø£Ù…Ø«Ù„Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠØ¹ÙŠØ¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©:

```py
>>> train_dataset = shuffled_dataset.skip(1000)
```

> ØªØ­Ø°ÙŠØ±: ØªÙ…Ù†Ø¹ `take` Ùˆ`skip` Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù„Ù€ `shuffle` Ù„Ø£Ù†Ù‡Ø§ ØªÙ‚ÙÙ„ ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¬Ø²Ø¦Ø©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ `Ø®Ù„Ø·` Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù‚Ø¨Ù„ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§.

<a id='interleave_datasets'></a>

## Ø§Ù„ØªØ¯Ø§Ø®Ù„

ÙŠÙ…ÙƒÙ† Ø£Ù† [`interleave_datasets`] Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† [`IterableDataset`] Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰. ØªØ¬Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø£Ù…Ø«Ù„Ø© Ù…ØªÙ†Ø§ÙˆØ¨Ø© Ù…Ù† ÙƒÙ„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©.

```py
>>> from datasets import interleave_datasets
>>> en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en"ØŒ split='train'ØŒ streaming=TrueØŒ trust_remote_code=True)
>>> fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr"ØŒ split='train'ØŒ streaming=TrueØŒ trust_remote_code=True)

>>> multilingual_dataset = interleave_datasets([en_datasetØŒ fr_dataset])
>>> list(multilingual_dataset.take(2))
[{'text': 'Ù‚Ø±ÙŠØ© Ù…ØªÙ†Ø¯ÙŠØ±ÙŠ Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø§Ù„Ø±Ø¤ÙŠØ© ...'}, {'text': "Ù…ÙŠØ¯ÙŠØ§ Ù„Ù„Ù†Ù‚Ø§Ø´ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© ÙˆØ§Ù„Ø£Ø¯Ø¨ ..."}]
```

Ø­Ø¯Ø¯ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ù† ÙƒÙ„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ ÙƒÙŠÙÙŠØ© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ù† ÙƒÙ„ Ù…Ù†Ù‡Ø§ ÙˆØ¯Ù…Ø¬Ù‡Ø§. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† ÙˆØ³ÙŠØ· `probabilities` Ù…Ø¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©:

```py
>>> multilingual_dataset_with_oversampling = interleave_datasets([en_datasetØŒ fr_dataset]ØŒ probabilities=[0.8ØŒ 0.2]ØŒ seed=42)
>>> list(multilingual_dataset_with_oversampling.take(2))
[{'text': 'Ù‚Ø±ÙŠØ© Ù…ØªÙ†Ø¯ÙŠØ±ÙŠ Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø§Ù„Ø±Ø¤ÙŠØ© ...'}, {'text': 'Ù„Ø§ ØªØ³ØªØ·ÙŠØ¹ Ù„ÙŠÙ„ÙŠ Ø¬ÙŠÙ…Ø³ Ù…Ø­Ø§Ø±Ø¨Ø© Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ ...'}]
```

Ø­ÙˆØ§Ù„ÙŠ 80% Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…ØµÙ†ÙˆØ¹Ø© Ù…Ù† `en_dataset`ØŒ Ùˆ20% Ù…Ù† `fr_dataset`.

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ `stopping_strategy`. Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©ØŒ `first_exhausted`ØŒ Ù‡ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ÙØ±Ø¹ÙŠØŒ Ø£ÙŠ ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø¬Ø±Ø¯ Ù†ÙØ§Ø¯ Ø¹ÙŠÙ†Ø§Øª Ø¥Ø­Ø¯Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ `stopping_strategy=all_exhausted` Ù„ØªÙ†ÙÙŠØ° Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØªÙˆÙ‚Ù Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„ Ø¹ÙŠÙ†Ø© ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ Ø¥Ø°Ø§ Ù†ÙØ¯Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙØ³ØªØ¹ÙˆØ¯ Ø¥Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡ Ø­ØªÙ‰ ÙŠØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø¹ÙŠØ§Ø± Ø§Ù„ØªÙˆÙ‚Ù.

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§ØªØŒ ÙØ³ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù„Ù‰ `max_length_datasets*nb_dataset samples`.

## Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ù…ÙŠØ© ÙˆØ§Ù„Ø¥Ø²Ø§Ù„Ø© ÙˆØ§Ù„ØµØ¨

ØªØ³Ù…Ø­ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±Ù‚ Ù…ÙÙŠØ¯Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ùˆ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§ Ø£Ùˆ ØªØºÙŠÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª.

### Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ù…ÙŠØ©

Ø§Ø³ØªØ®Ø¯Ù… [`IterableDataset.rename_column`] Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø¹Ù…ÙˆØ¯ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. ÙŠØªÙ… Ù†Ù‚Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙØ¹Ù„ÙŠÙ‹Ø§ ØªØ­Øª Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ø¬Ø±Ø¯ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ Ù…ÙƒØ§Ù†Ù‡.

Ù‚Ù… Ø¨ØªØ²ÙˆÙŠØ¯ [`IterableDataset.rename_column`] Ø¨Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠØŒ ÙˆØ§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en'ØŒ streaming=TrueØŒ split='train'ØŒ trust_remote_code=True)
>>> dataset = dataset.rename_column("text"ØŒ "content")
```

### Ø¥Ø²Ø§Ù„Ø©

Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø£ÙƒØ«Ø±ØŒ Ù‚Ù… Ø¨ØªØ²ÙˆÙŠØ¯ [`IterableDataset.remove_columns`] Ø¨Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡. Ù‚Ù… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙˆÙÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('mc4', 'en'ØŒ streaming=TrueØŒ split='train'ØŒ trust_remote_code=True)
>>> dataset = dataset.remove_columns('timestamp')
```

### Cast

[`IterableDataset.cast`] ÙŠØºÙŠØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ù„ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©. ØªØ£Ø®Ø° Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© `Features` Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙƒÙˆØ³ÙŠØ· Ù„Ù‡Ø§. ÙŠÙˆØ¶Ø­ Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙŠÙÙŠØ© ØªØºÙŠÙŠØ± Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù€ `ClassLabel` Ùˆ`Value`:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc'ØŒ split='train'ØŒ streaming=True)
>>> dataset.features
{'sentence1': Value(dtype='string'ØŒ id=None)ØŒ
'sentence2': Value(dtype='string'ØŒ id=None)ØŒ
'label': ClassLabel(num_classes=2ØŒ names=['not_equivalent'ØŒ 'equivalent']ØŒ names_file=NoneØŒ id=None)ØŒ
'idx': Value(dtype='int32'ØŒ id=None)}

>>> from datasets import ClassLabelØŒ Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=['negative'ØŒ 'positive'])
>>> new_features["idx"] = Value('int64')
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string'ØŒ id=None)ØŒ
'sentence2': Value(dtype='string'ØŒ id=None)ØŒ
'label': ClassLabel(num_classes=2ØŒ names=['negative'ØŒ 'positive']ØŒ names_file=NoneØŒ id=None)ØŒ
'idx': Value(dtype='int64'ØŒ id=None)}
```

> ØªÙ„Ù…ÙŠØ­: ÙŠØ¹Ù…Ù„ Ø§Ù„ØµØ¨ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…ØªÙˆØ§ÙÙ‚ÙŠÙ†. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØµØ¨ Ø¹Ù…ÙˆØ¯ Ø¨Ø³Ù…Ø§Øª `Value('int32')` Ø¥Ù„Ù‰ `Value('bool')` Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙŠØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£Ø­Ø§Ø¯ ÙˆØµÙØ§Ø±.

Ø§Ø³ØªØ®Ø¯Ù… [`IterableDataset.cast_column`] Ù„ØªØºÙŠÙŠØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ù„Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·. Ù…Ø±Ø± Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡ ÙƒÙˆØ³ÙŠØ·Ø§Øª:

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100ØŒ mono=TrueØŒ id=None)}

>>> dataset = dataset.cast_column("audio"ØŒ Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000ØŒ mono=TrueØŒ id=None)}
```
## Map

Ø¹Ù„Ù‰ ØºØ±Ø§Ø± ÙˆØ¸ÙŠÙØ© [`Dataset.map`] Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø§Ø¯ÙŠØ© [`Dataset`]ØŒ ØªÙˆÙØ± Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets ÙˆØ¸ÙŠÙØ© [`IterableDataset.map`] Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© [`IterableDataset`].

ØªØ·Ø¨Ù‚ [`IterableDataset.map`] Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… Ø¨Ø« Ø§Ù„Ø£Ù…Ø«Ù„Ø©.

ØªØªÙŠØ­ Ù„Ùƒ ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ Ø£Ùˆ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª. ÙŠÙ…ÙƒÙ† Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø­ØªÙ‰ Ø¥Ù†Ø´Ø§Ø¡ ØµÙÙˆÙ ÙˆØ£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©.

ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙŠÙÙŠØ© ØªÙˆÙƒÙŠÙ†Ø²Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [`IterableDataset`]. ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¥Ø®Ø±Ø§Ø¬ "dict":

```py
>>> def add_prefix(example):
...     example['text'] = 'My text: ' + example['text']
...     return example
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`IterableDataset.map`]:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> updated_dataset = dataset.map(add_prefix)
>>> list(updated_dataset.take(3))
[{'id': 0, 'text': 'My text: Mtendere Village was inspired by...'},
{'id': 1, 'text': 'My text: Lily James cannot fight the music...'},
{'id': 2, 'text': 'My text: "I\'d love to help kickstart...'}]
```

Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø¢Ø®Ø±ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø©ØŒ Ø³ÙˆÙ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`IterableDataset.map`]. Ø¹Ù†Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ØŒ ØªØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ ÙÙ‚Ø· Ø¨Ø¹Ø¯ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø«Ø§Ù„ Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©. ÙŠØ³Ù…Ø­ Ù‡Ø°Ø§ Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù‚Ø¨Ù„ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§.

Ø­Ø¯Ø¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø¥Ø²Ø§Ù„ØªÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ³ÙŠØ· `remove_columns` ÙÙŠ [`IterableDataset.map`]:

```py
>>> updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
>>> list(updated_dataset.take(3))
[{'text': 'My text: Mtendere Village was inspired by...'},
{'text': 'My text: Lily James cannot fight the music...'},
{'text': 'My text: "I\'d love to help kickstart...'}]
```

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª

ÙŠØ¯Ø¹Ù… [`IterableDataset.map`] Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø¯ÙØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø©. Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹Ø§ØªØŒ Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `batched=True`. Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ 1000ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø¶Ø¨Ø·Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ³ÙŠØ· `batch_size`. ÙŠÙØªØ­ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø§Ø¨ Ø£Ù…Ø§Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø«Ù„ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø²Ø§ØªØŒ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£Ù‚ØµØ±ØŒ ÙˆØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

#### Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø²Ø§Øª

```py
>>> from datasets import load_dataset
>>> from transformers import AutoTokenizer
>>> dataset = load_dataset("mc4", "en", streaming=True, split="train", trust_remote_code=True)
>>> tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
>>> def encode(examples):
...     return tokenizer(examples['text'], truncation=True, padding='max_length')
>>> dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
>>> next(iter(dataset))
{'input_ids': [101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}
```

<Tip>

Ø±Ø§Ø¬Ø¹ Ø£Ù…Ø«Ù„Ø© Ø£Ø®Ø±Ù‰ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª ÙÙŠ ÙˆØ«Ø§Ø¦Ù‚ [Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø°Ø§Øª Ø§Ù„Ø¯ÙØ¹Ø§Øª](./process#batch-processing). ØªØ¹Ù…Ù„ Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¨Ø«.

</Tip>

### Ù…Ø±Ø´Ø­

ÙŠÙ…ÙƒÙ†Ùƒ ØªØµÙÙŠØ© Ø§Ù„ØµÙÙˆÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø´Ø±Ø·ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.filter`]. ÙÙ‡Ùˆ ÙŠØ¹ÙŠØ¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø´Ø±Ø· Ù…Ø­Ø¯Ø¯:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)
>>> start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
>>> next(iter(start_with_ar))
{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)?...'}
```

ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ù„Ù€ [`Dataset.filter`] Ø§Ù„ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ† `with_indices=True`:

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> list(even_dataset.take(3))
[{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...'},
{'id': 2, 'text': '"I\'d love to help kickstart continued development! And 0 EUR/month...'},
{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...'}]
```

## Ø§Ù„Ø¨Ø« ÙÙŠ Ø­Ù„Ù‚Ø© ØªØ¯Ø±ÙŠØ¨

ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ [`IterableDataset`] ÙÙŠ Ø­Ù„Ù‚Ø© ØªØ¯Ø±ÙŠØ¨. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù‚Ù… Ø¨Ø®Ù„Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

<frameworkcontent>
<pt>
```py
>>> seed, buffer_size = 42, 10_000
>>> dataset = dataset.shuffle(seed, buffer_size=buffer_size)
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù„Ù‚Ø© ØªØ¯Ø±ÙŠØ¨ Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ø¨Ø¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```py
>>> import torch
>>> from torch.utils.data import DataLoader
>>> from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
>>> from tqdm import tqdm
>>> dataset = dataset.with_format("torch")
>>> dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
>>> device = 'cuda' if torch.cuda.is_available() else 'cpu'
>>> model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
>>> model.train().to(device)
>>> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
>>> for epoch in range(3):
...     dataset.set_epoch(epoch)
...     for i, batch in enumerate(tqdm(dataloader, total=5)):
...         if i == 5:
...             break
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs[0]
...         loss.backward()
...         optimizer.step()
...         optimizer.zero_grad()
...         if i % 10 == 0:
...             print(f"loss: {loss}")
```

</pt>
</frameworkcontent>

<!-- TODO: Ø§ÙƒØªØ¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ TF! -->

### Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªÙƒØ±Ø§Ø±

Ø¥Ø°Ø§ ØªÙˆÙ‚ÙØª Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŒ ÙÙ‚Ø¯ ØªØ±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø­ÙŠØ« ØªÙˆÙ‚ÙØª. Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù„Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ±Ø§Ø¨Ø·ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.

Ù„Ø§ ØªÙˆÙØ± Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¨Ø« Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¥Ù„Ù‰ ÙÙ‡Ø±Ø³ Ù…Ø«Ø§Ù„ Ù…Ø­Ø¯Ø¯ Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¹Ù…Ù„ Ù…Ù†Ù‡ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`IterableDataset.state_dict`] Ùˆ [`IterableDataset.load_state_dict`] Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¹Ù…Ù„ Ù…Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„ÙƒØŒ Ø¹Ù„Ù‰ ØºØ±Ø§Ø± Ù…Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù‡ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ±Ø§Ø¨Ø·:

```python
>>> iterable_dataset = Dataset.from_dict({"a": range(6)}).to_iterable_dataset(num_shards=3)
>>> for idx, example in enumerate(iterable_dataset):
...     print(example)
...     if idx == 2:
...         state_dict = iterable_dataset.state_dict()
...         print("checkpoint")
...         break
>>> iterable_dataset.load_state_dict(state_dict)
>>> print(f"restart from checkpoint")
>>> for example in iterable_dataset:
...     print(example)
```

Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹:

```
{'a': 0}
{'a': 1}
{'a': 2}
checkpoint
restart from checkpoint
{'a': 3}
{'a': 4}
{'a': 5}
```

ØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡ØŒ ØªØ­ØªÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¨Ø« Ø¨ØªØªØ¨Ø¹ Ø§Ù„Ø´Ø±ÙŠØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªØªÙ… Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ ÙˆÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©ØŒ ÙˆØªØ®Ø²Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ `state_dict`.

Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ù…Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ØŒ ØªÙ‚ÙˆÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØªØ®Ø·ÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø§Ù„ØªÙŠ ØªÙ… Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ø³Ø§Ø¨Ù‚Ù‹Ø§ Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù Ø§Ù„Ø¹Ù…Ù„ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.

Ø«Ù… ØªÙ‚Ø±Ø£ Ø§Ù„Ø´Ø±ÙŠØ­Ø© ÙˆØªØªØ®Ø·Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø­ØªÙ‰ ØªØµÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù…Ù† Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´.

Ù„Ø°Ù„ÙƒØŒ ÙØ¥Ù† Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ù…Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„ØºØ§ÙŠØ©ØŒ Ø­ÙŠØ« Ù„Ù† ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø§Ù„ØªÙŠ ØªÙ…Øª Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ø¨Ø§Ù„ÙØ¹Ù„. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙØ¥Ù† Ø§Ø³ØªØ¦Ù†Ø§Ù Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙŠØ³ ÙÙˆØ±ÙŠÙ‹Ø§ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… Ù„Ø£Ù†Ù‡ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø´Ø±ÙŠØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆÙŠØªØ®Ø·Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø­ØªÙ‰ ÙŠØµÙ„ Ø¥Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´.

ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ù…Ø¹ `StatefulDataLoader` Ù…Ù† `torchdata`:

```python
>>> from torchdata.stateful_dataloader import StatefulDataLoader
>>> iterable_dataset = load_dataset("deepmind/code_contests", streaming=True, split="train")
>>> dataloader = StatefulDataLoader(iterable_dataset, batch_size=32, num_workers=4)
>>> # checkpoint
>>> state_dict = dataloader.state_dict() # uses iterable_dataset.state_dict() under the hood
>>> # resume from checkpoint
>>> dataloader.load_state_dict(state_dict) # uses iterable_dataset.load_state_dict() under the hood
```

<Tip>

ØªØ³ØªØ£Ù†Ù Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù…Ù† Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø°ÙŠ ØªÙ… Ø­ÙØ¸Ù‡Ø§ ÙÙŠÙ‡ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… `.shuffle()`: ÙŠØªÙ… ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ù…Ø®Ø§Ø²Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø®Ù„Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù ÙˆÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ù…Ù„Ø¡ Ø§Ù„Ù…Ø®Ø§Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.

</Tip>