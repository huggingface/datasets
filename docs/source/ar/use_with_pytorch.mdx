# الاستخدام مع PyTorch

هذه الوثيقة هي مقدمة سريعة لاستخدام `datasets` مع PyTorch، مع التركيز بشكل خاص على كيفية الحصول على كائنات `torch.Tensor` من مجموعات البيانات الخاصة بنا، وكيفية استخدام PyTorch `DataLoader` و Hugging Face `Dataset` بأفضل أداء.

## تنسيق مجموعة البيانات

بشكل افتراضي، تعيد مجموعات البيانات كائنات بايثون عادية: أعداد صحيحة، أعداد ذات فاصلة عائمة، سلاسل نصية، قوائم، إلخ.

للحصول على تنسورات PyTorch بدلاً من ذلك، يمكنك تعيين تنسيق مجموعة البيانات إلى `pytorch` باستخدام [`Dataset.with_format`]:

```py
>>> from datasets import Dataset
>>> data = [[1, 2],[3, 4]]
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': tensor([1, 2])}
>>> ds[:2]
{'data': tensor([[1, 2],
         [3, 4]])}
```

<Tip>

كائن [`Dataset`] هو غلاف لجدول Arrow، والذي يسمح بقراءات سريعة بدون نسخ من المصفوفات في مجموعة البيانات إلى تنسورات PyTorch.

</Tip>

لتحميل البيانات كـتنسورات على GPU، حدد وسيط `device`:

```py
>>> import torch
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> ds = ds.with_format("torch", device=device)
>>> ds[0]
{'data': tensor([1, 2], device='cuda:0')}
```

## المصفوفات متعددة الأبعاد

إذا كانت مجموعة البيانات الخاصة بك تتكون من مصفوفات متعددة الأبعاد، فستلاحظ أنها تعتبر افتراضيًا نفس التنسور إذا كان الشكل ثابتًا:

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]  # fixed shape
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': tensor([[1, 2],
        [3, 4]])}
```

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3]],[[4, 5, 6],[7, 8]]]  # varying shape
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': [tensor([1, 2]), tensor([3])]}
```

ومع ذلك، يتطلب هذا المنطق غالبًا مقارنات شكل بطيئة ونسخ بيانات.

للتغلب على ذلك، يجب عليك استخدام [`Array`] نوع الميزة بشكل صريح وتحديد شكل تنسوراتك:

```py
>>> from datasets import Dataset, Features, Array2D
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
>>> features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
>>> ds = Dataset.from_dict({"data": data}, features=features)
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': tensor([[1, 2],
         [3, 4]])}
>>> ds[:2]
{'data': tensor([[[1, 2],
          [3, 4]],
 
         [[5, 6],
          [7, 8]]])}
```

## أنواع الميزات الأخرى

يتم تحويل بيانات [`ClassLabel`] بشكل صحيح إلى تنسورات:

```py
>>> from datasets import Dataset, Features, ClassLabel
>>> labels = [0, 0, 1]
>>> features = Features({"label": ClassLabel(names=["negative", "positive"])})
>>> ds = Dataset.from_dict({"label": labels}, features=features)
>>> ds = ds.with_format("torch")
>>> ds[:3]
{'label': tensor([0, 0, 1])}
```

لا تتغير الكائنات النصية والثنائية، لأن PyTorch يدعم الأرقام فقط.

يتم أيضًا دعم أنواع الميزات [`Image`] و [`Audio`].

<Tip>

لاستخدام نوع الميزة [`Image`]`، ستحتاج إلى تثبيت الميزة الإضافية `vision` كما يلي:
`pip install datasets[vision]`.

</Tip>

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> images = ["path/to/image.png"] * 10
>>> features = Features({"image": Image()})
>>> ds = Dataset.from_dict({"image": images}, features=features) 
>>> ds = ds.with_format("torch")
>>> ds[0]["image"].shape
torch.Size([512, 512, 4])
>>> ds[0]
{'image': tensor([[[255, 215, 106, 255],
         [255, 215, 106, 255],
         ...,
         [255, 255, 255, 255],
         [255, 255, 255, 255]]], dtype=torch.uint8)}
>>> ds[:2]["image"].shape
torch.Size([2, 512, 512, 4])
>>> ds[:2]
{'image': tensor([[[[255, 215, 106, 255],
          [255, 215, 106, 255],
          ...,
          [255, 255, 255, 255],
          [255, 255, 255, 255]]]], dtype=torch.uint8)}
```

<Tip>

لاستخدام نوع الميزة [`Audio`]`، ستحتاج إلى تثبيت الميزة الإضافية `audio` كما يلي:
`pip install datasets[audio]`.

</Tip>

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> audio = ["path/to/audio.wav"] * 10
>>> features = Features({"audio": Audio()})
>>> ds = Dataset.from_dict({"audio": audio}, features=features)
>>> ds = ds.with_format("torch")
>>> ds[0]["audio"]["array"]
tensor([ 6.1035e-05,  1.5259e-05,  1.6785e-04,  ..., -1.5259e-05,
        -1.5259e-05,  1.5259e-05])
>>> ds[0]["audio"]["sampling_rate"]
tensor(44100)
```

## تحميل البيانات

مثل كائنات `torch.utils.data.Dataset`، يمكن تمرير كائن [`Dataset`] مباشرةً إلى PyTorch `DataLoader`:

```py
>>> import numpy as np
>>> from datasets import Dataset
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(16)
>>> label = np.random.randint(0, 2, size=16)
>>> ds = Dataset.from_dict({"data": data, "label": label}).with_format("torch")
>>> dataloader = DataLoader(ds, batch_size=4)
>>> for batch in dataloader:
...     print(batch)
{'data': tensor([0.0047, 0.4979, 0.6726, 0.8105]), 'label': tensor([0, 1, 0, 1])}
{'data': tensor([0.4832, 0.2723, 0.4259, 0.2224]), 'label': tensor([0, 0, 0, 0])}
{'data': tensor([0.5837, 0.3444, 0.4658, 0.6417]), 'label': tensor([0, 1, 0, 0])}
{'data': tensor([0.7022, 0.1225, 0.7228, 0.8259]), 'label': tensor([1, 1, 1, 1])}
```

### تحسين تحميل البيانات

هناك عدة طرق يمكنك من خلالها زيادة سرعة تحميل البيانات، والتي يمكن أن توفر لك الوقت، خاصة إذا كنت تعمل مع مجموعات بيانات كبيرة.

يوفر PyTorch تحميل بيانات متوازي، واسترداد دفعات من المؤشرات بدلاً من ذلك، والبث لتخطي مجموعة البيانات دون تنزيلها على القرص.

#### استخدام Workers متعددة

يمكنك موازاة تحميل البيانات باستخدام وسيط `num_workers` من PyTorch `DataLoader` والحصول على إنتاجية أعلى.

تحت الغطاء، يبدأ `DataLoader` عدد `num_workers` من العمليات.

تتم إعادة تحميل كل عملية لمجموعة البيانات التي تم تمريرها إلى `DataLoader` ويتم استخدامها لاستعلام الأمثلة.

لا يؤدي إعادة تحميل مجموعة البيانات داخل عامل إلى ملء ذاكرة الوصول العشوائي (RAM)، نظرًا لأنه يقوم ببساطة برسم خريطة الذاكرة لمجموعة البيانات مرة أخرى من القرص.

```py
>>> import numpy as np
>>> from datasets import Dataset, load_from_disk
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(10_000)
>>> Dataset.from_dict({"data": data}).save_to_disk("my_dataset")
>>> ds = load_from_disk("my_dataset").with_format("torch")
>>> dataloader = DataLoader(ds, batch_size=32, num_workers=4)
```

### بث البيانات

قم ببث مجموعة بيانات عن طريق تحميلها كـ [`IterableDataset`]. يسمح لك هذا بالتقدم تدريجيًا خلال مجموعة بيانات عن بُعد دون تنزيلها على القرص أو ملفات بيانات محلية.

تعرف على المزيد حول نوع مجموعة البيانات الأنسب لحالتك الاستخدامية في دليل [الاختيار بين مجموعة بيانات عادية أو مجموعة بيانات قابلة للبث](./about_mapstyle_vs_iterable).

ترث مجموعة البيانات القابلة للبث من `datasets` من `torch.utils.data.IterableDataset` بحيث يمكنك تمريرها إلى `torch.utils.data.DataLoader`:

```py
>>> import numpy as np
>>> from datasets import Dataset, load_dataset
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(10_000)
>>> Dataset.from_dict({"data": data}).push_to_hub("<username>/my_dataset")  # Upload to the Hugging Face Hub
>>> my_iterable_dataset = load_dataset("<username>/my_dataset", streaming=True, split="train")
>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32)
```

إذا تم تقسيم مجموعة البيانات إلى عدة شظايا (أي إذا كانت مجموعة البيانات تتكون من عدة ملفات بيانات)، فيمكنك البث بالتوازي باستخدام `num_workers`:

```py
>>> my_iterable_dataset = load_dataset("deepmind/code_contests", streaming=True, split="train")
>>> my_iterable_dataset.n_shards
39
>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32, num_workers=4)
```

في هذه الحالة، يتم منح كل عامل مجموعة فرعية من قائمة الشظايا للبث منها.

إذا كنت بحاجة إلى DataLoader يمكنك حفظ نقطة تفتيش له واستئنافه في منتصف التدريب، فيمكنك استخدام `StatefulDataLoader` من [torchdata](https://github.com/pytorch/data):

```py
>>> from torchdata.stateful_dataloader import StatefulDataLoader
>>> my_iterable_dataset = load_dataset("deepmind/code_contests", streaming=True, split="train")
>>> dataloader = StatefulDataLoader(my_iterable_dataset, batch_size=32, num_workers=4)
>>> # save in the middle of training
>>> state_dict = dataloader.state_dict()
>>> # and resume later
>>> dataloader.load_state_dict(state_dict)
```

هذا ممكن بفضل [`IterableDataset.state_dict`] و [`IterableDataset.load_state_dict`].

### موزعة

لتقسيم مجموعة البيانات الخاصة بك عبر عقد التدريب الخاصة بك، يمكنك استخدام [`datasets.distributed.split_dataset_by_node`]:

```python
import os
from datasets.distributed import split_dataset_by_node

ds = split_dataset_by_node(ds, rank=int(os.environ["RANK"]), world_size=int(os.environ["WORLD_SIZE"]))
```

يعمل هذا لكل من مجموعات البيانات على الطريقة الخرائطية ومجموعات البيانات القابلة للبث.

يتم تقسيم مجموعة البيانات للعقدة في الرتبة `rank` في مجموعة من العقد بحجم `world_size`.

بالنسبة لمجموعات البيانات على الطريقة الخرائطية:

يتم تعيين كتلة بيانات لكل عقدة، على سبيل المثال، يتم إعطاء الرتبة 0 الكتلة الأولى من مجموعة البيانات.

بالنسبة لمجموعات البيانات القابلة للبث:

إذا كان عدد شظايا مجموعة البيانات عاملًا لـ `world_size` (أي إذا كان `dataset.n_shards % world_size == 0`)،

يتم توزيع الشظايا بالتساوي عبر العقد، وهو الأكثر تحسينًا.

وإلا، تحتفظ كل عقدة بمثال واحد من أصل `world_size`، متخطية الأمثلة الأخرى.

يمكن أيضًا دمج هذا مع `torch.utils.data.DataLoader` إذا كنت تريد أن يستخدم كل عقدة عدة عمال لتحميل البيانات.