# الفروق بين Dataset و IterableDataset

هناك نوعان من كائنات مجموعة البيانات، [`Dataset`] و [`IterableDataset`]. يعتمد نوع مجموعة البيانات التي تختار استخدامها أو إنشائها على حجم مجموعة البيانات. بشكل عام، يعد [`IterableDataset`] مثاليًا لمجموعات البيانات الكبيرة (مئات غيغابايت!) بسبب سلوكه "الكَسول" ومزايا السرعة، في حين أن [`Dataset`] رائع لكل شيء آخر. ستعمل هذه الصفحة على مقارنة الاختلافات بين [`Dataset`] و [`IterableDataset`] لمساعدتك في اختيار كائن مجموعة البيانات المناسب لك.

## التنزيل والبث

عند استخدام [`Dataset`] عادي، يمكنك الوصول إليه باستخدام `my_dataset[0]`. يوفر هذا الوصول العشوائي إلى الصفوف. تسمى مجموعات البيانات هذه أيضًا مجموعات بيانات "على الطراز الخرائطي". على سبيل المثال، يمكنك تنزيل ImageNet-1k والوصول إلى أي صف على النحو التالي:

```python
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", split="train")  # يقوم بتنزيل مجموعة البيانات الكاملة
print(imagenet[0])
```

ولكن أحد التحذيرات هو أنه يجب عليك تخزين مجموعة البيانات بالكامل على القرص الخاص بك أو في الذاكرة، مما يمنعك من الوصول إلى مجموعات البيانات الأكبر من القرص. نظرًا لأنه قد يصبح غير مريح لمجموعات البيانات الكبيرة، هناك نوع آخر من مجموعات البيانات، وهو [`IterableDataset`].

عند استخدام `IterableDataset`، يمكنك الوصول إليها باستخدام حلقة `for` لتحميل البيانات تدريجيًا أثناء التنقل خلال مجموعة البيانات. بهذه الطريقة، يتم تحميل جزء صغير فقط من الأمثلة في الذاكرة، ولا تقوم بكتابة أي شيء على القرص. على سبيل المثال، يمكنك بث مجموعة بيانات ImageNet-1k دون تنزيلها على القرص على النحو التالي:

```python
from datasets import load_dataset

imagenet = load_dataset("imagenet-1k", split="train", streaming=True)  # سيبدأ تحميل البيانات عند التنقل خلال مجموعة البيانات
for example in imagenet:
  print(example)
  break
```

يمكن للبث قراءة البيانات عبر الإنترنت دون كتابة أي ملف على القرص. على سبيل المثال، يمكنك بث مجموعات البيانات المصنوعة من شظايا متعددة، يبلغ حجم كل منها مئات الغيغابايت مثل [C4](https://huggingface.co/datasets/c4) أو [OSCAR](https://huggingface.co/datasets/oscar) أو [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en). تعرف على المزيد حول كيفية بث مجموعة بيانات في [دليل بث مجموعة البيانات](./stream).

ولكن هذا ليس هو الاختلاف الوحيد، لأن السلوك "الكَسول" لـ `IterableDataset` موجود أيضًا عند إنشاء مجموعة البيانات ومعالجتها.

## إنشاء مجموعات بيانات على الطراز الخرائطي ومجموعات البيانات القابلة للتنقل

يمكنك إنشاء [`Dataset`] باستخدام القوائم أو القواميس، ويتم تحويل البيانات بالكامل إلى Arrow بحيث يمكنك الوصول إلى أي صف بسهولة:

```python
my_dataset = Dataset.from_dict({"col_1": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})
print(my_dataset[0])
```

ومن ناحية أخرى، لإنشاء `IterableDataset`، يجب عليك توفير طريقة "كسولة" لتحميل البيانات. في Python، نستخدم بشكل عام وظائف المولدات. تقوم هذه الوظائف `yield` بمثال واحد في كل مرة، مما يعني أنه لا يمكنك الوصول إلى صف عن طريق تقطيعه مثل [`Dataset`] عادي:

```python
def my_generator(n):
    for i in range(n):
        yield {"col_1": i}

my_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={"n": 10})
for example in my_iterable_dataset:
    print(example)
    break
```

## تحميل الملفات المحلية بالكامل والتدريجي

من الممكن تحويل ملفات البيانات المحلية أو البعيدة إلى [`Dataset`] Arrow باستخدام [`load_dataset`]:

```python
data_files = {"train": ["path/to/data.csv"]}
my_dataset = load_dataset("csv", data_files=data_files, split="train")
print(my_dataset[0])
```

ومع ذلك، يتطلب ذلك خطوة تحويل من تنسيق CSV إلى تنسيق Arrow، والتي تستغرق وقتًا ومساحة على القرص إذا كانت مجموعة البيانات كبيرة. لتوفير مساحة على القرص وتخطي خطوة التحويل، يمكنك تحديد `IterableDataset` عن طريق البث من الملفات المحلية مباشرة. بهذه الطريقة، يتم قراءة البيانات تدريجيًا من الملفات المحلية أثناء التنقل خلال مجموعة البيانات:

```python
data_files = {"train": ["path/to/data.csv"]}
my_iterable_dataset = load_dataset("csv", data_files=data_files, split="train", streaming=True)
for example in my_iterable_dataset:  # يقرأ هذا الملف CSV تدريجيًا أثناء التنقل خلال مجموعة البيانات
    print(example)
    break  
```

يتم دعم العديد من تنسيقات الملفات، مثل CSV وJSONL وParquet، بالإضافة إلى ملفات الصور والصوت. يمكنك العثور على مزيد من المعلومات في الأدلة المقابلة لتحميل مجموعات البيانات [الجدولية](./tabular_load) و [النصية](./nlp_load) و [الرؤية](./image_load)، و [الصوتية](./audio_load]).

## معالجة البيانات المتحمسة ومعالجة البيانات الكسولة

عند معالجة كائن [`Dataset`] باستخدام [`Dataset.map`]`، تتم معالجة مجموعة البيانات بالكامل على الفور وإعادتها. هذا مشابه لكيفية عمل `pandas` على سبيل المثال.

```python
my_dataset = my_dataset.map(process_fn)  # يتم تطبيق `process_fn` على جميع الأمثلة في مجموعة البيانات
print(my_dataset[0])
```

ومن ناحية أخرى، نظرًا للطابع "الكسول" لـ `IterableDataset`، فإن استدعاء [`IterableDataset.map`] لا يطبق دالة `map` على مجموعة البيانات بالكامل. بدلاً من ذلك، يتم تطبيق دالة `map` الخاصة بك أثناء التنقل. بسبب ذلك، يمكنك تسلسل خطوات المعالجة المتعددة وسيتم تشغيلها جميعًا مرة واحدة عند بدء التنقل خلال مجموعة البيانات:

```python
my_iterable_dataset = my_iterable_dataset.map(process_fn_1)
my_iterable_dataset = my_iterable_dataset.filter(filter_fn)
my_iterable_dataset = my_iterable_dataset.map(process_fn_2)

# يتم تطبيق `process_fn_1` و `filter_fn` و `process_fn_2` أثناء التنقل خلال مجموعة البيانات
for example in my_iterable_dataset:
    print(example)
    break
```

## الخلط الدقيق والتقريبي السريع

عند خلط [`Dataset`] باستخدام [`Dataset.shuffle`]`، فإنك تطبق خلطًا دقيقًا لمجموعة البيانات. تعمل هذه الطريقة عن طريق أخذ قائمة من المؤشرات `[0، 1، 2، ... len (my_dataset) - 1]` وخلط هذه القائمة. بعد ذلك، يعيد الوصول إلى `my_dataset[0]` الصف والمؤشر المحدد بواسطة العنصر الأول من تعيين المؤشرات الذي تم خلطه:

```python
my_dataset = my_dataset.shuffle(seed=42)
print(my_dataset[0])
```

نظرًا لأنه لا يمكننا الوصول العشوائي إلى الصفوف في حالة `IterableDataset`، لا يمكننا استخدام قائمة مؤشرات مختلطة والوصول إلى صف في موضع عشوائي. يمنع ذلك استخدام الخلط الدقيق. بدلاً من ذلك، يتم استخدام خلط تقريبي سريع في [`IterableDataset.shuffle`]. يستخدم هذا الخلط مخزن مؤقت لاختيار أمثلة عشوائية بشكل تكراري من مجموعة البيانات. نظرًا لأنه لا يزال يتم قراءة مجموعة البيانات بشكل تكراري، فإنها توفر أداء سرعة ممتاز:

```python
my_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)
for example in my_iterable_dataset:
    print(example)
    break
```

ولكن استخدام مخزن مؤقت للخلط ليس كافيًا لتوفير خلط مرضٍ لتدريب نموذج التعلم الآلي. لذلك، فإن [`IterableDataset.shuffle`] يقوم أيضًا بخلط شظايا مجموعة البيانات إذا كانت مجموعة البيانات الخاصة بك تتكون من ملفات أو مصادر متعددة:

```python
# Stream from the internet
my_iterable_dataset = load_dataset("deepmind/code_contests", split="train", streaming=True)
my_iterable_dataset.n_shards  # 39

# Stream from local files
data_files = {"train": [f"path/to/data_{i}.csv" for i in range(1024)]}
my_iterable_dataset = load_dataset("csv", data_files=data_files, split="train", streaming=True)
my_iterable_dataset.n_shards  # 1024

# From a generator function
def my_generator(n, sources):
    for source in sources:
        for example_id_for_current_source in range(n):
            yield {"example_id": f"{source}_{example_id_for_current_source}"}

gen_kwargs = {"n": 10, "sources": [f"path/to/data_{i}" for i in range(1024)]}
my_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)
my_iterable_dataset.n_shards  # 1024
```

## اختلافات السرعة

تعتمد كائنات [`Dataset`] العادية على Arrow الذي يوفر وصولًا عشوائيًا سريعًا إلى الصفوف. بفضل تعيين الذاكرة وحقيقة أن Arrow هو تنسيق في الذاكرة، فإن قراءة البيانات من القرص لا تقوم بمكالمات النظام الباهظة وإلغاء التسلسل. يوفر تحميل البيانات بشكل أسرع عند التنقل باستخدام حلقة `for` عن طريق التنقل خلال دفعات سجلات Arrow المتجاورة.

ومع ذلك، بمجرد أن يكون لديك [`Dataset`] يحتوي على تعيين مؤشرات (عبر [`Dataset.shuffle`] على سبيل المثال)، يمكن أن تصبح السرعة أبطأ بعشر مرات. يرجع ذلك إلى وجود خطوة إضافية للحصول على مؤشر الصف الذي سيتم قراءته باستخدام تعيين المؤشرات، والأهم من ذلك، أنك لم تعد تقرأ قطعًا متجاورة من البيانات. لاستعادة السرعة، ستحتاج إلى إعادة كتابة مجموعة البيانات بالكامل على القرص الخاص بك مرة أخرى باستخدام [`Dataset.flatten_indices`]`، والذي يزيل تعيين المؤشرات. ومع ذلك، فقد يستغرق ذلك الكثير من الوقت اعتمادًا على حجم مجموعة البيانات الخاصة بك:

```python
my_dataset[0]  # سريع
my_dataset = my_dataset.shuffle(seed=42)
my_dataset[0]  # أبطأ بعشر مرات
my_dataset = my_dataset.flatten_indices()  # إعادة كتابة مجموعة البيانات المخلوطة على القرص كقطع متجاورة من البيانات
my_dataset[0]  # سريع مرة أخرى
```

في هذه الحالة، نوصي بالتبديل إلى [`IterableDataset`] والاستفادة من طريقة الخلط التقريبي السريع [`IterableDataset.shuffle`]. يقوم هذا الخلط بخلط ترتيب الشظايا فقط وإضافة مخزن مؤقت للخلط إلى مجموعة البيانات الخاصة بك، مما يحافظ على سرعة مجموعة البيانات المثالية. يمكنك أيضًا إعادة خلط مجموعة البيانات بسهولة:

```python
for example in enumerate(my_iterable_dataset):  # سريع
    pass

shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)

for example in enumerate(shuffled_iterable_dataset):  # سريع مثل السابق
    pass

shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # إعادة الخلط باستخدام بذرة أخرى فوري

for example in enumerate(shuffled_iterable_dataset):  # لا يزال سريعًا مثل السابق
    pass
```

إذا كنت تستخدم مجموعة البيانات الخاصة بك على عدة عصور، فإن البذرة الفعالة لخلط ترتيب الشظايا في مخزن الخلط هي `seed + epoch`. يجعل هذا من السهل إعادة خلط مجموعة البيانات بين العصور:

```python
for epoch in range(n_epochs):
    my_iterable_dataset.set_epoch(epoch)
    for example in my_iterable_dataset:  # سريع + إعادة الخلط في كل عصر باستخدام `effective_seed = seed + epoch`
        pass
```

## الاختلافات بين نقاط التفتيش والاستئناف

إذا توقفت حلقة التدريب الخاصة بك، فقد ترغب في إعادة تشغيل التدريب من حيث توقفت. للقيام بذلك، يمكنك حفظ نقطة تفتيش لنموذجك ومؤشرات الأداء، بالإضافة إلى برنامج تحميل البيانات الخاص بك.

لاستئناف التنقل خلال مجموعة بيانات على الطراز الخرائطي، يمكنك ببساطة تخطي الأمثلة الأولى:

```python
my_dataset = my_dataset.select(range(start_index, len(dataset)))
```

ولكن إذا كنت تستخدم `DataLoader` مع `Sampler`، فيجب عليك بدلاً من ذلك حفظ حالة برنامج العينات الخاص بك (قد تحتاج إلى كتابة برنامج عينات مخصص يسمح بالاستئناف). من ناحية أخرى، لا توفر مجموعات البيانات القابلة للتنقل الوصول العشوائي إلى مثال محدد لاستئنافه. ولكن يمكنك استخدام [`IterableDataset.state_dict`] و [`IterableDataset.load_state_dict`] لاستئناف العمل من نقطة تفتيش، على غرار ما يمكنك القيام به للنماذج ومؤشرات الأداء:

```python
>>> iterable_dataset = Dataset.from_dict({"a": range(6)}).to_iterable_dataset(num_shards=3)
>>> # الحفظ في منتصف التدريب
>>> state_dict = iterable_dataset.state_dict()
>>> # والاستئناف لاحقًا
>>> iterable_dataset.load_state_dict(state_dict)
```

تحت الغطاء، تحتفظ مجموعة البيانات القابلة للتنقل بتتبع الشظية الحالية التي تتم قراءتها ومؤشر المثال في الشظية الحالية ويتم تخزين هذه المعلومات في `state_dict`. لاستئناف العمل من نقطة تفتيش، تقوم مجموعة البيانات بتخطي جميع الشظايا التي تم قراءتها سابقًا لإعادة التشغيل من الشظية الحالية. ثم يقوم بقراءة الشظية ويجري تخطي الأمثلة حتى يصل إلى المثال الدقيق من نقطة التفتيش. لذلك، فإن إعادة تشغيل مجموعة البيانات سريعة للغاية، حيث لن تقوم بإعادة قراءة الشظايا التي تم التنقل خلالها بالفعل. ومع ذلك، فإن استئناف مجموعة البيانات ليس فوريًا بشكل عام، حيث يتعين عليه إعادة تشغيل القراءة من بداية الشظية الحالية ويجري تخطي الأمثلة حتى يصل إلى موقع نقطة التفتيش.

يمكن استخدام هذا مع `StatefulDataLoader` من `torchdata`، راجع [البث مع برنامج تحميل بيانات PyTorch](./use_with_pytorch#stream-data).

## التبديل من الطراز الخرائطي إلى القابل للتنقل

إذا كنت تريد الاستفادة من السلوك "الكسول" لـ [`IterableDataset`] أو مزايا السرعة الخاصة به، فيمكنك التبديل من [`Dataset`] على الطراز الخرائطي إلى [`IterableDataset`]:

```python
my_iterable_dataset = my_dataset.to_iterable_dataset()
```

إذا كنت تريد خلط مجموعة البيانات الخاصة بك أو [استخدامها مع برنامج تحميل بيانات PyTorch](./use_with_pytorch#stream-data)، فإننا نوصي بإنشاء [`IterableDataset`] مجزأ:

```python
my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)
my_iterable_dataset.n_shards  # 1024
```
