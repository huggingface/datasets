# استخدام مجموعات البيانات مع TensorFlow

هذا المستند هو مقدمة سريعة لاستخدام `datasets` مع TensorFlow، مع التركيز بشكل خاص على كيفية الحصول على كائنات `tf.Tensor` من مجموعات البيانات الخاصة بنا، وكيفية بث البيانات من كائنات `Dataset` من Hugging Face إلى أساليب Keras مثل `model.fit()`.

## تنسيق مجموعة البيانات

بشكل افتراضي، تعيد مجموعات البيانات كائنات Python عادية: أعداد صحيحة، أعداد ذات فاصلة عائمة، سلاسل نصية، قوائم، إلخ.

للحصول على Tensors TensorFlow بدلاً من ذلك، يمكنك تعيين تنسيق مجموعة البيانات إلى "tf":

```py
>>> from datasets import Dataset
>>> data = [[1, 2],[3, 4]]
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("tf")
>>> ds[0]
{'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>}
>>> ds[:2]
{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[1, 2],
      [3, 4]])>}
```

<Tip>

كائن [`Dataset`] هو عبارة عن غلاف (wrapper) لجدول Arrow، والذي يسمح بالقراءة السريعة من المصفوفات في مجموعة البيانات إلى Tensors TensorFlow.

</Tip>

يمكن أن يكون هذا مفيدًا لتحويل مجموعة البيانات إلى قاموس من كائنات `Tensor`، أو لكتابة مولد لتحميل عينات TF منها. إذا كنت ترغب في تحويل مجموعة البيانات بأكملها إلى `Tensor`، فما عليك سوى استعلام مجموعة البيانات بالكامل:

```py
>>> ds[:]
{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[1, 2],
        [3, 4]])>}
```

## المصفوفات متعددة الأبعاد

إذا كانت مجموعة البيانات الخاصة بك تتكون من مصفوفات متعددة الأبعاد، فستلاحظ أنها تعتبر افتراضيًا نفس المصفوفة إذا كان الشكل ثابتًا:

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]] # شكل ثابت
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("tf")
>>> ds[0]
{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[1, 2],
      [3, 4]])>}
```

وبخلاف ذلك، فإن مجموعة البيانات بتنسيق TensorFlow تُخرج `RaggedTensor` بدلاً من مصفوفة واحدة:

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3]],[[4, 5, 6],[7, 8]]] # شكل متغير
>>> ds = Dataset.from_dict({"data": data})
>>> ds = ds.with_format("torch")
>>> ds[0]
{'data': <tf.RaggedTensor [[1, 2], [3]]>}
```

ومع ذلك، يتطلب هذا المنطق غالبًا مقارنات شكل بطيئة ونسخًا للبيانات.

للتغلب على هذه المشكلة، يجب عليك استخدام نوع ميزة [`Array`] وتحديد شكل المصفوفات الخاصة بك بشكل صريح:

```py
>>> from datasets import Dataset, Features, Array2D
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
>>> features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
>>> ds = Dataset.from_dict({"data": data}, features=features)
>>> ds = ds.with_format("tf")
>>> ds[0]
{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=
 array([[1, 2],
        [3, 4]])>}
>>> ds[:2]
{'data': <tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=
 array([[[1, 2],
         [3, 4]],
 
        [[5, 6],
         [7, 8]]])>}
```

## أنواع الميزات الأخرى

تتم تحويل بيانات [`ClassLabel`] بشكل صحيح إلى المصفوفات:

```py
>>> from datasets import Dataset, Features, ClassLabel
>>> labels = [0, 0, 1]
>>> features = Features({"label": ClassLabel(names=["negative", "positive"])})
>>> ds = Dataset.from_dict({"label": labels}, features=features)
>>> ds = ds.with_format("tf")
>>> ds[:3]
{'label': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 0, 1])>}
```

كما يتم دعم السلاسل النصية والكائنات الثنائية:

```py
>>> from datasets import Dataset, Features
>>> text = ["foo", "bar"]
>>> data = [0, 1]
>>> ds = Dataset.from_dict({"text": text, "data": data})
>>> ds = ds.with_format("tf")
>>> ds[:2]
{'text': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'foo', b'bar'], dtype=object)>,
'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>}
```

يمكنك أيضًا تنسيق أعمدة معينة بشكل صريح وترك الأعمدة الأخرى بدون تنسيق:

```py
>>> ds = ds.with_format("tf", columns=["data"], output_all_columns=True)
>>> ds[:2]
{'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>,
'text': ['foo', 'bar']}
```

لا تتغير السلاسل النصية والكائنات الثنائية، نظرًا لأن PyTorch يدعم الأرقام فقط.

كما يتم دعم نوعي الميزات [`Image`] و [`Audio`].

<Tip>

لاستخدام نوع الميزة [`Image`]`، ستحتاج إلى تثبيت الميزة الإضافية `vision` كما يلي:

`pip install datasets[vision]`.

</Tip>

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> images = ["path/to/image.png"] * 10
>>> features = Features({"image": Image()})
>>> ds = Dataset.from_dict({"image": images}, features=features) 
>>> ds = ds.with_format("tf")  
>>> ds[0]
{'image': <tf.Tensor: shape=(512, 512, 4), dtype=uint8, numpy=
 array([[[255, 215, 106, 255],
         [255, 215, 106, 255],
         ...,
         [255, 255, 255, 255],
         [255, 255, 255, 255]]], dtype=uint8)>}
>>> ds[:2]
{'image': <tf.Tensor: shape=(2, 512, 512, 4), dtype=uint8, numpy=
 array([[[[255, 215, 106, 255],
          [255, 215, 106, 255],
          ...,
          [255, 255, 255, 255],
          [255, 255, 255, 255]]]], dtype=uint8)>}
```

<Tip>

لاستخدام نوع الميزة [`Audio`]`، ستحتاج إلى تثبيت الميزة الإضافية `audio` كما يلي:

`pip install datasets[audio]`.

</Tip>

```py
>>> from datasets import Dataset, Features, Audio, Image
>>> audio = ["path/to/audio.wav"] * 10
>>> features = Features({"audio": Audio()})
>>> ds = Dataset.from_dict({"audio": audio}, features=features)
>>> ds[0]["audio"]["array"]
<tf.Tensor: shape=(202311,), dtype=float32, numpy=
array([ 6.1035156e-05,  1.5258789e-05,  1.6784668e-04, ...,
-1.5258789e-05, -1.5258789e-05,  1.5258789e-05], dtype=float32)>
>>> ds[0]["audio"]["sampling_rate"]
<tf.Tensor: shape=(), dtype=int32, numpy=44100>
```

## تحميل البيانات

على الرغم من أنه يمكنك تحميل عينات فردية ودفعات عن طريق الفهرسة فقط في مجموعة البيانات الخاصة بك، إلا أن هذا لن ينجح إذا كنت تريد استخدام أساليب Keras مثل `fit()` و`predict()`. يمكنك كتابة دالة مولد تقوم بخلط وتحميل دفعات من مجموعة البيانات الخاصة بك و`fit()` عليها، ولكن يبدو ذلك مثل الكثير من العمل غير الضروري. بدلاً من ذلك، إذا كنت تريد بث البيانات من مجموعة البيانات الخاصة بك أثناء التنقل، نوصي بتحويل مجموعة البيانات إلى `tf.data.Dataset` باستخدام طريقة `to_tf_dataset()`.

تغطي فئة `tf.data.Dataset` مجموعة واسعة من حالات الاستخدام - غالبًا ما يتم إنشاؤها من المصفوفات في الذاكرة، أو باستخدام دالة تحميل لقراءة الملفات على القرص أو التخزين الخارجي. يمكن تحويل مجموعة البيانات بشكل تعسفي باستخدام طريقة `map()`، أو يمكن استخدام أساليب مثل `batch()` و`shuffle()` لإنشاء مجموعة بيانات جاهزة للتدريب. لا تعدل هذه الأساليب البيانات المخزنة بأي طريقة - بدلاً من ذلك، تقوم الأساليب ببناء رسم بياني لخط أنابيب البيانات الذي يتم تنفيذه عند التنقل خلال مجموعة البيانات، عادةً أثناء تدريب النموذج أو الاستدلال. وهذا يختلف عن طريقة `map()` لكائنات `Dataset` من Hugging Face، والتي تقوم بتشغيل دالة الخريطة على الفور وحفظ الأعمدة الجديدة أو المعدلة.

نظرًا لأنه يمكن تجميع خط أنابيب معالجة البيانات بأكمله في `tf.data.Dataset`، فإن هذا النهج يسمح بتحميل البيانات والتدريب بشكل موازي ومتوازي بشكل كبير. ومع ذلك، فإن متطلبات تجميع الرسوم البيانية يمكن أن تكون محدودة، خاصة بالنسبة إلى برامج Hugging Face لتحويل النص إلى أجزاء، والتي لا يمكن تجميعها عادةً (بعد!) كجزء من رسم بياني لـ TF. ونتيجة لذلك، فإننا ننصح عادةً بمعالجة مجموعة البيانات مسبقًا كمجموعة بيانات Hugging Face، حيث يمكن استخدام وظائف Python تعسفية، ثم تحويلها إلى `tf.data.Dataset` بعد ذلك باستخدام `to_tf_dataset()` للحصول على مجموعة بيانات مجمعة جاهزة للتدريب. لمشاهدة أمثلة على هذا النهج، يرجى الاطلاع على [الأمثلة](https://github.com/huggingface/transformers/tree/main/examples) أو [دفاتر الملاحظات](https://huggingface.co/docs/transformers/notebooks) لـ `transformers`.

### استخدام `to_tf_dataset()`

يعد استخدام `to_tf_dataset()` أمرًا مباشرًا. بمجرد معالجة مجموعة البيانات الخاصة بك وجاهزيتها، فما عليك سوى استدعائها كما يلي:

```py
>>> from datasets import Dataset
>>> data = {"inputs": [[1, 2],[3, 4]], "labels": [0, 1]}
>>> ds = Dataset.from_dict(data)
>>> tf_ds = ds.to_tf_dataset(
            columns=["inputs"],
            label_cols=["labels"],
            batch_size=2,
            shuffle=True
            )
```

يعتبر كائن `tf_ds` الذي تمت إعادته هنا جاهزًا الآن للتدريب عليه بالكامل، ويمكن تمريره مباشرة إلى `model.fit()`. لاحظ أنك قمت بتعيين حجم الدفعة عند إنشاء مجموعة البيانات، لذلك لا تحتاج إلى تحديده عند استدعاء `fit()`:

```py
>>> model.fit(tf_ds, epochs=2)
```

للاطلاع على وصف كامل للحجج، يرجى الاطلاع على وثائق [`~Dataset.to_tf_dataset`]. في كثير من الحالات، ستحتاج أيضًا إلى إضافة `collate_fn` إلى استدعائك. هذه هي الدالة التي تأخذ عناصر متعددة من مجموعة البيانات وتجمعها في دفعة واحدة. عندما يكون لجميع العناصر نفس الطول، فإن الدالة الافتراضية المدمجة ستكون كافية، ولكن بالنسبة للمهام الأكثر تعقيدًا، قد تكون هناك حاجة إلى دالة تجميع مخصصة. على وجه الخصوص، تحتوي العديد من المهام على عينات ذات أطوال تسلسل متغيرة والتي ستحتاج إلى [دالة تجميع بيانات](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator) يمكنها إضافة وسائد إلى الدفعات بشكل صحيح. يمكنك الاطلاع على أمثلة على ذلك في أمثلة NLP الخاصة بـ `transformers` [أمثلة](https://github.com/huggingface/transformers/tree/main/examples) و[دفاتر الملاحظات](https://huggingface.co/docs/transformers/notebooks)، حيث تعد أطوال التسلسل المتغيرة شائعة جدًا.

إذا وجدت أن التحميل باستخدام `to_tf_dataset` بطيء، فيمكنك أيضًا استخدام وسيط `num_workers`. يقوم هذا بإنشاء عمليات فرعية متعددة لتحميل البيانات بشكل متواز. هذه الميزة حديثة ولا تزال تجريبية إلى حد ما - يرجى إرسال مشكلة إذا واجهتك أي أخطاء أثناء استخدامها!

### متى يجب استخدام to_tf_dataset

قد يلاحظ القارئ المتمعن في هذه المرحلة أننا قدمنا نهجين لتحقيق نفس الهدف - إذا كنت تريد تمرير مجموعة البيانات إلى نموذج TensorFlow، فيمكنك إما تحويل مجموعة البيانات إلى `Tensor` أو `dict` من `Tensors` باستخدام `.with_format('tf')`، أو يمكنك تحويل مجموعة البيانات إلى `tf.data.Dataset` مع `to_tf_dataset()`. يمكن تمرير أي منهما إلى `model.fit()`، لذلك أيهما يجب أن تختار؟

الشيء الرئيسي الذي يجب إدراكه هو أنه عند تحويل مجموعة البيانات بأكملها إلى مصفوفات، تكون ثابتة ومحملة بالكامل في ذاكرة الوصول العشوائي. على الرغم من أن هذا بسيط وملائم، إلا أنه إذا انطبق أي مما يلي، فيجب عليك على الأرجح استخدام `to_tf_dataset()` بدلاً من ذلك:

- مجموعة البيانات كبيرة جدًا بحيث لا يمكن وضعها في ذاكرة الوصول العشوائي. `to_tf_dataset()` يبث دفعة واحدة فقط في كل مرة، لذلك حتى مجموعات البيانات الكبيرة جدًا يمكن التعامل معها بهذه الطريقة.

- تريد تطبيق تحويلات عشوائية باستخدام `dataset.with_transform()` أو `collate_fn`. هذا أمر شائع في عدة طرائق، مثل الزيادات الصورية عند تدريب نماذج الرؤية، أو التعتيم العشوائي عند تدريب نماذج اللغة المعتمة. سيؤدي استخدام `to_tf_dataset()` إلى تطبيق هذه التحولات في اللحظة التي يتم فيها تحميل دفعة، مما يعني أن العينات نفسها ستحصل على زيادات مختلفة في كل مرة يتم تحميلها فيها. هذا هو ما تريده عادة.

- تحتوي بياناتك على بُعد متغير، مثل النصوص المدخلة في NLP التي تتكون من أعداد متغيرة من الرموز. عند إنشاء دفعة بعينات ذات بُعد متغير، يتمثل الحل القياسي في إضافة وسائد إلى العينات الأقصر لتتناسب مع طول العينة الأطول. عند بث العينات من مجموعة بيانات باستخدام `to_tf_dataset`، يمكنك تطبيق هذه الوسادة على كل دفعة عبر `collate_fn` الخاص بك. ومع ذلك، إذا كنت تريد تحويل مثل هذه المجموعة من البيانات إلى مصفوفات كثيفة، فسيتعين عليك إضافة وسائد إلى العينات لتتناسب مع طول العينة الأطول في *مجموعة البيانات بأكملها!* يمكن أن يؤدي هذا إلى كميات هائلة من الوسائد، مما يهدر الذاكرة ويقلل من سرعة نموذجك.

### التحذيرات والقيود

في الوقت الحالي، يعيد `to_tf_dataset()` دائماً مجموعة بيانات مجمعة - وسنضيف دعمًا لمجموعات البيانات غير المجمعة قريبًا