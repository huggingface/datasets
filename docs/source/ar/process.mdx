# Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

ÙŠÙˆÙØ± ğŸ¤— Datasets Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†ÙŠØ© ÙˆÙ…Ø­ØªÙˆÙ‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙˆØªØ¹Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ù…Ù‡Ù…Ø© Ù„ØªÙ†Ø¸ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø£Ø¹Ù…Ø¯Ø© Ø¥Ø¶Ø§ÙÙŠØ©ØŒ ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚Ø§ØªØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯.

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:

- Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙÙˆÙ ÙˆØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
- Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ¥Ø²Ø§Ù„ØªÙ‡Ø§ØŒ ÙˆØ¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„Ø£Ø®Ø±Ù‰.
- ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
- Ø¯Ù…Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
- ØªØ·Ø¨ÙŠÙ‚ ØªØ­ÙˆÙŠÙ„ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø®ØµØµ.
- Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØµØ¯ÙŠØ±Ù‡Ø§.

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø­ÙˆÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø±Ø§Ø¦Ù‚ Ø£Ø®Ø±Ù‰ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªØŒ Ø£Ùˆ Ø¯Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±ØŒ Ø£Ùˆ Ø¯Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª NLP.

ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª MRPCØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ø®ØªÙŠØ§Ø±Ùƒ ÙˆÙ…ØªØ§Ø¨Ø¹Ø© Ø°Ù„Ùƒ!

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("glue", "mrpc", split="train")
```

> ØªÙØ±Ø¬Ø¹ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒØ§Ø¦Ù† [`Dataset`] Ø¬Ø¯ÙŠØ¯. Ù„Ø§ ÙŠØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹. ÙƒÙ† Ø­Ø°Ø±Ù‹Ø§ Ø¨Ø´Ø£Ù† ØªØ¬Ø§ÙˆØ² Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ!

## Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„Ø®Ù„Ø· ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ… ÙˆØ§Ù„ØªØ¬Ø²Ø¦Ø©

Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© ÙˆØ¸Ø§Ø¦Ù Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø¨Ù†ÙŠØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…ÙÙŠØ¯Ø© Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯Ù‡Ø§ ÙÙ‚Ø·ØŒ ÙˆØ¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø³ÙŠÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ ÙˆØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£ØµØºØ±.

### Ø§Ù„ØªØ±ØªÙŠØ¨

Ø§Ø³ØªØ®Ø¯Ù… [`~Dataset.sort`] Ù„ÙØ±Ø² Ù‚ÙŠÙ… Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù‚ÙŠÙ…Ù‡Ø§ Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ù‚Ø¯Ù… Ù…ØªÙˆØ§ÙÙ‚Ù‹Ø§ Ù…Ø¹ NumPy.

```py
>>> dataset["label"][:10]
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
>>> sorted_dataset = dataset.sort("label")
>>> sorted_dataset["label"][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> sorted_dataset["label"][-10:]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡ØŒ ÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ÙØ±Ø²Ù‡Ø§ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¹Ù…ÙˆØ¯.

ÙŠØªÙ… Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù‡Ø°Ù‡ Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙŠ Ø¬Ø¯ÙˆÙ„ Arrow Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ.

### Ø§Ù„Ø®Ù„Ø·

ØªÙ‚ÙˆÙ… ÙˆØ¸ÙŠÙØ© [`~Dataset.shuffle`] Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ù‚ÙŠÙ… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ù„Ù…Ø© `generator` ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… `numpy.random.Generator` Ù…Ø®ØªÙ„ÙØ© Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ù…Ø²ÙŠØ¯Ù‹Ø§ Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ø®Ù„Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

```py
>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)
>>> shuffled_dataset["label"][:10]
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]
```

ÙŠØ£Ø®Ø° Ø§Ù„Ø®Ù„Ø· Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª `[0:len(my_dataset)]` ÙˆÙŠØ®Ù„Ø·Ù‡Ø§ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ù…Ø¤Ø´Ø±Ø§Øª.

ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ø¨Ù…Ø¬Ø±Ø¯ Ø£Ù† ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¹Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ù…Ø¤Ø´Ø±Ø§ØªØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØµØ¨Ø­ Ø§Ù„Ø³Ø±Ø¹Ø© Ø£Ø¨Ø·Ø£ 10 Ù…Ø±Ø©.

ÙˆÙŠØ±Ø¬Ø¹ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ ÙˆØ¬ÙˆØ¯ Ø®Ø·ÙˆØ© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¤Ø´Ø± Ø§Ù„ØµÙ Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§ØªØŒ ÙˆØ§Ù„Ø£Ù‡Ù… Ù…Ù† Ø°Ù„ÙƒØŒ Ø£Ù†Ùƒ Ù„Ù… ØªØ¹Ø¯ ØªÙ‚Ø±Ø£ Ù‚Ø·Ø¹Ù‹Ø§ Ù…ØªØ¬Ø§ÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø±Ø¹Ø©ØŒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Dataset.flatten_indices`]ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ²ÙŠÙ„ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª.

Ø£ÙˆØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¥Ù„Ù‰ [`IterableDataset`] ÙˆØ§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø®Ù„Ø· Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹ [`IterableDataset.shuffle`]:

```py
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)
>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### Ø§Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ù„ØªØµÙÙŠØ©

Ù‡Ù†Ø§Ùƒ Ø®ÙŠØ§Ø±Ø§Ù† Ù„ØªØµÙÙŠØ© Ø§Ù„ØµÙÙˆÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: [`~Dataset.select`] Ùˆ [`~Dataset.filter`].

- ÙŠØ¹ÙŠØ¯ [`~Dataset.select`] Ø§Ù„ØµÙÙˆÙ ÙˆÙÙ‚Ù‹Ø§ Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª:

```py
>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
>>> len(small_dataset)
6
```

- ÙŠØ¹ÙŠØ¯ [`~Dataset.filter`] Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø´Ø±Ø· Ù…Ø­Ø¯Ø¯:

```py
>>> start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
>>> len(start_with_ar)
6
>>> start_with_ar["sentence1"]
['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',
'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',
'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',
"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .",
'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'
]
```

ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø£Ù† ÙŠÙ‚ÙˆÙ… [`~Dataset.filter`] Ø¨Ø§Ù„ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ† `with_indices=True`:

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> len(even_dataset)
1834
>>> len(dataset) / 2
1834.0
```

Ù…Ø§ Ù„Ù… ØªÙƒÙ† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡Ø§ Ù…ØªØ¬Ø§ÙˆØ±Ø©ØŒ ÙØ¥Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±Ù‚ ØªØ®Ù„Ù‚ Ø£ÙŠØ¶Ù‹Ø§ Ø®Ø±ÙŠØ·Ø© Ù…Ø¤Ø´Ø±Ø§Øª ØªØ­Øª Ø§Ù„ØºØ·Ø§Ø¡.

### Ø§Ù„ØªÙ‚Ø³ÙŠÙ…

ØªÙ‚ÙˆÙ… ÙˆØ¸ÙŠÙØ© [`~Dataset.train_test_split`] Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø³ÙŠÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ø§Ù„ÙØ¹Ù„. ÙŠØ³Ù…Ø­ Ù„Ùƒ Ø°Ù„Ùƒ Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„Ù‚ Ù„Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ ØªÙ‚Ø³ÙŠÙ…. ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© `test_size` Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø³ÙŠÙ… Ø§Ø®ØªØ¨Ø§Ø± ÙŠÙƒÙˆÙ† 10% Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:

```py
>>> dataset.train_test_split(test_size=0.1)
{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),
'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}
>>> 0.1 * len(dataset)
366.8
```

ÙŠØªÙ… Ø®Ù„Ø· Ø§Ù„ØªÙ‚Ø³ÙŠÙ…Ø§Øª Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹ÙŠÙŠÙ† `shuffle=False` Ù„Ù…Ù†Ø¹ Ø§Ù„Ø®Ù„Ø·.

### Ø§Ù„ØªØ¬Ø²Ø¦Ø©

ÙŠØ¯Ø¹Ù… ğŸ¤— Datasets Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ù„ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ù…Ø­Ø¯Ø¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ø§Ù„Ù‚Ø·Ø¹. Ø­Ø¯Ø¯ Ù…Ø¹Ù„Ù…Ø© `num_shards` ÙÙŠ [`~Dataset.shard`] Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„ÙŠÙ‡Ø§. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªÙˆÙÙŠØ± Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„Ù…Ø© `index`.

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [imdb](https://huggingface.co/datasets/imdb) Ø¹Ù„Ù‰ 25000 Ù…Ø«Ø§Ù„:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("imdb", split="train")
>>> print(dataset)
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
```

Ø¨Ø¹Ø¯ ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ø¨Ø¹ Ù‚Ø·Ø¹ØŒ Ù„Ù† ØªØ­ØªÙˆÙŠ Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø³ÙˆÙ‰ Ø¹Ù„Ù‰ 6250 Ù…Ø«Ø§Ù„Ù‹Ø§:

```py
>>> dataset.shard(num_shards=4, index=0)
Dataset({
    features: ['text', 'label'],
    num_rows: 6250
})
>>> print(25000/4)
6250.0
```

## Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ù…ÙŠØ© ÙˆØ§Ù„Ø¥Ø²Ø§Ù„Ø© ÙˆØ§Ù„ØµØ¨ ÙˆØ§Ù„ØªØ¨Ø³ÙŠØ·

ØªØ³Ù…Ø­ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…ÙÙŠØ¯Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ùˆ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§ØŒ ÙˆØªØºÙŠÙŠØ± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ ÙˆØªØ¨Ø³ÙŠØ· Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¶Ù…Ù†Ø©.

### Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ù…ÙŠØ©

Ø§Ø³ØªØ®Ø¯Ù… [`~Dataset.rename_column`] Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ³Ù…ÙŠØ© Ø¹Ù…ÙˆØ¯ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. ÙŠØªÙ… Ù†Ù‚Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙØ¹Ù„ÙŠÙ‹Ø§ ØªØ­Øª Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ø¬Ø±Ø¯ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙÙŠ Ù…ÙƒØ§Ù†Ù‡.

Ù‚Ù… Ø¨ØªØ²ÙˆÙŠØ¯ [`~Dataset.rename_column`] Ø¨Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠØŒ ÙˆØ§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯:

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.rename_column("sentence1", "sentenceA")
>>> dataset = dataset.rename_column("sentence2", "sentenceB")
>>> dataset
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
```

### Ø¥Ø²Ø§Ù„Ø©

Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ Ø£Ùˆ Ø£ÙƒØ«Ø±ØŒ Ù‚Ù… Ø¨ØªÙˆÙÙŠØ± Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ Ø¥Ù„Ù‰ ÙˆØ¸ÙŠÙØ© [`~Dataset.remove_columns`]. Ù‚Ù… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙˆÙÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:

```py
>>> dataset = dataset.remove_columns("label")
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.remove_columns(["sentence1", "sentence2"])
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙƒØ³ Ù…Ù† Ø°Ù„ÙƒØŒ ÙŠÙ‚ÙˆÙ… [`~Dataset.select_columns`] Ø¨ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…ÙˆØ¯ Ø£Ùˆ Ø£ÙƒØ«Ø± Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ù‚ÙŠ. ØªØ£Ø®Ø° Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¥Ù…Ø§ Ø¹Ù…ÙˆØ¯Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ø£Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns('idx')
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

### Cast

ØªÙ‚ÙˆÙ… ÙˆØ¸ÙŠÙØ© [`~Dataset.cast`] Ø¨ØªØ­ÙˆÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ù„Ø¹Ù…ÙˆØ¯ Ø£Ùˆ Ø£ÙƒØ«Ø±. ØªÙ‚Ø¨Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© [`Features`] Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙƒØ­Ø¬ØªÙ‡Ø§. ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ ÙƒÙŠÙÙŠØ© ØªØºÙŠÙŠØ± Ù…ÙŠØ²Ø§Øª [`ClassLabel`] Ùˆ [`Value`]:

```py
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=["negative", "positive"])
>>> new_features["idx"] = Value("int64")
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

> ØªÙ„Ù…ÙŠØ­:

ÙŠØ¹Ù…Ù„ Ø§Ù„ØµØ¨ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…ØªÙˆØ§ÙÙ‚ÙŠÙ†. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØµØ¨ Ø¹Ù…ÙˆØ¯ Ø¨Ù†ÙˆØ¹ Ø§Ù„Ù…ÙŠØ²Ø© `Value("int32")` Ø¥Ù„Ù‰ `Value("bool")` Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ ÙŠØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£Ø­Ø§Ø¯ ÙˆØµÙØ§Ø±.

Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© [`~Dataset.cast_column`] Ù„ØªØºÙŠÙŠØ± Ù†ÙˆØ¹ Ù…ÙŠØ²Ø© Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙˆÙ†ÙˆØ¹ Ù…ÙŠØ²ØªÙ‡ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒØ­Ø¬Ø¬:

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

### Ø§Ù„ØªØ³Ø·ÙŠØ­

ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø­ÙŠØ§Ù†ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù‡ÙŠÙƒÙ„Ù‹Ø§ Ù…ØªØ¯Ø§Ø®Ù„Ø§Ù‹ Ù…Ù† Ø¹Ø¯Ø© Ø£Ù†ÙˆØ§Ø¹. Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø£Ø¯Ù†Ø§Ù‡ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQuAD:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("squad", split="train")
>>> dataset.features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
'context': Value(dtype='string', id=None),
'id': Value(dtype='string', id=None),
'question': Value(dtype='string', id=None),
'title': Value(dtype='string', id=None)}
```

ÙŠØ­ØªÙˆÙŠ Ø­Ù‚Ù„ `answers` Ø¹Ù„Ù‰ Ø­Ù‚Ù„ÙŠÙ† ÙØ±Ø¹ÙŠÙŠÙ†: `text` Ùˆ`answer_start`. Ø§Ø³ØªØ®Ø¯Ù… ÙˆØ¸ÙŠÙØ© [`~Dataset.flatten`] Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø¥Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© Ø®Ø§ØµØ© Ø¨Ù‡Ø§:

```py
>>> flat_dataset = dataset.flatten()
>>> flat_dataset
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
    num_rows: 87599
})
```

Ù„Ø§Ø­Ø¸ ÙƒÙŠÙ Ø£ØµØ¨Ø­Øª Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø¢Ù† Ø£Ø¹Ù…Ø¯Ø© Ù…Ø³ØªÙ‚Ù„Ø©: `answers.text` Ùˆ`answers.answer_start`.

## Map

ØªØ£ØªÙŠ Ø¨Ø¹Ø¶ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ğŸ¤— Datasets Ø§Ù„Ø£ÙƒØ«Ø± Ù‚ÙˆØ© Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`~Dataset.map`]. Ø§Ù„ØºØ±Ø¶ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† [`~Dataset.map`] Ù‡Ùˆ ØªØ³Ø±ÙŠØ¹ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. ÙÙ‡Ùˆ ÙŠØ³Ù…Ø­ Ù„Ùƒ Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ Ø£Ùˆ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª. ÙŠÙ…ÙƒÙ† Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø­ØªÙ‰ Ø¥Ù†Ø´Ø§Ø¡ ØµÙÙˆÙ ÙˆØ£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©.

ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø£Ø¶Ù Ø¨Ø§Ø¯Ø¦Ø© "My sentence: " Ø¥Ù„Ù‰ ÙƒÙ„ Ù‚ÙŠÙ…Ø© `sentence1` ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© ØªØ¶ÙŠÙ "My sentence: " Ø¥Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© ÙƒÙ„ Ø¬Ù…Ù„Ø©. ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¨Ù„ Ø§Ù„Ø¯Ø§Ù„Ø© ÙƒØ§Ø¦Ù†Ù‹Ø§ Ù…Ù† Ø§Ù„Ù†ÙˆØ¹ dict ÙˆØªÙØ®Ø±Ø¬ ÙƒØ§Ø¦Ù†Ù‹Ø§ Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹:

```py
>>> def add_prefix(example):
...     example["sentence1"] = 'My sentence: ' + example["sentence1"]
...     return example
```

Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ù… [`~Dataset.map`] Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© `add_prefix` Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:

```py
>>> updated_dataset = small_dataset.map(add_prefix)
>>> updated_dataset["sentence1"][:5]
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
```

Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø¢Ø®Ø±ØŒ ÙˆÙ„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø©ØŒ Ø³ÙˆÙ ØªØ²ÙŠÙ„ Ø¹Ù…ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Dataset.map`]. Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚ÙˆÙ… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ØŒ ÙØ¥Ù†Ù‡ Ù„Ø§ ÙŠØªÙ… Ø¥Ø²Ø§Ù„ØªÙ‡ Ø¥Ù„Ø§ Ø¨Ø¹Ø¯ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø«Ø§Ù„ Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©. ÙŠØ³Ù…Ø­ Ù‡Ø°Ø§ Ù„Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù‚Ø¨Ù„ Ø¥Ø²Ø§Ù„ØªÙ‡Ø§.

Ø­Ø¯Ø¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø¥Ø²Ø§Ù„ØªÙ‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„Ù…Ø© `remove_columns` ÙÙŠ [`~Dataset.map`]:

```py
>>> updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])
>>> updated_dataset.column_names
['sentence2', 'label', 'idx', 'new_sentence']
```

<Tip>

ÙŠØ­ØªÙˆÙŠ ğŸ¤— Datasets Ø£ÙŠØ¶Ù‹Ø§ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© [`~Dataset.remove_columns`] ÙˆØ§Ù„ØªÙŠ ØªÙƒÙˆÙ† Ø£Ø³Ø±Ø¹ Ù„Ø£Ù†Ù‡Ø§ Ù„Ø§ ØªÙ‚ÙˆÙ… Ø¨Ù†Ø³Ø® Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©.

</Tip>

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Dataset.map`] Ù…Ø¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ† `with_indices=True`. ÙŠØ¶ÙŠÙ Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¥Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© ÙƒÙ„ Ø¬Ù…Ù„Ø©:

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
>>> updated_dataset["sentence2"][:5]
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
"2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
'3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
'4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
```

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ¹Ø¯Ø¯Ø©

ØªØ³Ø±Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©. Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ù…Ø¹Ù„Ù…Ø© `num_proc` ÙÙŠ [`~Dataset.map`] Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§:

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True, num_proc=4)
```

ÙŠØ¹Ù…Ù„ [`~Dataset.map`] Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø¹ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¥Ø°Ø§ Ù‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ† `with_rank=True`. Ù‡Ø°Ø§ Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù…Ø¹Ù„Ù…Ø© `with_indices`. ÙŠØªÙ… ÙˆØ¶Ø¹ Ù…Ø¹Ù„Ù…Ø© `with_rank` ÙÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø¹Ø¯ Ù…Ø¹Ù„Ù…Ø© `index` Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„.

```py
>>> import torch
>>> from multiprocess import set_start_method
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> from datasets import load_dataset
>>>
>>> # Get an example dataset
>>> dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")
>>>
>>> # Get an example model and its tokenizer
>>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat").eval()
>>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat")
>>>
>>> def gpu_computation(batch, rank):
...     # Move the model on the right GPU if it's not there already
...     device = f"cuda:{(rank or 0) % torch.cuda.device_count()}"
...     model.to(device)
...
...     # Your big GPU call goes here, for example:
...     chats = [[
...         {"role": "system", "content": "You are a helpful assistant."},
...         {"role": "user", "content": prompt}
...     ] for prompt in batch["prompt"]]
...     texts = [tokenizer.apply_chat_template(
...         chat,
...         tokenize=False,
...         add_generation_prompt=True
...     ) for chat in chats]
...     model_inputs = tokenizer(texts, padding=True, return_tensors="pt").to(device)
...     with torch.no_grad():
...         outputs = model.generate(**model_inputs, max_new_tokens=512)
...     batch["output"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)
...     return batch
>>>
>>> if __name__ == "__main__":
...     set_start_method("spawn")
...     updated_dataset = dataset.map(
...         gpu_computation,
...         batched=True,
...         batch_size=16,
...         with_rank=True,
...         num_proc=torch.cuda.device_count(),  # one process per GPU
...     )
```

ØªØªÙ…Ø«Ù„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ Ù…ÙˆØ§Ø²Ø§Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¹Ø¨Ø± Ø¹Ø¯Ø© ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³ÙˆÙ…ÙŠØ© (GPU). ÙŠØªØ·Ù„Ø¨ Ø°Ù„Ùƒ ØªØ¹ÙŠÙŠÙ† `multiprocess.set_start_method("spawn")`. Ø¥Ø°Ø§ Ù„Ù… ØªÙ‚Ù… Ø¨Ø°Ù„ÙƒØŒ ÙØ³ØªØªÙ„Ù‚Ù‰ Ø®Ø·Ø£ CUDA Ø§Ù„ØªØ§Ù„ÙŠ:

```bash
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.
```

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª

ØªØ¯Ø¹Ù… Ø¯Ø§Ù„Ø© [`~Dataset.map`] Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø¯ÙØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True`. Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ 1000ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø¶Ø¨Ø·Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„Ù…Ø© `batch_size`. ØªÙ…ÙƒÙ† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹ÙŠØ© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø«Ù„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£Ù‚ØµØ± ÙˆØ²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

#### ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø©

Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§ØŒ ÙÙ‚Ø¯ ØªØ±ØºØ¨ ÙÙŠ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø¹Ø¯Ø© Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ±. Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø©:

1. ØªÙ‚Ø³ÙŠÙ… Ø­Ù‚Ù„ `sentence1` Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù† 50 Ø­Ø±Ù.
2. Ù‚Ù… Ø¨ØªÙƒØ¯ÙŠØ³ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø¹Ù‹Ø§ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.

```py
>>> def chunk_examples(examples):
...     chunks = []
...     for sentence in examples["sentence1"]:
...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
...     return {"chunks": chunks}
```

Ù‚Ù… Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Dataset.map`]:

```py
>>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
>>> chunked_dataset[:10]
{'chunks': ['Amrozi accused his brother , whom he called " the ',
            'witness " , of deliberately distorting his evidenc',
            'e .',
            "Yucaipa owned Dominick 's before selling the chain",
            ' to Safeway in 1998 for $ 2.5 billion .',
            'They had published an advertisement on the Interne',
            't on June 10 , offering the cargo for sale , he ad',
            'ded .',
            'Around 0335 GMT , Tab shares were up 19 cents , or',
            ' 4.4 % , at A $ 4.56 , having earlier set a record']}
```

Ù„Ø§Ø­Ø¸ ÙƒÙŠÙ ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø¢Ù† Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£Ù‚ØµØ±ØŒ ÙˆÙ‡Ù†Ø§Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙÙˆÙ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> chunked_dataset
Dataset({
    features: ['chunks'],
    num_rows: 10470
})
```

#### Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

ÙŠÙ…ÙƒÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`~Dataset.map`] Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø±Ù…ÙˆØ² Ù…Ù…ÙŠØ²Ø© Ù…Ø­Ø¬ÙˆØ¨Ø© ÙÙŠ Ø¬Ù…Ù„Ø©.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ [RoBERTA](https://huggingface.co/roberta-base) ÙÙŠ [FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline) Ù…Ù† ğŸ¤— Transformers:

```py
>>> from random import randint
>>> from transformers import pipeline

>>> fillmask = pipeline("fill-mask", model="roberta-base")
>>> mask_token = fillmask.tokenizer.mask_token
>>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)
```

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù„Ø§Ø®ØªÙŠØ§Ø± ÙƒÙ„Ù…Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø¥Ø®ÙØ§Ø¦Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¬Ù…Ù„Ø©. ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ£ÙØ¶Ù„ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ÙŠÙ† ØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ù…Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© RoBERTA.

```py
>>> def augment_data(examples):
...     outputs = []
...     for sentence in examples["sentence1"]:
...         words = sentence.split(' ')
...         K = randint(1, len(words)-1)
...         masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
...         predictions = fillmask(masked_sentence)
...         augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
...         outputs += [sentence] + augmented_sequences
...
...     return {"data": outputs}
```

Ø§Ø³ØªØ®Ø¯Ù… [`~Dataset.map`] Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:

```py
>>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
>>> augmented_dataset[:9]["data"]
['Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'Amrozi accused his brother, whom he called " the witness ", of deliberately withholding his evidence.',
'Amrozi accused his brother, whom he called " the witness ", of deliberately suppressing his evidence.',
'Amrozi accused his brother, whom he called " the witness ", of deliberately destroying his evidence.',
"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',
"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.",
'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'
]
```

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø£ØµÙ„ÙŠØ©ØŒ Ù‚Ø§Ù…Øª RoBERTA Ø¨Ø²ÙŠØ§Ø¯Ø© ÙƒÙ„Ù…Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø«Ù„Ø§Ø«Ø© Ø¨Ø¯Ø§Ø¦Ù„. ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© "distorting" Ø¨ÙƒÙ„Ù…Ø§Øª "withholding" Ùˆ"suppressing" Ùˆ"destroying".

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ø§Ù†Ù‚Ø³Ø§Ù…Ø§Øª

ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù†Ù‚Ø³Ø§Ù…Ø§Øª ÙŠÙ…ÙƒÙ† Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DatasetDict.map`]. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¬Ø±Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªÙ…ÙŠÙŠØ² Ù„Ù„Ø¬Ø²Ø¡ `sentence1` ÙÙŠ Ø§Ù„Ø§Ù†Ù‚Ø³Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±ÙŠ Ø¹Ù† Ø·Ø±ÙŠÙ‚:

```py
>>> from datasets import load_dataset

# ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ù†Ù‚Ø³Ø§Ù…Ø§Øª
>>> dataset = load_dataset('glue', 'mrpc')
>>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)
>>> encoded_dataset["train"][0]
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
'label': 1,
'idx': 0,
'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

### Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ²Ø¹

Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Dataset.map`] ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙˆØ²Ø¹ØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… [torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier). ÙŠØ¶Ù…Ù† Ù‡Ø°Ø§ Ù‚ÙŠØ§Ù… Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø£Ø¯Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ®Ø·ÙŠØ·ØŒ ÙÙŠ Ø­ÙŠÙ† ØªÙ‚ÙˆÙ… Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ØªØ¬Ù†Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…ÙƒØ±Ø±.

ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… `torch.distributed.barrier` Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª:

```py
>>> from datasets import Dataset
>>> import torch.distributed

>>> dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

>>> if training_args.local_rank > 0:
...     print("Waiting for main process to perform the mapping")
...     torch.distributed.barrier()

>>> dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

>>> if training_args.local_rank == 0:
...     print("Loading results from main process")
...     torch Mieczyslaw
```

## Ø¯Ù…Ø¬

ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†ÙØµÙ„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ´ØªØ±Ùƒ ÙÙŠ Ù†ÙØ³ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©. Ø¯Ù…Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`concatenate_datasets`]:

```py
>>> from datasets import concatenate_datasets, load_dataset

>>> bookcorpus = load_dataset("bookcorpus", split="train")
>>> wiki = load_dataset("wikipedia", "20220301.en", split="train")
>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"]) # Ø§Ø­ØªÙØ¸ Ø¨Ø¹Ù…ÙˆØ¯ "Ø§Ù„Ù†Øµ" ÙÙ‚Ø·

>>> assert bookcorpus.features.type == wiki.features.type
>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø¯Ù…Ø¬ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ† Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙÙ‚ÙŠÙ‹Ø§ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `axis=1` Ø·Ø§Ù„Ù…Ø§ Ø£Ù† Ù„Ø¯Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†ÙØ³ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ:

```py
>>> from datasets import Dataset
>>> bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
>>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```
### Interleave
ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø®Ù„Ø· Ø¹Ø¯Ø© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ù‹Ø§ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø£Ø®Ø° Ø£Ù…Ø«Ù„Ø© Ù…ØªÙ†Ø§ÙˆØ¨Ø© Ù…Ù† ÙƒÙ„ Ù…Ù†Ù‡Ø§ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©. ÙŠÙØ¹Ø±Ù Ù‡Ø°Ø§ Ø¨Ø§Ø³Ù… *Ø§Ù„ØªØ¯Ø§Ø®Ù„*ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙ…ÙƒÙŠÙ†Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¯Ø§Ù„Ø© [`interleave_datasets`]. ÙŠØ¹Ù…Ù„ ÙƒÙ„ Ù…Ù† [`interleave_datasets`] Ùˆ [`concatenate_datasets`] Ù…Ø¹ ÙƒØ§Ø¦Ù†Ø§Øª [`Dataset`] Ùˆ [`IterableDataset`] Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.

Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ [Stream](./stream#interleave) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ¯Ø§Ø®Ù„ ÙƒØ§Ø¦Ù†Ø§Øª [`IterableDataset`].

ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ ÙƒÙŠÙÙŠØ© ØªØ¯Ø§Ø®Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø­ØªÙ‰ ØªÙ†ÙØ¯ Ø¥Ø­Ø¯Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª.

```py
>>> from datasets import Dataset, interleave_datasets
>>> seed = 42
>>> probabilities = [0.3, 0.5, 0.2]
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
>>> dataset["a"]
[10, 11, 20, 12, 0, 21, 13]
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­Ø¯ÙŠØ¯ `stopping_strategy`. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©ØŒ `first_exhausted`ØŒ Ù‡ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ØŒ Ø£ÙŠ ÙŠØªÙˆÙ‚Ù Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø¬Ø±Ø¯ Ù†ÙØ§Ø¯ Ø¥Ø­Ø¯Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª.

ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ `stopping_strategy=all_exhausted` Ù„ØªÙ†ÙÙŠØ° Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ ÙŠØªÙˆÙ‚Ù Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„ Ø¹ÙŠÙ†Ø© ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„. ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŒ ÙŠØ¹Ù†ÙŠ Ø°Ù„Ùƒ Ø£Ù†Ù‡ Ø¥Ø°Ø§ ØªÙ… Ø§Ø³ØªÙ†ÙØ§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙØ³ÙŠØªÙ… Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡ Ø­ØªÙ‰ ÙŠØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø¹ÙŠØ§Ø± Ø§Ù„ØªÙˆÙ‚Ù.

Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©ØŒ ÙØ³ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¹Ù„Ù‰ `max_length_datasets * nb_dataset samples`.

```py
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

## Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
ØªØºÙŠØ± Ø¯Ø§Ù„Ø© [`~Dataset.set_format`] ØªÙ†Ø³ÙŠÙ‚ Ø¹Ù…ÙˆØ¯ Ù„ÙŠÙƒÙˆÙ† Ù…ØªÙˆØ§ÙÙ‚Ù‹Ø§ Ù…Ø¹ Ø¨Ø¹Ø¶ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©. Ø­Ø¯Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯Ù‡ ÙÙŠ Ù…Ø¹Ù„Ù…Ø© `type` ÙˆØ§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªÙ†Ø³ÙŠÙ‚Ù‡Ø§. ÙŠØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„.

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ tensers PyTorch Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `type="torch"`:

```py
>>> import torch
>>> dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

ØªØºÙŠØ± Ø¯Ø§Ù„Ø© [`~Dataset.with_format`] Ø£ÙŠØ¶Ù‹Ø§ ØªÙ†Ø³ÙŠÙ‚ Ø¹Ù…ÙˆØ¯ØŒ Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø£Ù†Ù‡Ø§ ØªØ¹ÙŠØ¯ ÙƒØ§Ø¦Ù† [`Dataset`] Ø¬Ø¯ÙŠØ¯Ù‹Ø§:

```py
>>> dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

<Tip>
ğŸ¤— ØªÙˆÙØ± Datasets Ø£ÙŠØ¶Ù‹Ø§ Ø¯Ø¹Ù…Ù‹Ø§ Ù„ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ NumPy ÙˆPandas ÙˆJAX. Ø±Ø§Ø¬Ø¹ Ø¯Ù„ÙŠÙ„ [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª TensorFlow Ø¨ÙƒÙØ§Ø¡Ø©.
</Tip>

Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚Ù‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØŒ ÙØ§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© [`~Dataset.reset_format`]:

```py
>>> dataset.format
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
>>> dataset.reset_format()
>>> dataset.format
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
ØªØ·Ø¨Ù‚ Ø¯Ø§Ù„Ø© [`~Dataset.set_transform`] ØªØ­ÙˆÙŠÙ„ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø®ØµØµ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„. ØªØ³ØªØ¨Ø¯Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø£ÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø­Ø¯Ø¯ Ø³Ø§Ø¨Ù‚Ù‹Ø§. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„. ÙŠØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø©:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> def encode(batch):
...     return tokenizer(batch["sentence1"], batch["sentence2"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
>>> dataset.set_transform(encode)
>>> dataset.format
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`~Dataset.set_transform`] Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ØªÙ†Ø³ÙŠÙ‚Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨ÙˆØ§Ø³Ø·Ø© [`Features`]. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠØ³ØªØ®Ø¯Ù… Ù…ÙŠØ²Ø© [`Audio`] [`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/) - ÙˆÙ‡ÙŠ Ù…ÙƒØªØ¨Ø© Ø³Ø±ÙŠØ¹Ø© ÙˆØ¨Ø³ÙŠØ·Ø© Ù„ØªØ«Ø¨ÙŠØª - ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ù„Ø§ ØªÙˆÙØ± Ø¯Ø¹Ù…Ù‹Ø§ Ù„ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„ØµÙˆØª Ø§Ù„Ø£Ù‚Ù„ Ø´ÙŠÙˆØ¹Ù‹Ø§. Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~Dataset.set_transform`] Ù„ØªØ·Ø¨ÙŠÙ‚ ØªØ­ÙˆÙŠÙ„ ØªØ±Ù…ÙŠØ² Ù…Ø®ØµØµ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ù‚Ù„. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù…ÙƒØªØ¨Ø© ØªØ±ÙŠØ¯Ù‡Ø§ Ù„ØªØ±Ù…ÙŠØ² Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØª.

ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¯Ù†Ø§Ù‡ Ø­Ø²Ù…Ø© [`pydub`](http://pydub.com/) Ù„ÙØªØ­ ØªÙ†Ø³ÙŠÙ‚ ØµÙˆØªÙŠ Ù„Ø§ ÙŠØ¯Ø¹Ù…Ù‡ `soundfile`:

```py
>>> import numpy as np
>>> from pydub import AudioSegment

>>> audio_dataset_amr = Dataset.from_dict({"audio": ["audio_samples/audio.amr"]})

>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):
...     def pydub_decode_file(audio_path):
...         sound = AudioSegment.from_file(audio_path)
...         if sound.frame_rate != sampling_rate:
...             sound = sound.set_frame_rate(sampling_rate)
...         channel_sounds = sound.split_to_mono()
...         samples = [s.get_array_of_samples() for s in channel_sounds]
...         fp_arr = np.array(samples).T.astype(np.float32)
...         fp_arr /= np.iinfo(samples[0].typecode).max
...         return fp_arr
...
...     batch["audio"] = [pydub_decode_file(audio_path) for audio_path in batch["audio"]]
...     return batch

>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)
```

## Ø§Ù„Ø­ÙØ¸
Ø¨Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡Ø§ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ù…Ø¹ [`~Dataset.save_to_disk`].

Ø§Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø­ÙØ¸Ù‡ ÙÙŠÙ‡:

```py
>>> encoded_dataset.save_to_disk("path/of/my/dataset/directory")
```

Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© [`load_from_disk`] Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

```py
>>> from datasets import load_from_disk
>>> reloaded_dataset = load_from_disk("path/of/my/dataset/directory")
```

<Tip>
Ù‡Ù„ ØªØ±ÙŠØ¯ Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ Ù…ÙˆÙØ± ØªØ®Ø²ÙŠÙ† Ø³Ø­Ø§Ø¨ÙŠØŸ Ø§Ù‚Ø±Ø£ Ø¯Ù„ÙŠÙ„Ù†Ø§ [Cloud Storage](./filesystems) Ù„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ AWS Ø£Ùˆ Google Cloud Storage.
</Tip>

## Ø§Ù„ØªØµØ¯ÙŠØ±
ÙŠØ¯Ø¹Ù… ğŸ¤— Datasets Ø§Ù„ØªØµØ¯ÙŠØ± Ø£ÙŠØ¶Ù‹Ø§ØŒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø£Ø®Ø±Ù‰. ÙŠÙØ¸Ù‡Ø± Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ ÙˆØ§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„ÙŠÙ‡Ø§:

| Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù | Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØµØ¯ÙŠØ± |
|-------------------------|----------------------------------------------------------------|
| CSV | [`Dataset.to_csv`] |
| JSON | [`Dataset.to_json`] |
| Parquet | [`Dataset.to_parquet`] |
| SQL | [`Dataset.to_sql`] |
| ÙƒØ§Ø¦Ù† Python ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© | [`Dataset.to_pandas`] Ø£Ùˆ [`Dataset.to_dict`] |

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨ØªØµØ¯ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ Ù…Ù„Ù CSV Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø­Ùˆ Ø§Ù„ØªØ§Ù„ÙŠ:

```py
>>> encoded_dataset.to_csv("path/of/my/dataset.csv")
```