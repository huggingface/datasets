# ุงููุนุงูุฌุฉ

ูููุฑ ๐ค Datasets ุงูุนุฏูุฏ ูู ุงูุฃุฏูุงุช ูุชุนุฏูู ุจููุฉ ููุญุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช. ูุชุนุฏ ูุฐู ุงูุฃุฏูุงุช ูููุฉ ูุชูุธูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฅูุดุงุก ุฃุนูุฏุฉ ุฅุถุงููุฉุ ูุงูุชุญููู ุจูู ุงูููุฒุงุช ูุงูุชูุณููุงุชุ ูุงููุฒูุฏ.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

- ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงูุตููู ูุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช.
- ุฅุนุงุฏุฉ ุชุณููุฉ ุงูุฃุนูุฏุฉ ูุฅุฒุงูุชูุงุ ูุนูููุงุช ุงูุนููุฏ ุงูุดุงุฆุนุฉ ุงูุฃุฎุฑู.
- ุชุทุจูู ุฏุงูุงุช ุงููุนุงูุฌุฉ ุนูู ูู ูุซุงู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช.
- ุฏูุฌ ูุฌููุนุงุช ุงูุจูุงูุงุช.
- ุชุทุจูู ุชุญููู ุชูุณูู ูุฎุตุต.
- ุญูุธ ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููุนุงูุฌุฉ ูุชุตุฏูุฑูุง.

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู ุงููุญุฏุฏุฉ ุญูู ูุนุงูุฌุฉ ุทุฑุงุฆู ุฃุฎุฑู ููุฌููุนุฉ ุงูุจูุงูุงุชุ ุฑุงุฌุน ุฏููู ูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุงูุตูุชุ ุฃู ุฏููู ูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุงูุตูุฑุ ุฃู ุฏููู ูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช NLP.

ุชุณุชุฎุฏู ุงูุฃูุซูุฉ ูู ูุฐุง ุงูุฏููู ูุฌููุนุฉ ุจูุงูุงุช MRPCุ ูููู ููููู ุชุญููู ุฃู ูุฌููุนุฉ ุจูุงูุงุช ูู ุงุฎุชูุงุฑู ููุชุงุจุนุฉ ุฐูู!

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("glue", "mrpc", split="train")
```

> ุชูุฑุฌุน ุฌููุน ุทุฑู ุงููุนุงูุฌุฉ ูู ูุฐุง ุงูุฏููู ูุงุฆู [`Dataset`] ุฌุฏูุฏ. ูุง ูุชู ุงูุชุนุฏูู ูู ุงููููุน. ูู ุญุฐุฑูุง ุจุดุฃู ุชุฌุงูุฒ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุณุงุจูุฉ ุงูุฎุงุตุฉ ุจู!

## ุงูุชุฑุชูุจ ูุงูุฎูุท ูุงูุชุญุฏูุฏ ูุงูุชูุณูู ูุงูุชุฌุฒุฆุฉ

ููุงู ุนุฏุฉ ูุธุงุฆู ูุฅุนุงุฏุฉ ุชุฑุชูุจ ุจููุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช.

ูุฐู ุงููุธุงุฆู ูููุฏุฉ ูุงุฎุชูุงุฑ ุงูุตููู ุงูุชู ุชุฑูุฏูุง ููุทุ ูุฅูุดุงุก ุชูุณููุงุช ุงูุชุฏุฑูุจ ูุงูุงุฎุชุจุงุฑุ ูุชูุณูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููุจูุฑุฉ ุฌุฏูุง ุฅูู ูุทุน ุฃุตุบุฑ.

### ุงูุชุฑุชูุจ

ุงุณุชุฎุฏู [`~Dataset.sort`] ููุฑุฒ ููู ุงูุนููุฏ ููููุง ูููููุง ุงูุนุฏุฏูุฉ. ูุฌุจ ุฃู ูููู ุงูุนููุฏ ุงูููุฏู ูุชูุงูููุง ูุน NumPy.

```py
>>> dataset["label"][:10]
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
>>> sorted_dataset = dataset.sort("label")
>>> sorted_dataset["label"][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> sorted_dataset["label"][-10:]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ุชุญุช ุงูุบุทุงุกุ ูููู ูุฐุง ุจุฅูุดุงุก ูุงุฆูุฉ ูู ุงููุคุดุฑุงุช ุงูุชู ูุชู ูุฑุฒูุง ููููุง ูููู ุงูุนููุฏ.

ูุชู ุจุนุฏ ุฐูู ุงุณุชุฎุฏุงู ุฎุฑูุทุฉ ุงููุคุดุฑุงุช ูุฐู ูููุตูู ุฅูู ุงูุตููู ุงูุตุญูุญุฉ ูู ุฌุฏูู Arrow ุงูุฃุณุงุณู.

### ุงูุฎูุท

ุชููู ูุธููุฉ [`~Dataset.shuffle`] ุจุฅุนุงุฏุฉ ุชุฑุชูุจ ููู ุงูุนููุฏ ุจุดูู ุนุดูุงุฆู. ููููู ุชุญุฏูุฏ ูุนููุฉ `generator` ูู ูุฐู ุงููุธููุฉ ูุงุณุชุฎุฏุงู `numpy.random.Generator` ูุฎุชููุฉ ุฅุฐุง ููุช ุชุฑูุฏ ูุฒูุฏูุง ูู ุงูุชุญูู ูู ุงูุฎูุงุฑุฒููุฉ ุงููุณุชุฎุฏูุฉ ูุฎูุท ูุฌููุนุฉ ุงูุจูุงูุงุช.

```py
>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)
>>> shuffled_dataset["label"][:10]
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]
```

ูุฃุฎุฐ ุงูุฎูุท ูุงุฆูุฉ ุงููุคุดุฑุงุช `[0:len(my_dataset)]` ููุฎูุทูุง ูุฅูุดุงุก ุฎุฑูุทุฉ ูุคุดุฑุงุช.

ููุน ุฐููุ ุจูุฌุฑุฏ ุฃู ุชุญุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุนูู ุฎุฑูุทุฉ ูุคุดุฑุงุชุ ูููู ุฃู ุชุตุจุญ ุงูุณุฑุนุฉ ุฃุจุทุฃ 10 ูุฑุฉ.

ููุฑุฌุน ุฐูู ุฅูู ูุฌูุฏ ุฎุทูุฉ ุฅุถุงููุฉ ููุญุตูู ุนูู ูุคุดุฑ ุงูุตู ููุฑุงุกุฉ ุจุงุณุชุฎุฏุงู ุฎุฑูุทุฉ ุงููุคุดุฑุงุชุ ูุงูุฃูู ูู ุฐููุ ุฃูู ูู ุชุนุฏ ุชูุฑุฃ ูุทุนูุง ูุชุฌุงูุฑุฉ ูู ุงูุจูุงูุงุช.

ูุงุณุชุนุงุฏุฉ ุงูุณุฑุนุฉุ ุณุชุญุชุงุฌ ุฅูู ุฅุนุงุฏุฉ ูุชุงุจุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุนูู ุงููุฑุต ูุฑุฉ ุฃุฎุฑู ุจุงุณุชุฎุฏุงู [`Dataset.flatten_indices`]ุ ูุงูุฐู ูุฒูู ุฎุฑูุทุฉ ุงููุคุดุฑุงุช.

ุฃูุ ููููู ุงูุชุจุฏูู ุฅูู [`IterableDataset`] ูุงูุงุณุชูุงุฏุฉ ูู ุงูุฎูุท ุงูุชูุฑูุจู ุงูุณุฑูุน [`IterableDataset.shuffle`]:

```py
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)
>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### ุงูุชุญุฏูุฏ ูุงูุชุตููุฉ

ููุงู ุฎูุงุฑุงู ูุชุตููุฉ ุงูุตููู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช: [`~Dataset.select`] ู [`~Dataset.filter`].

- ูุนูุฏ [`~Dataset.select`] ุงูุตููู ููููุง ููุงุฆูุฉ ุงููุคุดุฑุงุช:

```py
>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
>>> len(small_dataset)
6
```

- ูุนูุฏ [`~Dataset.filter`] ุงูุตููู ุงูุชู ุชุชุทุงุจู ูุน ุดุฑุท ูุญุฏุฏ:

```py
>>> start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
>>> len(start_with_ar)
6
>>> start_with_ar["sentence1"]
['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',
'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',
'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',
"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .",
'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'
]
```

ูููู ุฃูุถูุง ุฃู ูููู [`~Dataset.filter`] ุจุงูุชุตููุฉ ุญุณุจ ุงููุคุดุฑุงุช ุฅุฐุง ููุช ุจุชุนููู `with_indices=True`:

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> len(even_dataset)
1834
>>> len(dataset) / 2
1834.0
```

ูุง ูู ุชูู ูุงุฆูุฉ ุงููุคุดุฑุงุช ุงูุชู ุณูุชู ุงูุงุญุชูุงุธ ุจูุง ูุชุฌุงูุฑุฉุ ูุฅู ูุฐู ุงูุทุฑู ุชุฎูู ุฃูุถูุง ุฎุฑูุทุฉ ูุคุดุฑุงุช ุชุญุช ุงูุบุทุงุก.

### ุงูุชูุณูู

ุชููู ูุธููุฉ [`~Dataset.train_test_split`] ุจุฅูุดุงุก ุชูุณููุงุช ุงูุชุฏุฑูุจ ูุงูุงุฎุชุจุงุฑ ุฅุฐุง ูู ููู ูุฏู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุจุงููุนู. ูุณูุญ ูู ุฐูู ุจุชุนุฏูู ุงููุณุจ ุงููุณุจูุฉ ุฃู ุงูุนุฏุฏ ุงููุทูู ููุนููุงุช ูู ูู ุชูุณูู. ูู ุงููุซุงู ุฃุฏูุงูุ ุงุณุชุฎุฏู ูุนููุฉ `test_size` ูุฅูุดุงุก ุชูุณูู ุงุฎุชุจุงุฑ ูููู 10% ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฃุตููุฉ:

```py
>>> dataset.train_test_split(test_size=0.1)
{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),
'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}
>>> 0.1 * len(dataset)
366.8
```

ูุชู ุฎูุท ุงูุชูุณููุงุช ุจุดูู ุงูุชุฑุงุถูุ ูููู ููููู ุชุนููู `shuffle=False` ูููุน ุงูุฎูุท.

### ุงูุชุฌุฒุฆุฉ

ูุฏุนู ๐ค Datasets ุงูุชุฌุฒุฆุฉ ูุชูุณูู ูุฌููุนุฉ ุจูุงูุงุช ูุจูุฑุฉ ุฌุฏูุง ุฅูู ุนุฏุฏ ูุญุฏุฏ ูุณุจููุง ูู ุงููุทุน. ุญุฏุฏ ูุนููุฉ `num_shards` ูู [`~Dataset.shard`] ูุชุญุฏูุฏ ุนุฏุฏ ุงููุทุน ุงูุชู ุณูุชู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูููุง. ุณุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชูููุฑ ุงููุทุนุฉ ุงูุชู ุชุฑูุฏ ุฅุฑุฌุงุนูุง ุจุงุณุชุฎุฏุงู ูุนููุฉ `index`.

ุนูู ุณุจูู ุงููุซุงูุ ุชุญุชูู ูุฌููุนุฉ ุจูุงูุงุช [imdb](https://huggingface.co/datasets/imdb) ุนูู 25000 ูุซุงู:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("imdb", split="train")
>>> print(dataset)
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
```

ุจุนุฏ ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ุฃุฑุจุน ูุทุนุ ูู ุชุญุชูู ุงููุทุนุฉ ุงูุฃููู ุณูู ุนูู 6250 ูุซุงููุง:

```py
>>> dataset.shard(num_shards=4, index=0)
Dataset({
    features: ['text', 'label'],
    num_rows: 6250
})
>>> print(25000/4)
6250.0
```

## ุฅุนุงุฏุฉ ุงูุชุณููุฉ ูุงูุฅุฒุงูุฉ ูุงูุตุจ ูุงูุชุจุณูุท

ุชุณูุญ ุงููุธุงุฆู ุงูุชุงููุฉ ุจุชุนุฏูู ุฃุนูุฏุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช. ูุฐู ุงููุธุงุฆู ูููุฏุฉ ูุฅุนุงุฏุฉ ุชุณููุฉ ุงูุฃุนูุฏุฉ ุฃู ุฅุฒุงูุชูุงุ ูุชุบููุฑ ุงูุฃุนูุฏุฉ ุฅูู ูุฌููุนุฉ ุฌุฏูุฏุฉ ูู ุงูููุฒุงุชุ ูุชุจุณูุท ููุงูู ุงูุฃุนูุฏุฉ ุงููุถููุฉ.

### ุฅุนุงุฏุฉ ุงูุชุณููุฉ

ุงุณุชุฎุฏู [`~Dataset.rename_column`] ุนูุฏ ุงูุญุงุฌุฉ ุฅูู ุฅุนุงุฏุฉ ุชุณููุฉ ุนููุฏ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู. ูุชู ููู ุงูููุฒุงุช ุงููุฑุชุจุทุฉ ุจุงูุนููุฏ ุงูุฃุตูู ูุนูููุง ุชุญุช ุงุณู ุงูุนููุฏ ุงูุฌุฏูุฏุ ุจุฏูุงู ูู ูุฌุฑุฏ ุงุณุชุจุฏุงู ุงูุนููุฏ ุงูุฃุตูู ูู ููุงูู.

ูู ุจุชุฒููุฏ [`~Dataset.rename_column`] ุจุงุณู ุงูุนููุฏ ุงูุฃุตููุ ูุงุณู ุงูุนููุฏ ุงูุฌุฏูุฏ:

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.rename_column("sentence1", "sentenceA")
>>> dataset = dataset.rename_column("sentence2", "sentenceB")
>>> dataset
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
```

### ุฅุฒุงูุฉ

ุนูุฏูุง ุชุญุชุงุฌ ุฅูู ุฅุฒุงูุฉ ุนููุฏ ุฃู ุฃูุซุฑุ ูู ุจุชูููุฑ ุงุณู ุงูุนููุฏ ุงูุฐู ุณูุชู ุฅุฒุงูุชู ุฅูู ูุธููุฉ [`~Dataset.remove_columns`]. ูู ุจุฅุฒุงูุฉ ุฃูุซุฑ ูู ุนููุฏ ูุงุญุฏ ุนู ุทุฑูู ุชูููุฑ ูุงุฆูุฉ ุจุฃุณูุงุก ุงูุฃุนูุฏุฉ:

```py
>>> dataset = dataset.remove_columns("label")
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.remove_columns(["sentence1", "sentence2"])
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

ุนูู ุงูุนูุณ ูู ุฐููุ ูููู [`~Dataset.select_columns`] ุจุชุญุฏูุฏ ุนููุฏ ุฃู ุฃูุซุฑ ููุงุญุชูุงุธ ุจู ูุฅุฒุงูุฉ ุงูุจุงูู. ุชุฃุฎุฐ ูุฐู ุงููุธููุฉ ุฅูุง ุนููุฏูุง ูุงุญุฏูุง ุฃู ูุงุฆูุฉ ุจุฃุณูุงุก ุงูุฃุนูุฏุฉ:

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns('idx')
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

### Cast

ุชููู ูุธููุฉ [`~Dataset.cast`] ุจุชุญููู ููุน ุงูููุฒุฉ ูุนููุฏ ุฃู ุฃูุซุฑ. ุชูุจู ูุฐู ุงููุธููุฉ [`Features`] ุงูุฌุฏูุฏ ุงูุฎุงุต ุจู ูุญุฌุชูุง. ููุถุญ ุงููุซุงู ุฃุฏูุงู ููููุฉ ุชุบููุฑ ููุฒุงุช [`ClassLabel`] ู [`Value`]:

```py
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=["negative", "positive"])
>>> new_features["idx"] = Value("int64")
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

> ุชูููุญ:

ูุนูู ุงูุตุจ ููุท ุฅุฐุง ูุงู ููุน ุงูููุฒุฉ ุงูุฃุตููุฉ ูููุน ุงูููุฒุฉ ุงูุฌุฏูุฏุฉ ูุชูุงูููู. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุตุจ ุนููุฏ ุจููุน ุงูููุฒุฉ `Value("int32")` ุฅูู `Value("bool")` ุฅุฐุง ูุงู ุงูุนููุฏ ุงูุฃุตูู ูุญุชูู ููุท ุนูู ุฃุญุงุฏ ูุตูุงุฑ.

ุงุณุชุฎุฏู ูุธููุฉ [`~Dataset.cast_column`] ูุชุบููุฑ ููุน ููุฒุฉ ุนููุฏ ูุงุญุฏ. ูู ุจุชูุฑูุฑ ุงุณู ุงูุนููุฏ ูููุน ููุฒุชู ุงูุฌุฏูุฏุฉ ูุญุฌุฌ:

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

### ุงูุชุณุทูุญ

ูู ุจุนุถ ุงูุฃุญูุงูุ ูููู ุฃู ูููู ุงูุนููุฏ ูููููุง ูุชุฏุงุฎูุงู ูู ุนุฏุฉ ุฃููุงุน. ุงูู ูุธุฑุฉ ุนูู ุงูุจููุฉ ุงููุชุฏุงุฎูุฉ ุฃุฏูุงู ูู ูุฌููุนุฉ ุจูุงูุงุช SQuAD:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("squad", split="train")
>>> dataset.features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
'context': Value(dtype='string', id=None),
'id': Value(dtype='string', id=None),
'question': Value(dtype='string', id=None),
'title': Value(dtype='string', id=None)}
```

ูุญุชูู ุญูู `answers` ุนูู ุญูููู ูุฑุนููู: `text` ู`answer_start`. ุงุณุชุฎุฏู ูุธููุฉ [`~Dataset.flatten`] ูุงุณุชุฎุฑุงุฌ ุงูุญููู ุงููุฑุนูุฉ ุฅูู ุฃุนูุฏุฉ ุฎุงุตุฉ ุจูุง:

```py
>>> flat_dataset = dataset.flatten()
>>> flat_dataset
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
    num_rows: 87599
})
```

ูุงุญุธ ููู ุฃุตุจุญุช ุงูุญููู ุงููุฑุนูุฉ ุงูุขู ุฃุนูุฏุฉ ูุณุชููุฉ: `answers.text` ู`answers.answer_start`.

## Map

ุชุฃุชู ุจุนุถ ุชุทุจููุงุช ๐ค Datasets ุงูุฃูุซุฑ ููุฉ ูู ุงุณุชุฎุฏุงู ุฏุงูุฉ [`~Dataset.map`]. ุงูุบุฑุถ ุงูุฃุณุงุณู ูู [`~Dataset.map`] ูู ุชุณุฑูุน ูุธุงุฆู ุงููุนุงูุฌุฉ. ููู ูุณูุญ ูู ุจุชุทุจูู ุฏุงูุฉ ูุนุงูุฌุฉ ุนูู ูู ูุซุงู ูู ูุฌููุนุฉ ุงูุจูุงูุงุชุ ุจุดูู ูุณุชูู ุฃู ูู ูุฌููุนุงุช. ูููู ููุฐู ุงูุฏุงูุฉ ุญุชู ุฅูุดุงุก ุตููู ูุฃุนูุฏุฉ ุฌุฏูุฏุฉ.

ูู ุงููุซุงู ุงูุชุงููุ ุฃุถู ุจุงุฏุฆุฉ "My sentence: " ุฅูู ูู ูููุฉ `sentence1` ูู ูุฌููุนุฉ ุงูุจูุงูุงุช.

ุงุจุฏุฃ ุจุฅูุดุงุก ุฏุงูุฉ ุชุถูู "My sentence: " ุฅูู ุจุฏุงูุฉ ูู ุฌููุฉ. ูุฌุจ ุฃู ุชูุจู ุงูุฏุงูุฉ ูุงุฆููุง ูู ุงูููุน dict ูุชูุฎุฑุฌ ูุงุฆููุง ูู ููุณ ุงูููุน:

```py
>>> def add_prefix(example):
...     example["sentence1"] = 'My sentence: ' + example["sentence1"]
...     return example
```

ุงูุขู ุงุณุชุฎุฏู [`~Dataset.map`] ูุชุทุจูู ุฏุงูุฉ `add_prefix` ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง:

```py
>>> updated_dataset = small_dataset.map(add_prefix)
>>> updated_dataset["sentence1"][:5]
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
```

ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ูุซุงู ุขุฎุฑุ ูููู ูุฐู ุงููุฑุฉุ ุณูู ุชุฒูู ุนููุฏูุง ุจุงุณุชุฎุฏุงู [`~Dataset.map`]. ุนูุฏูุง ุชููู ุจุฅุฒุงูุฉ ุนููุฏุ ูุฅูู ูุง ูุชู ุฅุฒุงูุชู ุฅูุง ุจุนุฏ ุชูููุฑ ุงููุซุงู ููุฏุงูุฉ ุงููุญุฏุฏุฉ. ูุณูุญ ูุฐุง ููุฏุงูุฉ ุงููุญุฏุฏุฉ ุจุงุณุชุฎุฏุงู ูุญุชูู ุงูุฃุนูุฏุฉ ูุจู ุฅุฒุงูุชูุง.

ุญุฏุฏ ุงูุนููุฏ ุงูุฐู ุชุฑูุฏ ุฅุฒุงูุชู ุจุงุณุชุฎุฏุงู ูุนููุฉ `remove_columns` ูู [`~Dataset.map`]:

```py
>>> updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])
>>> updated_dataset.column_names
['sentence2', 'label', 'idx', 'new_sentence']
```

<Tip>

ูุญุชูู ๐ค Datasets ุฃูุถูุง ุนูู ุฏุงูุฉ [`~Dataset.remove_columns`] ูุงูุชู ุชููู ุฃุณุฑุน ูุฃููุง ูุง ุชููู ุจูุณุฎ ุจูุงูุงุช ุงูุฃุนูุฏุฉ ุงููุชุจููุฉ.

</Tip>

ููููู ุฃูุถูุง ุงุณุชุฎุฏุงู [`~Dataset.map`] ูุน ุงููุคุดุฑุงุช ุฅุฐุง ููุช ุจุชุนููู `with_indices=True`. ูุถูู ุงููุซุงู ุฃุฏูุงู ุงูููุฑุณ ุฅูู ุจุฏุงูุฉ ูู ุฌููุฉ:

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
>>> updated_dataset["sentence2"][:5]
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
"2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
'3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
'4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
```

### ูุนุงูุฌุฉ ูุชุนุฏุฏุฉ

ุชุณุฑุน ุงููุนุงูุฌุฉ ุงููุชุนุฏุฏุฉ ุจุดูู ูุจูุฑ ูู ุงููุนุงูุฌุฉ ูู ุฎูุงู ููุงุฒุงุฉ ุงูุนูููุงุช ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ. ูู ุจุชุนููู ูุนููุฉ `num_proc` ูู [`~Dataset.map`] ูุชุญุฏูุฏ ุนุฏุฏ ุงูุนูููุงุช ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง:

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True, num_proc=4)
```

ูุนูู [`~Dataset.map`] ุฃูุถูุง ูุน ุชุฑุชูุจ ุงูุนูููุฉ ุฅุฐุง ููุช ุจุชุนููู `with_rank=True`. ูุฐุง ูุดุงุจู ููุนููุฉ `with_indices`. ูุชู ูุถุน ูุนููุฉ `with_rank` ูู ุงูุฏุงูุฉ ุงููุญุฏุฏุฉ ุจุนุฏ ูุนููุฉ `index` ุฅุฐุง ูุงูุช ููุฌูุฏุฉ ุจุงููุนู.

```py
>>> import torch
>>> from multiprocess import set_start_method
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> from datasets import load_dataset
>>>
>>> # Get an example dataset
>>> dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")
>>>
>>> # Get an example model and its tokenizer
>>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat").eval()
>>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat")
>>>
>>> def gpu_computation(batch, rank):
...     # Move the model on the right GPU if it's not there already
...     device = f"cuda:{(rank or 0) % torch.cuda.device_count()}"
...     model.to(device)
...
...     # Your big GPU call goes here, for example:
...     chats = [[
...         {"role": "system", "content": "You are a helpful assistant."},
...         {"role": "user", "content": prompt}
...     ] for prompt in batch["prompt"]]
...     texts = [tokenizer.apply_chat_template(
...         chat,
...         tokenize=False,
...         add_generation_prompt=True
...     ) for chat in chats]
...     model_inputs = tokenizer(texts, padding=True, return_tensors="pt").to(device)
...     with torch.no_grad():
...         outputs = model.generate(**model_inputs, max_new_tokens=512)
...     batch["output"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)
...     return batch
>>>
>>> if __name__ == "__main__":
...     set_start_method("spawn")
...     updated_dataset = dataset.map(
...         gpu_computation,
...         batched=True,
...         batch_size=16,
...         with_rank=True,
...         num_proc=torch.cuda.device_count(),  # one process per GPU
...     )
```

ุชุชูุซู ุงูุญุงูุฉ ุงูุงุณุชุฎุฏุงููุฉ ุงูุฑุฆูุณูุฉ ููุชุฑุชูุจ ูู ููุงุฒุงุฉ ุงูุญุณุงุจ ุนุจุฑ ุนุฏุฉ ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณูููุฉ (GPU). ูุชุทูุจ ุฐูู ุชุนููู `multiprocess.set_start_method("spawn")`. ุฅุฐุง ูู ุชูู ุจุฐููุ ูุณุชุชููู ุฎุทุฃ CUDA ุงูุชุงูู:

```bash
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.
```

### ูุนุงูุฌุฉ ุงูุฏูุนุงุช

ุชุฏุนู ุฏุงูุฉ [`~Dataset.map`] ุงูุนูู ูุน ุฏูุนุงุช ูู ุงูุฃูุซูุฉ. ููููู ุงูุนูู ุนูู ุงูุฏูุนุงุช ุนู ุทุฑูู ุชุนููู `batched=True`. ุญุฌู ุงูุฏูุนุฉ ุงูุงูุชุฑุงุถู ูู 1000ุ ูููู ููููู ุถุจุทู ุจุงุณุชุฎุฏุงู ูุนููุฉ `batch_size`. ุชููู ุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ ุชุทุจููุงุช ูุซูุฑุฉ ููุงูุชูุงู ูุซู ุชูุณูู ุงูุฌูู ุงูุทูููุฉ ุฅูู ุฃุฌุฒุงุก ุฃูุตุฑ ูุฒูุงุฏุฉ ุงูุจูุงูุงุช.

#### ุชูุณูู ุงูุฃูุซูุฉ ุงูุทูููุฉ

ุนูุฏูุง ุชููู ุงูุฃูุซูุฉ ุทูููุฉ ุฌุฏูุงุ ููุฏ ุชุฑุบุจ ูู ุชูุณูููุง ุฅูู ุนุฏุฉ ุฃุฌุฒุงุก ุฃุตุบุฑ. ุงุจุฏุฃ ุจุฅูุดุงุก ุฏุงูุฉ:

1. ุชูุณูู ุญูู `sentence1` ุฅูู ุฃุฌุฒุงุก ูู 50 ุญุฑู.
2. ูู ุจุชูุฏูุณ ุฌููุน ุงูุฃุฌุฒุงุก ูุนูุง ูุฅูุดุงุก ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฌุฏูุฏุฉ.

```py
>>> def chunk_examples(examples):
...     chunks = []
...     for sentence in examples["sentence1"]:
...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
...     return {"chunks": chunks}
```

ูู ุจุชุทุจูู ุงูุฏุงูุฉ ุจุงุณุชุฎุฏุงู [`~Dataset.map`]:

```py
>>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
>>> chunked_dataset[:10]
{'chunks': ['Amrozi accused his brother , whom he called " the ',
            'witness " , of deliberately distorting his evidenc',
            'e .',
            "Yucaipa owned Dominick 's before selling the chain",
            ' to Safeway in 1998 for $ 2.5 billion .',
            'They had published an advertisement on the Interne',
            't on June 10 , offering the cargo for sale , he ad',
            'ded .',
            'Around 0335 GMT , Tab shares were up 19 cents , or',
            ' 4.4 % , at A $ 4.56 , having earlier set a record']}
```

ูุงุญุธ ููู ุชู ุชูุณูู ุงูุฌูู ุงูุขู ุฅูู ุฃุฌุฒุงุก ุฃูุตุฑุ ูููุงู ุงููุฒูุฏ ูู ุงูุตููู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช.

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> chunked_dataset
Dataset({
    features: ['chunks'],
    num_rows: 10470
})
```

#### ุฒูุงุฏุฉ ุงูุจูุงูุงุช

ูููู ุฃูุถูุง ุงุณุชุฎุฏุงู ุฏุงูุฉ [`~Dataset.map`] ูุฒูุงุฏุฉ ุงูุจูุงูุงุช. ูููู ุงููุซุงู ุงูุชุงูู ุจุชูููุฏ ูููุงุช ุฅุถุงููุฉ ูุฑููุฒ ูููุฒุฉ ูุญุฌูุจุฉ ูู ุฌููุฉ.

ูู ุจุชุญููู ูุงุณุชุฎุฏุงู ูููุฐุฌ [RoBERTA](https://huggingface.co/roberta-base) ูู [FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline) ูู ๐ค Transformers:

```py
>>> from random import randint
>>> from transformers import pipeline

>>> fillmask = pipeline("fill-mask", model="roberta-base")
>>> mask_token = fillmask.tokenizer.mask_token
>>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)
```

ูู ุจุฅูุดุงุก ุฏุงูุฉ ูุงุฎุชูุงุฑ ูููุฉ ุนุดูุงุฆูุฉ ูุฅุฎูุงุฆูุง ูู ุงูุฌููุฉ. ูุฌุจ ุฃู ุชููู ุงูุฏุงูุฉ ุฃูุถูุง ุจุฅุฑุฌุงุน ุงูุฌููุฉ ุงูุฃุตููุฉ ูุฃูุถู ุงุณุชุจุฏุงููู ุชู ุชูููุฏููุง ุจูุงุณุทุฉ RoBERTA.

```py
>>> def augment_data(examples):
...     outputs = []
...     for sentence in examples["sentence1"]:
...         words = sentence.split(' ')
...         K = randint(1, len(words)-1)
...         masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
...         predictions = fillmask(masked_sentence)
...         augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
...         outputs += [sentence] + augmented_sequences
...
...     return {"data": outputs}
```

ุงุณุชุฎุฏู [`~Dataset.map`] ูุชุทุจูู ุงูุฏุงูุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง:

```py
>>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
>>> augmented_dataset[:9]["data"]
['Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'Amrozi accused his brother, whom he called " the witness ", of deliberately withholding his evidence.',
'Amrozi accused his brother, whom he called " the witness ", of deliberately suppressing his evidence.',
'Amrozi accused his brother, whom he called " the witness ", of deliberately destroying his evidence.',
"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',
"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.",
'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'
]
```

ุจุงููุณุจุฉ ููู ุฌููุฉ ุฃุตููุฉุ ูุงูุช RoBERTA ุจุฒูุงุฏุฉ ูููุฉ ุนุดูุงุฆูุฉ ุจุซูุงุซุฉ ุจุฏุงุฆู. ุชูุช ุฅุถุงูุฉ ุงููููุฉ ุงูุฃุตููุฉ "distorting" ุจูููุงุช "withholding" ู"suppressing" ู"destroying".

### ูุนุงูุฌุฉ ุนุฏุฉ ุงููุณุงูุงุช

ุชุญุชูู ุงูุนุฏูุฏ ูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุนูู ุงููุณุงูุงุช ูููู ูุนุงูุฌุชูุง ูู ููุณ ุงูููุช ุจุงุณุชุฎุฏุงู [`DatasetDict.map`]. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุฅุฌุฑุงุก ุนูููุฉ ุชูููุฒ ููุฌุฒุก `sentence1` ูู ุงูุงููุณุงู ุงูุชุฏุฑูุจู ูุงูุงุฎุชุจุงุฑู ุนู ุทุฑูู:

```py
>>> from datasets import load_dataset

# ุชุญููู ุฌููุน ุงูุงููุณุงูุงุช
>>> dataset = load_dataset('glue', 'mrpc')
>>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)
>>> encoded_dataset["train"][0]
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
'label': 1,
'idx': 0,
'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

### ุงูุงุณุชุฎุฏุงู ุงูููุฒุน

ุนูุฏ ุงุณุชุฎุฏุงู [`~Dataset.map`] ูู ุฅุนุฏุงุฏ ููุฒุนุ ูุฌุจ ุนููู ุฃูุถูุง ุงุณุชุฎุฏุงู [torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier). ูุถูู ูุฐุง ููุงู ุงูุนูููุฉ ุงูุฑุฆูุณูุฉ ุจุฃุฏุงุก ุนูููุฉ ุงูุชุฎุทูุทุ ูู ุญูู ุชููู ุงูุนูููุงุช ุงูุฃุฎุฑู ุจุชุญููู ุงููุชุงุฆุฌุ ูุจุงูุชุงูู ุชุฌูุจ ุงูุนูู ุงูููุฑุฑ.

ููุถุญ ุงููุซุงู ุงูุชุงูู ููููุฉ ุงุณุชุฎุฏุงู `torch.distributed.barrier` ููุฒุงููุฉ ุงูุนูููุงุช:

```py
>>> from datasets import Dataset
>>> import torch.distributed

>>> dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

>>> if training_args.local_rank > 0:
...     print("Waiting for main process to perform the mapping")
...     torch.distributed.barrier()

>>> dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

>>> if training_args.local_rank == 0:
...     print("Loading results from main process")
...     torch Mieczyslaw
```

## ุฏูุฌ

ูููู ุฏูุฌ ูุฌููุนุงุช ุจูุงูุงุช ูููุตูุฉ ุฅุฐุง ูุงูุช ุชุดุชุฑู ูู ููุณ ุฃููุงุน ุงูุฃุนูุฏุฉ. ุฏูุฌ ูุฌููุนุงุช ุงูุจูุงูุงุช ุจุงุณุชุฎุฏุงู [`concatenate_datasets`]:

```py
>>> from datasets import concatenate_datasets, load_dataset

>>> bookcorpus = load_dataset("bookcorpus", split="train")
>>> wiki = load_dataset("wikipedia", "20220301.en", split="train")
>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"]) # ุงุญุชูุธ ุจุนููุฏ "ุงููุต" ููุท

>>> assert bookcorpus.features.type == wiki.features.type
>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

ููููู ุฃูุถูุง ุฏูุฌ ูุฌููุนุชูู ูู ุงูุจูุงูุงุช ุฃููููุง ุนู ุทุฑูู ุชุนููู `axis=1` ุทุงููุง ุฃู ูุฏู ูุฌููุนุงุช ุงูุจูุงูุงุช ููุณ ุนุฏุฏ ุงูุตููู:

```py
>>> from datasets import Dataset
>>> bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
>>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```
### Interleave
ููููู ุฃูุถูุง ุฎูุท ุนุฏุฉ ูุฌููุนุงุช ุจูุงูุงุช ูุนูุง ุนู ุทุฑูู ุฃุฎุฐ ุฃูุซูุฉ ูุชูุงูุจุฉ ูู ูู ูููุง ูุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ. ููุนุฑู ูุฐุง ุจุงุณู *ุงูุชุฏุงุฎู*ุ ูุงูุฐู ูุชู ุชููููู ุจูุงุณุทุฉ ุฏุงูุฉ [`interleave_datasets`]. ูุนูู ูู ูู [`interleave_datasets`] ู [`concatenate_datasets`] ูุน ูุงุฆูุงุช [`Dataset`] ู [`IterableDataset`] ุงูุนุงุฏูุฉ.

ุฑุงุฌุน ุฏููู [Stream](./stream#interleave) ููุญุตูู ุนูู ูุซุงู ุญูู ููููุฉ ุชุฏุงุฎู ูุงุฆูุงุช [`IterableDataset`].

ููููู ุชุญุฏูุฏ ุงุญุชูุงูุงุช ุงููุนุงููุฉ ููู ูุฌููุนุฉ ูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฃุตููุฉ ูุชุญุฏูุฏ ููููุฉ ุชุฏุงุฎู ูุฌููุนุงุช ุงูุจูุงูุงุช.

ูู ูุฐู ุงูุญุงูุฉุ ูุชู ุจูุงุก ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฌุฏูุฏุฉ ุนู ุทุฑูู ุงูุญุตูู ุนูู ุฃูุซูุฉ ูุงุญุฏุฉ ุชูู ุงูุฃุฎุฑู ูู ูุฌููุนุฉ ุจูุงูุงุช ุนุดูุงุฆูุฉ ุญุชู ุชููุฏ ุฅุญุฏู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ุงูุนููุงุช.

```py
>>> from datasets import Dataset, interleave_datasets
>>> seed = 42
>>> probabilities = [0.3, 0.5, 0.2]
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
>>> dataset["a"]
[10, 11, 20, 12, 0, 21, 13]
```

ููููู ุฃูุถูุง ุชุญุฏูุฏ `stopping_strategy`. ุงุณุชุฑุงุชูุฌูุฉ ุงูุงูุชุฑุงุถูุฉุ `first_exhausted`ุ ูู ุงุณุชุฑุงุชูุฌูุฉ ุงูุงุณุชุจุนุงุฏุ ุฃู ูุชููู ุจูุงุก ูุฌููุนุฉ ุงูุจูุงูุงุช ุจูุฌุฑุฏ ููุงุฏ ุฅุญุฏู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ุงูุนููุงุช.

ููููู ุชุญุฏูุฏ `stopping_strategy=all_exhausted` ูุชูููุฐ ุงุณุชุฑุงุชูุฌูุฉ ุงูุฅูุฑุงุท ูู ุงููุนุงููุฉ. ูู ูุฐู ุงูุญุงูุฉุ ูุชููู ุจูุงุก ูุฌููุนุฉ ุงูุจูุงูุงุช ุจูุฌุฑุฏ ุฅุถุงูุฉ ูู ุนููุฉ ูู ูู ูุฌููุนุฉ ุจูุงูุงุช ูุฑุฉ ูุงุญุฏุฉ ุนูู ุงูุฃูู. ูู ุงูููุงุฑุณุฉ ุงูุนูููุฉุ ูุนูู ุฐูู ุฃูู ุฅุฐุง ุชู ุงุณุชููุงุฏ ูุฌููุนุฉ ุจูุงูุงุชุ ูุณูุชู ุงูุฑุฌูุน ุฅูู ุจุฏุงูุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฐู ุญุชู ูุชู ุงููุตูู ุฅูู ูุนูุงุฑ ุงูุชููู.

ูุงุญุธ ุฃูู ุฅุฐุง ูู ูุชู ุชุญุฏูุฏ ุงุญุชูุงูุงุช ุงููุนุงููุฉุ ูุณุชุญุชูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฌุฏูุฏุฉ ุนูู `max_length_datasets * nb_dataset samples`.

```py
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

## ุงูุชูุณูู
ุชุบูุฑ ุฏุงูุฉ [`~Dataset.set_format`] ุชูุณูู ุนููุฏ ููููู ูุชูุงูููุง ูุน ุจุนุถ ุชูุณููุงุช ุงูุจูุงูุงุช ุงูุดุงุฆุนุฉ. ุญุฏุฏ ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชุฑูุฏู ูู ูุนููุฉ `type` ูุงูุฃุนูุฏุฉ ุงูุชู ุชุฑูุฏ ุชูุณูููุง. ูุชู ุชุทุจูู ุงูุชูุณูู ุฃุซูุงุก ุงูุชููู.

ุนูู ุณุจูู ุงููุซุงูุ ูู ุจุฅูุดุงุก tensers PyTorch ุนู ุทุฑูู ุชุนููู `type="torch"`:

```py
>>> import torch
>>> dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

ุชุบูุฑ ุฏุงูุฉ [`~Dataset.with_format`] ุฃูุถูุง ุชูุณูู ุนููุฏุ ุจุงุณุชุซูุงุก ุฃููุง ุชุนูุฏ ูุงุฆู [`Dataset`] ุฌุฏูุฏูุง:

```py
>>> dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

<Tip>
๐ค ุชููุฑ Datasets ุฃูุถูุง ุฏุนููุง ูุชูุณููุงุช ุงูุจูุงูุงุช ุงูุดุงุฆุนุฉ ุงูุฃุฎุฑู ูุซู NumPy ูPandas ูJAX. ุฑุงุฌุน ุฏููู [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ููููุฉ ุฅูุดุงุก ูุฌููุนุฉ ุจูุงูุงุช TensorFlow ุจููุงุกุฉ.
</Tip>

ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ุฅุนุงุฏุฉ ุชุนููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ุชูุณูููุง ุงูุฃุตููุ ูุงุณุชุฎุฏู ุฏุงูุฉ [`~Dataset.reset_format`]:

```py
>>> dataset.format
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
>>> dataset.reset_format()
>>> dataset.format
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### ุชุญููู ุงูุชูุณูู
ุชุทุจู ุฏุงูุฉ [`~Dataset.set_transform`] ุชุญููู ุชูุณูู ูุฎุตุต ุฃุซูุงุก ุงูุชููู. ุชุณุชุจุฏู ูุฐู ุงูุฏุงูุฉ ุฃู ุชูุณูู ูุญุฏุฏ ุณุงุจููุง. ุนูู ุณุจูู ุงููุซุงูุ ููููู ุงุณุชุฎุฏุงู ูุฐู ุงูุฏุงูุฉ ููุชุนุฑู ุนูู ุงูุฑููุฒ ูุชุนุจุฆุฉ ุงูุฑููุฒ ุฃุซูุงุก ุงูุชููู. ูุชู ุชุทุจูู ุงูุชุนุฑู ุนูู ุงูุฑููุฒ ููุท ุนูุฏ ุงููุตูู ุฅูู ุงูุฃูุซูุฉ:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> def encode(batch):
...     return tokenizer(batch["sentence1"], batch["sentence2"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
>>> dataset.set_transform(encode)
>>> dataset.format
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

ููููู ุฃูุถูุง ุงุณุชุฎุฏุงู ุฏุงูุฉ [`~Dataset.set_transform`] ูุชุฑููุฒ ุงูุชูุณููุงุช ุบูุฑ ุงููุฏุนููุฉ ุจูุงุณุทุฉ [`Features`]. ุนูู ุณุจูู ุงููุซุงูุ ูุณุชุฎุฏู ููุฒุฉ [`Audio`] [`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/) - ููู ููุชุจุฉ ุณุฑูุนุฉ ูุจุณูุทุฉ ูุชุซุจูุช - ูููููุง ูุง ุชููุฑ ุฏุนููุง ูุชูุณููุงุช ุงูุตูุช ุงูุฃูู ุดููุนูุง. ููุง ููููู ุงุณุชุฎุฏุงู [`~Dataset.set_transform`] ูุชุทุจูู ุชุญููู ุชุฑููุฒ ูุฎุตุต ุฃุซูุงุก ุงูุชููู. ููููู ุงุณุชุฎุฏุงู ุฃู ููุชุจุฉ ุชุฑูุฏูุง ูุชุฑููุฒ ูููุงุช ุงูุตูุช.

ูุณุชุฎุฏู ุงููุซุงู ุฃุฏูุงู ุญุฒูุฉ [`pydub`](http://pydub.com/) ููุชุญ ุชูุณูู ุตูุชู ูุง ูุฏุนูู `soundfile`:

```py
>>> import numpy as np
>>> from pydub import AudioSegment

>>> audio_dataset_amr = Dataset.from_dict({"audio": ["audio_samples/audio.amr"]})

>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):
...     def pydub_decode_file(audio_path):
...         sound = AudioSegment.from_file(audio_path)
...         if sound.frame_rate != sampling_rate:
...             sound = sound.set_frame_rate(sampling_rate)
...         channel_sounds = sound.split_to_mono()
...         samples = [s.get_array_of_samples() for s in channel_sounds]
...         fp_arr = np.array(samples).T.astype(np.float32)
...         fp_arr /= np.iinfo(samples[0].typecode).max
...         return fp_arr
...
...     batch["audio"] = [pydub_decode_file(audio_path) for audio_path in batch["audio"]]
...     return batch

>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)
```

## ุงูุญูุธ
ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ูุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุ ููููู ุญูุธูุง ูุฅุนุงุฏุฉ ุงุณุชุฎุฏุงููุง ูุงุญููุง ูุน [`~Dataset.save_to_disk`].

ุงุญูุธ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุนู ุทุฑูู ุชูููุฑ ุงููุณุงุฑ ุฅูู ุงูุฏููู ุงูุฐู ุชุฑูุฏ ุญูุธู ููู:

```py
>>> encoded_dataset.save_to_disk("path/of/my/dataset/directory")
```

ุงุณุชุฎุฏู ุฏุงูุฉ [`load_from_disk`] ูุฅุนุงุฏุฉ ุชุญููู ูุฌููุนุฉ ุงูุจูุงูุงุช:

```py
>>> from datasets import load_from_disk
>>> reloaded_dataset = load_from_disk("path/of/my/dataset/directory")
```

<Tip>
ูู ุชุฑูุฏ ุญูุธ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ูููุฑ ุชุฎุฒูู ุณุญุงุจูุ ุงูุฑุฃ ุฏููููุง [Cloud Storage](./filesystems) ููุนุฑูุฉ ููููุฉ ุญูุธ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู AWS ุฃู Google Cloud Storage.
</Tip>

## ุงูุชุตุฏูุฑ
ูุฏุนู ๐ค Datasets ุงูุชุตุฏูุฑ ุฃูุถูุงุ ุญุชู ุชุชููู ูู ุงูุนูู ูุน ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ูู ุชุทุจููุงุช ุฃุฎุฑู. ููุธูุฑ ุงูุฌุฏูู ุงูุชุงูู ุชูุณููุงุช ุงููููุงุช ุงููุฏุนููุฉ ุญุงูููุง ูุงูุชู ููููู ุงูุชุตุฏูุฑ ุฅูููุง:

| ููุน ุงูููู | ุทุฑููุฉ ุงูุชุตุฏูุฑ |
|-------------------------|----------------------------------------------------------------|
| CSV | [`Dataset.to_csv`] |
| JSON | [`Dataset.to_json`] |
| Parquet | [`Dataset.to_parquet`] |
| SQL | [`Dataset.to_sql`] |
| ูุงุฆู Python ูู ุงูุฐุงูุฑุฉ | [`Dataset.to_pandas`] ุฃู [`Dataset.to_dict`] |

ุนูู ุณุจูู ุงููุซุงูุ ูู ุจุชุตุฏูุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ููู CSV ุนูู ุงููุญู ุงูุชุงูู:

```py
>>> encoded_dataset.to_csv("path/of/my/dataset.csv")
```