# ุงูุงุณุชุฎุฏุงู ูุน Spark

ูุฐู ุงููุซููุฉ ูู ููุฏูุฉ ุณุฑูุนุฉ ูุงุณุชุฎุฏุงู ูุฌููุนุงุช ุงูุจูุงูุงุช ๐ค ูุน Sparkุ ูุน ุงูุชุฑููุฒ ุจุดูู ุฎุงุต ุนูู ููููุฉ ุชุญููู DataFrame ูู Spark ูู ูุงุฆู [`Dataset`].

ูู ููุงูุ ููููู ุงููุตูู ุงูุณุฑูุน ุฅูู ุฃู ุนูุตุฑ ูููููู ุงุณุชุฎุฏุงูู ููุญูู ุจูุงูุงุช ูุชุฏุฑูุจ ุงูููุงุฐุฌ.

## ุงูุชุญููู ูู Spark

ูุงุฆู [`Dataset`] ูู ุบูุงู ูุฌุฏูู Arrowุ ูุงูุฐู ูุณูุญ ุจุงููุฑุงุกุงุช ุงูุณุฑูุนุฉ ูู ุงููุตูููุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู PyTorch ู TensorFlow ู JAX tensors.

ูุชู ุชุนููู ุฌุฏูู Arrow ูู ุงูุฐุงูุฑุฉ ูู ุงููุฑุตุ ูุงูุฐู ููููู ุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฃูุจุฑ ูู ุฐุงูุฑุฉ ุงููุตูู ุงูุนุดูุงุฆู ุงููุชููุฑุฉ.

ููููู ุงูุญุตูู ุนูู [`Dataset`] ูู DataFrame ูู Spark ุจุงุณุชุฎุฏุงู [`Dataset.from_spark`]:

```py
>>> from datasets import Dataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = Dataset.from_spark(df)
```

ูููู ุนูุงู Spark ุจูุชุงุจุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุนูู ุงููุฑุต ูู ุฏููู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููููุงุช Arrowุ ููุชู ุชุญููู [`Dataset`] ูู ููุงู.

ุฃู ููููู ุชุฎุทู ุงูุชุฎุฒูู ุงููุคูุช ุจุงุณุชุฎุฏุงู [`IterableDataset.from_spark`]ุ ูุงูุฐู ูุนูุฏ [`IterableDataset`]:

```py
>>> from datasets import IterableDataset
>>> df = spark.createDataFrame(
...     data=[[1, "Elia"], [2, "Teo"], [3, "Fang"]],
...     columns=["id", "name"],
... )
>>> ds = IterableDataset.from_spark(df)
>>> print(next(iter(ds)))
{"id": 1, "name": "Elia"}
```

### ุงูุชุฎุฒูู ุงููุคูุช

ุนูุฏ ุงุณุชุฎุฏุงู [`Dataset.from_spark`]ุ ูุชู ุชุฎุฒูู ูุงุฆู [`Dataset`] ุงููุงุชุฌ ูุคูุชูุงุ ุฅุฐุง ููุช ุจุงุณุชุฏุนุงุก [`Dataset.from_spark`] ุนุฏุฉ ูุฑุงุช ุนูู ููุณ DataFrameุ ููู ูููู ุจุฅุนุงุฏุฉ ุชุดุบูู ูููุฉ Spark ุงูุชู ุชูุชุจ ูุฌููุนุฉ ุงูุจูุงูุงุช ููููุงุช Arrow ุนูู ุงููุฑุต.

ููููู ุชุนููู ูููุน ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุนู ุทุฑูู ุชูุฑูุฑ `cache_dir=` ุฅูู [`Dataset.from_spark`].

ุชุฃูุฏ ูู ุงุณุชุฎุฏุงู ูุฑุต ูุชููุฑ ููู ูู ุนูุงูู ูุขูุชู ุงูุญุงููุฉ (ุงูุณุงุฆู).

<Tip warning={true}>
ูู ุฌูุณุฉ ูุฎุชููุฉุ ูุง ูููู ูุฌุฏูู ุจูุงูุงุช Spark ููุณ [hash ุงูุฏูุงูู](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.semanticHash.html)ุ ูุณูุนูุฏ ุชุดุบูู ูููุฉ Spark ููุฎุฒููุง ูู ุฐุงูุฑุฉ ุชุฎุฒูู ูุคูุช ุฌุฏูุฏุฉ.
</Tip>

### ุฃููุงุน ุงูููุฒุงุช

ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุชุชููู ูู ุตูุฑ ุฃู ุจูุงูุงุช ุตูุชูุฉ ุฃู ูุตูููุงุช ูุชุนุฏุฏุฉ ุงูุฃุจุนุงุฏุ ูููููู ุชุญุฏูุฏ ูุณูุทุฉ `features=` ูู [`Dataset.from_spark`] (ุฃู [`IterableDataset.from_spark`]):

```py
>>> from datasets import Dataset, Features, Image, Value
>>> data = [(0, open("image.png", "rb").read())]
>>> df = spark.createDataFrame(data, "idx: int, image: binary")
>>> # ูุนูู ุฃูุถูุง ุฅุฐุง ูุงู ูุฏูู ูุตูููุงุช
>>> # data = [(0, np.zeros(shape=(32, 32, 3), dtype=np.int32).tolist())]
>>> # df = spark.createDataFrame(data, "idx: int, image: array<array<array<int>>>")
>>> features = Features({"idx": Value("int64"), "image": Image()})
>>> dataset = Dataset.from_spark(df, features=features)
>>> dataset[0]
{'idx': 0, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>}
```

ููููู ุงูุชุญูู ูู ูุซุงุฆู [`Features`] ููุชุนุฑู ุนูู ุฌููุน ุฃููุงุน ุงูููุฒุงุช ุงููุชุงุญุฉ.