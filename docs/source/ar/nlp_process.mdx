# ูุนุงูุฌุฉ ุจูุงูุงุช ุงููุตูุต

ููุถุญ ูุฐุง ุงูุฏููู ุทุฑููุง ูุญุฏุฏุฉ ููุนุงูุฌุฉ ูุฌููุนุงุช ุจูุงูุงุช ุงููุตูุต. ุชุนูู ููููุฉ:

- ุชูุณูู ูุฌููุนุฉ ุจูุงูุงุช ุจุงุณุชุฎุฏุงู [`~Dataset.map`].
- ูุญุงุฐุงุฉ ุชุณููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ูุน ูุนุฑูุงุช ุงูุชุณููุงุช ููุฌููุนุงุช ุจูุงูุงุช NLI.

ููุงุทูุงุน ุนูู ุฏููู ุญูู ููููุฉ ูุนุงูุฌุฉ ุฃู ููุน ูู ูุฌููุนุงุช ุงูุจูุงูุงุชุ ุฑุงุฌุน <a class="underline decoration-sky-400 decoration-2 font-semibold" href="./process">ุฏููู ุนูููุฉ ุงููุนุงูุฌุฉ ุงูุนุงูุฉ</a>.

## Map

ุชุฏุนู ุฏุงูุฉ [`~Dataset.map`] ูุนุงูุฌุฉ ุฏูุนุงุช ูู ุงูุฃูุซูุฉ ูู ููุณ ุงูููุชุ ููุง ูุณุฑุน ูู ุนูููุฉ ุงูุชูุณูู ุฅูู ุฑููุฒ.

ูู ุจุชุญููู ุฃุฏุงุฉ ุงูุชูุณูู ุฅูู ุฑููุฒ ูู ๐ค [Transformers](https://huggingface.co/transformers/):

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ุงุถุจุท ูุนููุฉ `batched` ุนูู `True` ูู ุฏุงูุฉ [`~Dataset.map`] ูุชุทุจูู ุฃุฏุงุฉ ุงูุชูุณูู ุฅูู ุฑููุฒ ุนูู ุฏูุนุงุช ูู ุงูุฃูุซูุฉ:

```py
>>> dataset = dataset.map(lambda examples: tokenizer(examples["text"]), batched=True)
>>> dataset[0]
{'text': 'the rock is destined to be the 21st century\'s new "conan" and that he\'s going to make a splash even greater than arnold schwarzenegger, jean-claud van damme or steven segal.',
'label': 1,
'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

ุชููู ุฏุงูุฉ [`~Dataset.map`] ุจุชุญููู ุงูููู ุงููุนุงุฏุฉ ุฅูู ุชูุณูู ูุฏุนูู ูู PyArrow. ูููู ุฅุนุงุฏุฉ ุงููุตูููุงุช ูู NumPy ุจุดูู ุตุฑูุญ ุฃุณุฑุน ูุฃููุง ุชูุณูู ูุฏุนูู ุจุดูู ุฃุตูู ูู PyArrow. ูู ุจุถุจุท `return_tensors="np"` ุนูุฏ ุชูุณูู ูุตู ุฅูู ุฑููุฒ:

```py
>>> dataset = dataset.map(lambda examples: tokenizer(examples["text"], return_tensors="np"), batched=True)
```

## Align

ุชููู ุฏุงูุฉ [`~Dataset.align_labels_with_mapping`] ุจูุญุงุฐุงุฉ ูุนุฑู ุชุณููุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุน ุงุณู ุงูุชุณููุฉ. ููุง ุชุชุจุน ุฌููุน ููุงุฐุฌ ๐ค Transformers ูุฎุทุท ุชุนููู ุงูุชุณููุงุช ุงูููุตู ุจู ููุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฃุตููุฉุ ุฎุงุตุฉ ุจุงููุณุจุฉ ููุฌููุนุงุช ุจูุงูุงุช NLI. ุนูู ุณุจูู ุงููุซุงูุ ูุณุชุฎุฏู ูุฌููุนุฉ ุจูุงูุงุช [MNLI](https://huggingface.co/datasets/glue) ูุฎุทุท ุงูุชุนููู ุงูุชุงูู:

```py
>>> label2id = {"entailment": 0, "neutral": 1, "contradiction": 2}
```

ูุชุญููู ูุญุงุฐุงุฉ ูุฎุทุท ุชุนููู ุงูุชุณููุงุช ููุฌููุนุฉ ุงูุจูุงูุงุช ูุน ุงููุฎุทุท ุงููุณุชุฎุฏู ูู ูุจู ูููุฐุฌุ ูู ุจุฅูุดุงุก ูุงููุณ ูุงุณู ุงูุชุณููุฉ ููุนุฑููุง ูุชุญููู ุงููุญุงุฐุงุฉ ุนููู:

```py
>>> label2id = {"contradiction": 0, "neutral": 1, "entailment": 2}
```

ูุฑุฑ ูุงููุณ ูุฎุทุทุงุช ุชุนููู ุงูุชุณููุงุช ุฅูู ุฏุงูุฉ [`~Dataset.align_labels_with_mapping`]ุ ูุงูุนููุฏ ุงูุฐู ุณูุชู ุชุญููู ุงููุญุงุฐุงุฉ ุนููู:

```py
>>> from datasets import load_dataset

>>> mnli = load_dataset("glue", "mnli", split="train")
>>> mnli_aligned = mnli.align_labels_with_mapping(label2id, "label")
```

ููููู ุฃูุถูุง ุงุณุชุฎุฏุงู ูุฐู ุงูุฏุงูุฉ ูุชุนููู ูุฎุทุท ูุฎุตุต ูุชุนููู ุงูุชุณููุงุช ุฅูู ูุนุฑูุงุช.