# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Øµ

ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Øµ. Ù„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø±Ø§Ø¬Ø¹ [Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…](./loading).

ØªØ¹Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Øµ Ø£Ø­Ø¯ Ø£ÙƒØ«Ø± Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø´ÙŠÙˆØ¹Ù‹Ø§ Ù„ØªØ®Ø²ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª. Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ÙŠÙ‚ÙˆÙ… ðŸ¤— Datasets Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ù„Ù Ø§Ù„Ù†Øµ Ø³Ø·Ø±Ù‹Ø§ Ø¨Ø³Ø·Ø± Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("text", data_files={"train": ["my_text_1.txt", "my_text_2.txt"], "test": "my_test_file.txt"})

# Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø¯Ù„ÙŠÙ„
>>> dataset = load_dataset("text", data_dir="path/to/text/dataset")
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù†ØµÙŠ ÙÙ‚Ø±Ø© Ø¨ÙÙ‚Ø±Ø© Ø£Ùˆ Ø­ØªÙ‰ Ù…Ø³ØªÙ†Ø¯Ù‹Ø§ ÙƒØ§Ù…Ù„Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø© `sample_by`:

```py
# Ø§Ù„Ø¹ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø§Ù„ÙÙ‚Ø±Ø©
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="paragraph")

# Ø§Ù„Ø¹ÙŠÙ†Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="document")
```

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù†Ù…Ø§Ø· grep Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ù…Ø­Ø¯Ø¯Ø©:

```py
>>> from datasets import load_dataset
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")
```

Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ø¹Ù† Ø¨ÙØ¹Ø¯ Ø¹Ø¨Ø± HTTPØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø¹Ù†Ø§ÙˆÙŠÙ† URL Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ:

```py
>>> dataset = load_dataset("text", data_files="https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt")
```