# تقدير العمق

تُستخدم مجموعات بيانات تقدير العمق لتدريب نموذج على تقدير المسافة النسبية لكل بكسل في صورة من الكاميرا، والتي يُطلق عليها أيضًا العمق. وتتركز التطبيقات التي تمكّنها هذه المجموعات من البيانات بشكل أساسي في مجالات مثل الإدراك البصري للآلات والإدراك في الروبوتات. وتشمل التطبيقات مثال رسم خرائط الشوارع للسيارات ذاتية القيادة. وسيوضح هذا الدليل كيفية تطبيق التحولات على مجموعة بيانات تقدير العمق.

قبل البدء، تأكد من أن لديك الإصدارات الأحدث من albumentations مثبتة:

```bash
pip install -U albumentations
```

[Albumentations](https://albumentations.ai/) هي مكتبة بايثون للقيام بزيادة البيانات للرؤية الحاسوبية. وهي تدعم العديد من مهام الرؤية الحاسوبية مثل تصنيف الصور، وكشف الأشياء، والتجزئة، وتقدير النقاط الرئيسية.

يستخدم هذا الدليل [NYU Depth V2](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2) dataset التي تتكون من تسلسلات فيديو من مشاهد داخلية مختلفة، تم تسجيلها بواسطة كاميرات RGB وعمق. وتتكون مجموعة البيانات من مشاهد من 3 مدن وتقدم صورًا جنبًا إلى جنب مع خرائط العمق الخاصة بها على شكل تسميات.

قم بتحميل قسم "التدريب" من مجموعة البيانات وانظر إلى مثال:

```py
>>> from datasets import load_dataset

>>> train_dataset = load_dataset("sayakpaul/nyu_depth_v2", split="train")
>>> index = 17
>>> example = train_dataset[index]
>>> example
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=640x480>,
'depth_map': <PIL.TiffImagePlugin.TiffImageFile image mode=F size=640x480>}
```

تحتوي مجموعة البيانات على حقلين:

* `image`: كائن صورة PNG PIL مع نوع بيانات `uint8`.
* `depth_map`: كائن صورة TIFF PIL مع نوع بيانات `float32` وهو خريطة العمق للصورة.

ومن الجدير بالذكر أن تنسيق JPEG/PNG لا يمكنه سوى تخزين بيانات `uint8` أو `uint16`. وبما أن خريطة العمق هي بيانات `float32`، فلا يمكن تخزينها باستخدام PNG/JPEG. ومع ذلك، يمكننا حفظ خريطة العمق باستخدام تنسيق TIFF لأنه يدعم مجموعة أوسع من أنواع البيانات، بما في ذلك بيانات `float32`.

بعد ذلك، تحقق من صورة باستخدام:

```py
>>> example["image"]
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_sample.png">
</div>

قبل أن نلقي نظرة على خريطة العمق، يجب علينا أولاً تحويل نوع البيانات الخاص بها إلى `uint8` باستخدام `.convert('RGB')` لأن PIL لا يمكنه عرض صور `float32`. الآن، الق نظرة على خريطة العمق المقابلة:

```py
>>> example["depth_map"].convert("RGB")
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target.png">
</div>

إنها سوداء تمامًا! ستحتاج إلى إضافة بعض الألوان إلى خريطة العمق لتصورها بشكل صحيح. للقيام بذلك، يمكننا إما تطبيق الألوان تلقائيًا أثناء العرض باستخدام `plt.imshow()` أو إنشاء خريطة عمق ملونة باستخدام `plt.cm` ثم عرضها. في هذا المثال، استخدمنا الأخيرة، حيث يمكننا حفظ/كتابة خريطة العمق الملونة لاحقًا. (تم أخذ الأداة المساعدة أدناه من [مستودع FastDepth](https://github.com/dwofk/fast-depth/blob/master/utils.py)).

```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> cmap = plt.cm.viridis

>>> def colored_depthmap(depth, d_min=None, d_max=None):
...     if d_min is None:
...         d_min = np.min(depth)
...     if d_max is None:
...         d_max = np.max(depth)
...     depth_relative = (depth - d_min) / (d_max - d_min)
...     return 255 * cmap(depth_relative)[:,:,:3]

>>> def show_depthmap(depth_map):
...    if not isinstance(depth_map, np.ndarray):
...        depth_map = np.array(depth_map)
...    if depth_map.ndim == 3:
...        depth_map = depth_map.squeeze()

...    d_min = np.min(depth_map)
...    d_max = np.max(depth_map)
...    depth_map = colored_depthmap(depth_map, d_min, d_max)

...    plt.imshow(depth_map.astype("uint8"))
...    pltMultiplier
...    plt.axis("off")
...    plt.show()

>>> show_depthmap(example["depth_map"])
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target_viz.png">
</div>

يمكنك أيضًا تصور العديد من الصور المختلفة وخرائط العمق المقابلة لها.

```py
>>> def merge_into_row(input_image, depth_target):
...     if not isinstance(input_image, np.ndarray):
...         input_image = np.array(input_image)
...
...     d_min = np.min(depth_target)
...     d_max = np.max(depth_target)
...     depth_target_col = colored_depthmap(depth_target, d_min, d_max)
...     img_merge = np.hstack([input_image, depth_target_col])
...
...     return img_merge

>>> random_indices = np.random.choice(len(train_dataset), 9).tolist()
>>> plt.figure(figsize=(15, 6))
>>> for i, idx in enumerate(random_indices):
...     example = train_dataset[idx]
...     ax = plt.subplot(3, 3, i + 1)
...     image_viz = merge_into_row(
...         example["image"], example["depth_map"]
...     )
...     plt.imshow(image_viz.astype("uint8"))
...     plt.axis("off")
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_collage.png">
</div>

الآن، قم بتطبيق بعض التحسينات باستخدام `albumentations`. تتضمن تحولات التعزيز ما يلي:

* الانعكاس الأفقي العشوائي
* القص العشوائي
* السطوع العشوائي والتباين
* تصحيح غاما العشوائي
* التشبع العشوائي للصبغة

```py
>>> import albumentations as A

>>> crop_size = (448, 576)
>>> transforms = [
...     A.HorizontalFlip(p=0.5),
...     A.RandomCrop(crop_size[0], crop_size[1]),
...     A.RandomBrightnessContrast(),
...     A.RandomGamma(),
...     A.HueSaturationValue()
... ]
```

بالإضافة إلى ذلك، حدد خريطة إلى الاسم المستهدف بشكل أفضل.

```py
>>> additional_targets = {"depth": "mask"}
>>> aug = A.Compose(transforms=transforms, additional_targets=additional_targets)
```

مع `additional_targets` المحددة، يمكنك تمرير خرائط العمق المستهدفة إلى وسيط `depth` من `aug` بدلاً من `mask`. ستلاحظ هذا التغيير في دالة `apply_transforms()` المحددة أدناه.

قم بإنشاء دالة لتطبيق التحول على الصور وكذلك خرائط العمق الخاصة بها:

```py
>>> def apply_transforms(examples):
...     transformed_images, transformed_maps = [], []
...     for image, depth_map in zip(examples["image"], examples["depth_map"]):
...         image, depth_map = np.array(image), np.array(depth_map)
...         transformed = aug(image=image, depth=depth_map)
...         transformed_images.append(transformed["image"])
...         transformed_maps.append(transformed["depth"])
...
...     examples["pixel_values"] = transformed_images
...     examples["labels"] = transformed_maps
...     return examples
```

استخدم وظيفة [`~Dataset.set_transform`] لتطبيق التحول أثناء التنقل على دفعات من مجموعة البيانات لتقليل استخدام مساحة القرص:

```py
>>> train_dataset.set_transform(apply_transforms)
```

يمكنك التحقق من أن التحول قد عمل عن طريق الفهرسة في `pixel_values` و`labels` لصورة مثال:

```py
>>> example = train_dataset[index]

>>> plt.imshow(example["pixel_values"])
>>> plt.axis("off")
>>> plt.show()
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_sample_aug.png">
</div>

قم بتصور نفس التحول على خريطة العمق المقابلة للصورة:

```py
>>> show_depthmap(example["labels"])
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_target_aug.png">
</div>

يمكنك أيضًا تصور عدة عينات تدريبية عن طريق إعادة استخدام `random_indices` السابقة:

```py
>>> plt.figure(figsize=(15, 6))

>>> for i, idx in enumerate(random_indices):
...     ax = plt.subplot(3, 3, i + 1)
...     example = train_dataset[idx]
...     image_viz = merge_into_row(
...         example["pixel_values"], example["labels"]
...     )
...     plt.imshow(image_viz.astype("uint8"))
...     plt.axis("off")
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/depth_est_aug_collage.png">
</div>