# Beam Datasets

<Tip warning={true}>

The Beam API is deprecated and will be removed in the next major release. 

</Tip>

Some datasets are too large to be processed on a single machine. Instead, you can process them with [Apache Beam](https://beam.apache.org/), a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/), or [Google Cloud Dataflow](https://cloud.google.com/dataflow).

We have already created Beam pipelines for some of the larger datasets like [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b). You can load these normally with [`load_dataset`]. But if you want to run your own Beam pipeline with Dataflow, here is how:

1. Specify the dataset and configuration you want to process:

```
DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20220301.en
```

2. Input your Google Cloud Platform information:

```
PROJECT=your_project
BUCKET=your_bucket
REGION=your_region
```

If you have never done so, don't forget to create a `default_applications_credentials.json` file (full docs [here](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login))

```
gcloud auth application-default login
```

This will create a file in `~/.config/gcloud/application_default_credentials.json` which the gcfs library will use to authenticate to Google Cloud Platform.

3. Specify your Python requirements:

Here are the dependpencies for the wikipedia dataset Beam pipeline:

```
echo "datasets" > /tmp/beam_requirements.txt
echo "apache_beam[gcp,dataframe]" >> /tmp/beam_requirements.txt
echo "git+https://github.com/earwig/mwparserfromhell.git@0f89f44" >> /tmp/beam_requirements.txt
```

The version of mwparserfromhell is currently quite precise because otherwise the production version (currently `0.6`) has a bug which crashes the pipeline. This is fixed on the `main` branch (see [Datasets issue #577](https://github.com/huggingface/datasets/issues/577) for details), and the next release of mwparserfromhell will likely contain this bugfix.

4. Run the pipeline:

```
datasets-cli run_beam datasets/$DATASET_NAME \
--name $CONFIG_NAME \
--save_info \
--cache_dir gs://$BUCKET/cache/datasets \
--beam_pipeline_options=\
"runner=DataflowRunner,project=$PROJECT,job_name=$DATASET_NAME-gen,"\
"staging_location=gs://$BUCKET/binaries,temp_location=gs://$BUCKET/temp,"\
"region=$REGION,requirements_file=/tmp/beam_requirements.txt"
```

<Tip>

When you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.

</Tip>

5. Example: running the wikipedia datasets pipeline on DataFlow

```
datasets-cli run_beam ${DATASET_NAME} \
--name 20220301.fr \
--language fr \
--date 20230601 \
--save_info \
--cache_dir gs://$BUCKET/cache/datasets \
--beam_pipeline_options=\
"runner=DataflowRunner,project=${PROJECT},job_name=${DATASET_NAME}-gen,"\
"staging_location=gs://${BUCKET}/binaries,temp_location=gs://${BUCKET}/temp,"\
"region=${REGION},requirements_file=/tmp/beam_requirements.txt"
```

Maybe a bit counter-intuitive is that you have to specify the current config name (`20220301.fr`) and then override it with the `language` and `date` arguments. This allows the dataset script [wikipedia](https://huggingface.co/datasets/wikipedia/blob/main/wikipedia.py) to have a valid config. That script allows a specific date and language to be specified, but only on top of a valid configuration.
