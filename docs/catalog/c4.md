<div itemscope itemtype="http://schema.org/Dataset">
  <div itemscope itemprop="includedInDataCatalog" itemtype="http://schema.org/DataCatalog">
    <meta itemprop="name" content="TensorFlow Datasets" />
  </div>
  <meta itemprop="name" content="c4" />
  <meta itemprop="description" content="A colossal, cleaned version of Common Crawl&#x27;s web crawl corpus.&#10;&#10;Based on Common Crawl dataset: &quot;https://commoncrawl.org&quot;&#10;&#10;Due to the overhead of cleaning the dataset, it is recommend you prepare it with&#10;a distributed service like Cloud Dataflow. More info at&#10;https://www.tensorflow.org/datasets/beam_datasets.&#10;&#10;&#10;To use this dataset:&#10;&#10;```python&#10;import tensorflow_datasets as tfds&#10;&#10;ds = tfds.load(&#x27;c4&#x27;, split=&#x27;train&#x27;)&#10;for ex in ds.take(4):&#10;  print(ex)&#10;```&#10;&#10;See [the guide](https://www.tensorflow.org/datasets/overview) for more&#10;informations on [tensorflow_datasets](https://www.tensorflow.org/datasets).&#10;&#10;" />
  <meta itemprop="url" content="https://www.tensorflow.org/datasets/catalog/c4" />
  <meta itemprop="sameAs" content="https://github.com/google-research/text-to-text-transfer-transformer#datasets" />
  <meta itemprop="citation" content="&#10;@article{2019t5,&#10;  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},&#10;  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},&#10;  journal = {arXiv e-prints},&#10;  year = {2019},&#10;  archivePrefix = {arXiv},&#10;  eprint = {1910.10683},&#10;}&#10;" />
</div>
# `c4`

Warning: Manual download required. See instructions below.

*   **Description**:

A colossal, cleaned version of Common Crawl's web crawl corpus.

Based on Common Crawl dataset: "https://commoncrawl.org"

Due to the overhead of cleaning the dataset, it is recommend you prepare it with
a distributed service like Cloud Dataflow. More info at
https://www.tensorflow.org/datasets/beam_datasets.

*   **Homepage**:
    [https://github.com/google-research/text-to-text-transfer-transformer#datasets](https://github.com/google-research/text-to-text-transfer-transformer#datasets)
*   **Source code**:
    [`tfds.text.c4.C4`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/c4.py)
*   **Versions**:
    *   **`2.3.0`** (default): Deduplicate lines within a page.
    *   `2.2.1`: Update dataset_info.json
    *   `2.2.0`: No release notes.
*   **Manual download instructions**: This dataset requires you to download the
    source data manually into `download_config.manual_dir`
    (defaults to `~/tensorflow_datasets/manual/c4/`):<br/>
    For the WebText-like config, you must manually download 'OpenWebText.zip'
    (from https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl
    WET files from August 2018 to July 2019
    (https://commoncrawl.org/the-data/get-started/) and place them in the
    `manual_dir`.
*   **Auto-cached**
    ([documentation](https://www.tensorflow.org/datasets/performances#auto-caching)):
    No
*   **Features**:

```python
FeaturesDict({
    'content-length': Text(shape=(), dtype=tf.string),
    'content-type': Text(shape=(), dtype=tf.string),
    'text': Text(shape=(), dtype=tf.string),
    'timestamp': Text(shape=(), dtype=tf.string),
    'url': Text(shape=(), dtype=tf.string),
})
```
*   **Supervised keys** (See
    [`as_supervised` doc](https://www.tensorflow.org/datasets/api_docs/python/tfds/load#args)):
    `None`
*   **Citation**:

```
@article{2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv e-prints},
  year = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.10683},
}
```

## c4/en (default config)

*   **Config description**: English C4 dataset.
*   **Download size**: `6.96 TiB`
*   **Dataset size**: `803.19 GiB`
*   **Splits**:

Split        | Examples
:----------- | ----------:
'train'      | 362,792,424
'validation' | 362,640

## c4/en.noclean

*   **Config description**: Disables all cleaning (deduplication, removal based
    on bad words, etc.)
*   **Download size**: `6.96 TiB`
*   **Dataset size**: `6.21 TiB`
*   **Splits**:

Split        | Examples
:----------- | ------------:
'train'      | 1,063,805,296
'validation' | 1,065,015

## c4/en.realnewslike

*   **Config description**: Filters from the default config to only include
    content from the domains used in the 'RealNews' dataset (Zellers et al.,
    2019).
*   **Download size**: `6.96 TiB`
*   **Dataset size**: `36.51 GiB`
*   **Splits**:

Split        | Examples
:----------- | ---------:
'train'      | 13,621,566
'validation' | 13,688

## c4/en.webtextlike

*   **Config description**: Filters from the default config to only include
    content from the URLs in OpenWebText
    (https://github.com/jcpeterson/openwebtext).
*   **Download size**: `6.96 TiB`
*   **Dataset size**: `17.81 GiB`
*   **Splits**:

Split        | Examples
:----------- | --------:
'train'      | 4,438,086
'validation' | 4,425
