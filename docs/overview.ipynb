{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6XvCUmCEd4Dm"
      },
      "source": [
        "# TensorFlow Datasets\n",
        "\n",
        "TFDS provides a collection of ready-to-use datasets. It handles downloading and preparing the data and constructing a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
        "\n",
        "Note: Do not confuse [TFDS](https://www.tensorflow.org/datasets) (this library) with [tf.data](https://www.tensorflow.org/guide/data) (TensorFlow API to build efficient data pipelines). TFDS is a high level wrapper around `tf.data`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J8y9ZkLXmAZc"
      },
      "source": [
        "Copyright 2018 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGw9EgE0tC0C"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/datasets/overview\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/overview.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_7hshda5eaGL"
      },
      "source": [
        "## Installation\n",
        "\n",
        "`pip install nlp`\n",
        "\n",
        "Note that `nlp` expects you to have TensorFlow already installed, and currently depends on `tensorflow` (or `tensorflow-gpu`) \u003e= `1.15.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "boeZp0sYbO41"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow nlp matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TTBSvHcSLBzc"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "import nlp\n",
        "nlp.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GmeeOokMODg2"
      },
      "source": [
        "## Citation\n",
        "\n",
        "Please include the following citation when using `nlp` for a paper, in addition to any citation specific to the used datasets.\n",
        "\n",
        "```\n",
        "@misc{TFDS,\n",
        "  title = { {TensorFlow Datasets}, A collection of ready-to-use datasets},\n",
        "  howpublished = {\\url{https://www.tensorflow.org/datasets}},\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8-ZBEd6Ie28N"
      },
      "source": [
        "## Eager execution\n",
        "\n",
        "TensorFlow Datasets is compatible with both TensorFlow [Eager mode](https://www.tensorflow.org/guide/eager) and Graph mode. For this colab, we'll run in Eager mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "o9H2EiXzfNgO"
      },
      "outputs": [],
      "source": [
        "tf.enable_v2_behavior()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VZZyuO13fPvk"
      },
      "source": [
        "## List the available datasets\n",
        "\n",
        "Each dataset is implemented as a [`nlp.DatasetBuilder`](https://www.tensorflow.org/datasets/api_docs/python/nlp/core/DatasetBuilder) and you can list all available builders with `nlp.list_builders()`.\n",
        "\n",
        "You can see all the datasets with additional documentation on the [datasets documentation page](https://www.tensorflow.org/datasets/catalog/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FAvbSVzjLCIb"
      },
      "outputs": [],
      "source": [
        "nlp.list_builders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VjI6VgOBf0v0"
      },
      "source": [
        "## `nlp.load`: A dataset in one line\n",
        "\n",
        "[`nlp.load`](https://www.tensorflow.org/datasets/api_docs/python/nlp/load) is a convenience method that's the simplest way to build and load a `tf.data.Dataset`.\n",
        "\n",
        "`tf.data.Dataset` is the standard TensorFlow API to build input pipelines. If you're not familiar with this API, we **strongly** encourage you to read [the official TensorFlow guide](https://www.tensorflow.org/guide/datasets).\n",
        "\n",
        "Below, we load the MNIST training data. It downloads and prepares the data, unless you specify `download=False`. Note that once data has been prepared, subsequent calls of `load` will reuse the prepared data.\n",
        "You can customize where the data is saved/loaded by specifying `data_dir=` (\n",
        "defaults to `~/nlp/`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dCou80mnLLPV"
      },
      "outputs": [],
      "source": [
        "ds_train = nlp.load(name=\"mnist\", split=\"train\")\n",
        "assert isinstance(ds_train, tf.data.Dataset)\n",
        "print(ds_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D4A6yp_Ii3G-"
      },
      "source": [
        "When loading a dataset, the canonical default version is used. It is however recommended to specify the major version of the dataset to use, and to advertise which version of the dataset was used in your results. See the\n",
        "[documentation on datasets versioning](https://github.com/tensorflow/datasets/blob/master/docs/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OCXCz-vhj0kE"
      },
      "outputs": [],
      "source": [
        "ds_all = nlp.load(\"mnist:3.*.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-GAxR79hGTr"
      },
      "source": [
        "## Feature dictionaries\n",
        "\n",
        "All `nlp` datasets contain feature dictionaries mapping feature names to Tensor values. A typical dataset, like MNIST, will have 2 keys: `\"image\"` and `\"label\"`. Below we inspect a single example.\n",
        "\n",
        "Note: In graph mode, see the [tf.data guide](https://www.tensorflow.org/guide/datasets#creating_an_iterator) to understand how to iterate on a `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YHE21nkHLrER"
      },
      "outputs": [],
      "source": [
        "for example in ds_train.take(1):  # Only take a single example\n",
        "  image, label = example[\"image\"], example[\"label\"]\n",
        "\n",
        "  plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))\n",
        "  print(\"Label: %d\" % label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EW-kEK_mhbhy"
      },
      "source": [
        "## `DatasetBuilder`\n",
        "\n",
        "`nlp.load` is really a thin conveninence wrapper around `DatasetBuilder`. We can accomplish the same as above directly with the MNIST `DatasetBuilder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9FDDJXmBhpQ4"
      },
      "outputs": [],
      "source": [
        "mnist_builder = nlp.builder(\"mnist\")\n",
        "mnist_builder.download_and_prepare()\n",
        "ds_train = mnist_builder.as_dataset(split=\"train\")\n",
        "ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7tlVOAzLlKqc"
      },
      "source": [
        "## Input pipelines\n",
        "\n",
        "Once you have a `tf.data.Dataset` object, it's simple to define the rest of an input pipeline suitable for model training by using the [`tf.data` API](https://www.tensorflow.org/guide/datasets).\n",
        "\n",
        "Here we'll repeat the dataset so that we have an infinite stream of examples, shuffle, and create batches of 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9OQZqGZMlSE8"
      },
      "outputs": [],
      "source": [
        "ds_train = ds_train.repeat().shuffle(1024).batch(32)\n",
        "\n",
        "# prefetch will enable the input pipeline to asynchronously fetch batches while\n",
        "# your model is training.\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Now you could loop over batches of the dataset and train\n",
        "# for batch in ds_train:\n",
        "#   ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uczpNuc_A7wE"
      },
      "source": [
        "## DatasetInfo\n",
        "\n",
        "After generation, the builder contains useful information on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mSamfFznA9Ph"
      },
      "outputs": [],
      "source": [
        "info = mnist_builder.info\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cspsneov2VbC"
      },
      "source": [
        "`DatasetInfo` also contains useful information about the features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "u1wL14QH2TW1"
      },
      "outputs": [],
      "source": [
        "print(info.features)\n",
        "print(info.features[\"label\"].num_classes)\n",
        "print(info.features[\"label\"].names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xbrm0WBmBLEI"
      },
      "source": [
        "You can also load the `DatasetInfo` directly with `nlp.load` using `with_info=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tvZYujQwBL7B"
      },
      "outputs": [],
      "source": [
        "ds_test, info = nlp.load(\"mnist\", split=\"test\", with_info=True)\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTRTEQqscxAE"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "For image classification datasets, you can use `nlp.show_examples` to display some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DpE2FD56cSQR"
      },
      "outputs": [],
      "source": [
        "fig = nlp.show_examples(info, ds_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//research/colab/notebook:notebook_backend_py3",
        "kind": "private"
      },
      "name": "tensorflow/datasets",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
