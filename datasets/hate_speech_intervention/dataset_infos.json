{"gab": {"description": "In order to encourage strategies of countering online hate speech, this benchmark introduces a novel task of generative hate speech intervention along with two fully-labeled datasets collected from Gab and Reddit. Distinct from existing hate speech datasets, these datasets retain their conversational context and introduce human-written intervention responses. Due to the data collecting strategy, all the posts in the datasets are manually labeled as hate or non-hate speech by Mechanical Turk workers, so they can also be used for the hate speech detection task.\n", "citation": "@inproceedings{qian-etal-2019-benchmark,\n    title = \"A Benchmark Dataset for Learning to Intervene in Online Hate Speech\",\n    author = \"Qian, Jing  and\n      Bethke, Anna  and\n      Liu, Yinyin  and\n      Belding, Elizabeth  and\n      Wang, William Yang\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/D19-1482\",\n    doi = \"10.18653/v1/D19-1482\",\n    pages = \"4755--4764\",\n    abstract = \"Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.\",\n}\n\n", "homepage": "https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech", "license": "", "features": {"id": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "text": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "hate_speech_idx": {"feature": {"dtype": "int32", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "response": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}}, "post_processed": null, "supervised_keys": null, "task_templates": null, "builder_name": "hate_speech_intervention", "config_name": "gab", "version": {"version_str": "1.1.0", "description": null, "major": 1, "minor": 1, "patch": 0}, "splits": {"full": {"name": "full", "num_bytes": 9135090, "num_examples": 11825, "dataset_name": "hate_speech_intervention"}}, "download_checksums": {"https://raw.githubusercontent.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/master/data/gab.csv": {"num_bytes": 9161771, "checksum": "79d858dc3a7545b73a44386625365f8311a7d147c6234c2cdf7e76d8e5e92314"}}, "download_size": 9161771, "post_processing_size": null, "dataset_size": 9135090, "size_in_bytes": 18296861}, "reddit": {"description": "In order to encourage strategies of countering online hate speech, this benchmark introduces a novel task of generative hate speech intervention along with two fully-labeled datasets collected from Gab and Reddit. Distinct from existing hate speech datasets, these datasets retain their conversational context and introduce human-written intervention responses. Due to the data collecting strategy, all the posts in the datasets are manually labeled as hate or non-hate speech by Mechanical Turk workers, so they can also be used for the hate speech detection task.\n", "citation": "@inproceedings{qian-etal-2019-benchmark,\n    title = \"A Benchmark Dataset for Learning to Intervene in Online Hate Speech\",\n    author = \"Qian, Jing  and\n      Bethke, Anna  and\n      Liu, Yinyin  and\n      Belding, Elizabeth  and\n      Wang, William Yang\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\n    month = nov,\n    year = \"2019\",\n    address = \"Hong Kong, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/D19-1482\",\n    doi = \"10.18653/v1/D19-1482\",\n    pages = \"4755--4764\",\n    abstract = \"Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets collected from Gab and Reddit. These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.\",\n}\n\n", "homepage": "https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech", "license": "", "features": {"id": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "text": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "hate_speech_idx": {"feature": {"dtype": "int32", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}, "response": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}}, "post_processed": null, "supervised_keys": null, "task_templates": null, "builder_name": "hate_speech_intervention", "config_name": "reddit", "version": {"version_str": "1.1.0", "description": null, "major": 1, "minor": 1, "patch": 0}, "splits": {"full": {"name": "full", "num_bytes": 7311031, "num_examples": 5020, "dataset_name": "hate_speech_intervention"}}, "download_checksums": {"https://raw.githubusercontent.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/master/data/reddit.csv": {"num_bytes": 7384220, "checksum": "b5a55dd1dd69e6770d836f7d0d8c1bd60477ec9d47b45111fbfe094d7268ab26"}}, "download_size": 7384220, "post_processing_size": null, "dataset_size": 7311031, "size_in_bytes": 14695251}}