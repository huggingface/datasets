---
annotations_creators:
- no-annotation
language_creators:
- crowdsourced
pretty_name: Wikipedia
paperswithcode_id: null
licenses:
- cc-by-sa-3.0
- gfdl-1.3-or-later
task_categories:
- sequence-modeling
task_ids:
- language-modeling
source_datasets:
- original
multilinguality:
- multilingual
size_categories:
- n<1K
- 1K<n<10K
- 10K<n<100K
- 100K<n<1M
- 1M<n<10M
- 10M<n<100M
languages:
  20200501.aa:
  - aa
  20200501.ab:
  - ab
  20200501.ace:
  - ace
  20200501.ady:
  - unknown
  20200501.af:
  - af
  20200501.ak:
  - ak
  20200501.als:
  - als
  20200501.am:
  - am
  20200501.an:
  - an
  20200501.ang:
  - ang
  20200501.ar:
  - ar
  20200501.arc:
  - arc
  20200501.arz:
  - arz
  20200501.as:
  - as
  20200501.ast:
  - ast
  20200501.atj:
  - atj
  20200501.av:
  - av
  20200501.ay:
  - ay
  20200501.az:
  - az
  20200501.azb:
  - azb
  20200501.ba:
  - ba
  20200501.bar:
  - bar
  20200501.bat-smg:
  - sgs
  20200501.bcl:
  - bcl
  20200501.be:
  - be
  20200501.be-x-old:
  - unknown
  20200501.bg:
  - bg
  20200501.bh:
  - bh
  20200501.bi:
  - bi
  20200501.bjn:
  - bjn
  20200501.bm:
  - bm
  20200501.bn:
  - bn
  20200501.bo:
  - bo
  20200501.bpy:
  - bpy
  20200501.br:
  - br
  20200501.bs:
  - bs
  20200501.bug:
  - bug
  20200501.bxr:
  - bxr
  20200501.ca:
  - ca
  20200501.cbk-zam:
  - cbk
  20200501.cdo:
  - cdo
  20200501.ce:
  - ce
  20200501.ceb:
  - ceb
  20200501.ch:
  - ch
  20200501.cho:
  - cho
  20200501.chr:
  - chr
  20200501.chy:
  - chy
  20200501.ckb:
  - ckb
  20200501.co:
  - co
  20200501.cr:
  - cr
  20200501.crh:
  - crh
  20200501.cs:
  - cs
  20200501.csb:
  - csb
  20200501.cu:
  - cu
  20200501.cv:
  - cv
  20200501.cy:
  - cy
  20200501.da:
  - da
  20200501.de:
  - de
  20200501.din:
  - din
  20200501.diq:
  - diq
  20200501.dsb:
  - dsb
  20200501.dty:
  - dty
  20200501.dv:
  - dv
  20200501.dz:
  - dz
  20200501.ee:
  - ee
  20200501.el:
  - el
  20200501.eml:
  - eml
  20200501.en:
  - en
  20200501.eo:
  - eo
  20200501.es:
  - es
  20200501.et:
  - et
  20200501.eu:
  - eu
  20200501.ext:
  - ext
  20200501.fa:
  - fa
  20200501.ff:
  - ff
  20200501.fi:
  - fi
  20200501.fiu-vro:
  - vro
  20200501.fj:
  - fj
  20200501.fo:
  - fo
  20200501.fr:
  - fr
  20200501.frp:
  - frp
  20200501.frr:
  - frr
  20200501.fur:
  - fur
  20200501.fy:
  - fy
  20200501.ga:
  - ga
  20200501.gag:
  - gag
  20200501.gan:
  - gan
  20200501.gd:
  - gd
  20200501.gl:
  - gl
  20200501.glk:
  - glk
  20200501.gn:
  - gn
  20200501.gom:
  - gom
  20200501.gor:
  - gor
  20200501.got:
  - got
  20200501.gu:
  - gu
  20200501.gv:
  - gv
  20200501.ha:
  - ha
  20200501.hak:
  - hak
  20200501.haw:
  - haw
  20200501.he:
  - he
  20200501.hi:
  - hi
  20200501.hif:
  - hif
  20200501.ho:
  - ho
  20200501.hr:
  - hr
  20200501.hsb:
  - hsb
  20200501.ht:
  - ht
  20200501.hu:
  - hu
  20200501.hy:
  - hy
  20200501.ia:
  - ia
  20200501.id:
  - id
  20200501.ie:
  - ie
  20200501.ig:
  - ig
  20200501.ii:
  - ii
  20200501.ik:
  - ik
  20200501.ilo:
  - ilo
  20200501.inh:
  - inh
  20200501.io:
  - io
  20200501.is:
  - is
  20200501.it:
  - it
  20200501.iu:
  - iu
  20200501.ja:
  - ja
  20200501.jam:
  - jam
  20200501.jbo:
  - jbo
  20200501.jv:
  - jv
  20200501.ka:
  - ka
  20200501.kaa:
  - kaa
  20200501.kab:
  - kab
  20200501.kbd:
  - kbd
  20200501.kbp:
  - kbp
  20200501.kg:
  - kg
  20200501.ki:
  - ki
  20200501.kj:
  - kj
  20200501.kk:
  - kk
  20200501.kl:
  - kl
  20200501.km:
  - km
  20200501.kn:
  - kn
  20200501.ko:
  - ko
  20200501.koi:
  - koi
  20200501.krc:
  - krc
  20200501.ks:
  - ks
  20200501.ksh:
  - ksh
  20200501.ku:
  - ku
  20200501.kv:
  - kv
  20200501.kw:
  - kw
  20200501.ky:
  - ky
  20200501.la:
  - la
  20200501.lad:
  - lad
  20200501.lb:
  - lb
  20200501.lbe:
  - lbe
  20200501.lez:
  - lez
  20200501.lfn:
  - lfn
  20200501.lg:
  - lg
  20200501.li:
  - li
  20200501.lij:
  - lij
  20200501.lmo:
  - lmo
  20200501.ln:
  - ln
  20200501.lo:
  - lo
  20200501.lrc:
  - lrc
  20200501.lt:
  - lt
  20200501.ltg:
  - ltg
  20200501.lv:
  - lv
  20200501.mai:
  - mai
  20200501.map-bms:
  - unknown
  20200501.mdf:
  - mdf
  20200501.mg:
  - mg
  20200501.mh:
  - mh
  20200501.mhr:
  - mhr
  20200501.mi:
  - mi
  20200501.min:
  - min
  20200501.mk:
  - mk
  20200501.ml:
  - ml
  20200501.mn:
  - mn
  20200501.mr:
  - mr
  20200501.mrj:
  - mrj
  20200501.ms:
  - ms
  20200501.mt:
  - mt
  20200501.mus:
  - mus
  20200501.mwl:
  - mwl
  20200501.my:
  - my
  20200501.myv:
  - myv
  20200501.mzn:
  - mzn
  20200501.na:
  - na
  20200501.nah:
  - nah
  20200501.nap:
  - nap
  20200501.nds:
  - nds
  20200501.nds-nl:
  - nds-nl
  20200501.ne:
  - ne
  20200501.new:
  - new
  20200501.ng:
  - ng
  20200501.nl:
  - nl
  20200501.nn:
  - nn
  20200501.no:
  - "no"
  20200501.nov:
  - nov
  20200501.nrm:
  - nrf
  20200501.nso:
  - nso
  20200501.nv:
  - nv
  20200501.ny:
  - ny
  20200501.oc:
  - oc
  20200501.olo:
  - olo
  20200501.om:
  - om
  20200501.or:
  - or
  20200501.os:
  - os
  20200501.pa:
  - pa
  20200501.pag:
  - pag
  20200501.pam:
  - pam
  20200501.pap:
  - pap
  20200501.pcd:
  - pcd
  20200501.pdc:
  - pdc
  20200501.pfl:
  - pfl
  20200501.pi:
  - pi
  20200501.pih:
  - pih
  20200501.pl:
  - pl
  20200501.pms:
  - pms
  20200501.pnb:
  - pnb
  20200501.pnt:
  - pnt
  20200501.ps:
  - ps
  20200501.pt:
  - pt
  20200501.qu:
  - qu
  20200501.rm:
  - rm
  20200501.rmy:
  - rmy
  20200501.rn:
  - rn
  20200501.ro:
  - ro
  20200501.roa-rup:
  - rup
  20200501.roa-tara:
  - unknown
  20200501.ru:
  - ru
  20200501.rue:
  - rue
  20200501.rw:
  - rw
  20200501.sa:
  - sa
  20200501.sah:
  - sah
  20200501.sat:
  - sat
  20200501.sc:
  - sc
  20200501.scn:
  - scn
  20200501.sco:
  - sco
  20200501.sd:
  - sd
  20200501.se:
  - se
  20200501.sg:
  - sg
  20200501.sh:
  - sh
  20200501.si:
  - si
  20200501.simple:
  - simple
  20200501.sk:
  - sk
  20200501.sl:
  - sl
  20200501.sm:
  - sm
  20200501.sn:
  - sn
  20200501.so:
  - so
  20200501.sq:
  - sq
  20200501.sr:
  - sr
  20200501.srn:
  - srn
  20200501.ss:
  - ss
  20200501.st:
  - st
  20200501.stq:
  - stq
  20200501.su:
  - su
  20200501.sv:
  - sv
  20200501.sw:
  - sw
  20200501.szl:
  - szl
  20200501.ta:
  - ta
  20200501.tcy:
  - tcy
  20200501.te:
  - te
  20200501.tet:
  - tdt
  20200501.tg:
  - tg
  20200501.th:
  - th
  20200501.ti:
  - ti
  20200501.tk:
  - tk
  20200501.tl:
  - tl
  20200501.tn:
  - tn
  20200501.to:
  - to
  20200501.tpi:
  - tpi
  20200501.tr:
  - tr
  20200501.ts:
  - ts
  20200501.tt:
  - tt
  20200501.tum:
  - tum
  20200501.tw:
  - tw
  20200501.ty:
  - ty
  20200501.tyv:
  - tyv
  20200501.udm:
  - udm
  20200501.ug:
  - ug
  20200501.uk:
  - uk
  20200501.ur:
  - ur
  20200501.uz:
  - uz
  20200501.ve:
  - ve
  20200501.vec:
  - vec
  20200501.vep:
  - vep
  20200501.vi:
  - vi
  20200501.vls:
  - vls
  20200501.vo:
  - vo
  20200501.wa:
  - wa
  20200501.war:
  - war
  20200501.wo:
  - wo
  20200501.wuu:
  - wuu
  20200501.xal:
  - xal
  20200501.xh:
  - xh
  20200501.xmf:
  - xmf
  20200501.yi:
  - yi
  20200501.yo:
  - yo
  20200501.za:
  - za
  20200501.zea:
  - zea
  20200501.zh:
  - zh
  20200501.zh-classical:
  - lzh
  20200501.zh-min-nan:
  - nan
  20200501.zh-yue:
  - yue
  20200501.zu:
  - zu
---

# Dataset Card for "wikipedia"

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://dumps.wikimedia.org](https://dumps.wikimedia.org)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 30739.25 MB
- **Size of the generated dataset:** 35376.35 MB
- **Total amount of disk used:** 66115.60 MB

### Dataset Summary

Wikipedia dataset containing cleaned articles of all languages.
The datasets are built from the Wikipedia dump
(https://dumps.wikimedia.org/) with one split per language. Each example
contains the content of one full Wikipedia article with cleaning to strip
markdown and unwanted sections (references, etc.).

The articles are parsed using the ``mwparserfromhell`` tool.

To load this dataset you need to install Apache Beam and ``mwparserfromhell`` first:

```
pip install apache_beam mwparserfromhell
```

Then can load any subset of Wikipedia per language and per date this way:

```python
from datasets import load_dataset

load_dataset("wikipedia", language="sw", date="20220120")
```

You can find the full list of languages and dates [here](https://dumps.wikimedia.org/backup-index.html).

### Supported Tasks and Leaderboards

The dataset is generally used for Language Modeling.

### Languages

You can find the list of languages [here](https://en.wikipedia.org/wiki/List_of_Wikipedias).

## Dataset Structure

We show detailed information for up to 5 configurations of the dataset.

### Data Instances

Some subsets of Wikipedia have already been processed by Hugging face, as you can see below:

#### 20200501.en

- **Size of downloaded dataset files:** 17396.28 MB
- **Size of the generated dataset:** 17481.07 MB
- **Total amount of disk used:** 34877.35 MB

An example looks as follows.
```
{
  'title': 'Yangliuqing',
  'text': 'Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin,
     ...
     and traditional period furnishings and crafts.\n\nSee also \n\nList of township-level divisions of Tianjin\n\nReferences \n\n
     http://arts.cultural-china.com/en/65Arts4795.html\n\nCategory:Towns in Tianjin'
}
```

#### 20200501.de

- **Size of downloaded dataset files:** 5531.82 MB
- **Size of the generated dataset:** 7716.79 MB
- **Total amount of disk used:** 13248.61 MB

#### 20200501.fr

- **Size of downloaded dataset files:** 4653.55 MB
- **Size of the generated dataset:** 6182.24 MB
- **Total amount of disk used:** 10835.79 MB

#### 20200501.frr

- **Size of downloaded dataset files:** 9.05 MB
- **Size of the generated dataset:** 5.88 MB
- **Total amount of disk used:** 14.93 MB

#### 20200501.it

- **Size of downloaded dataset files:** 2970.57 MB
- **Size of the generated dataset:** 3809.89 MB
- **Total amount of disk used:** 6780.46 MB

### Data Fields

The data fields are the same among all splits and configurations:

- `title`: a `string` feature corresponding to the title of the article
- `text`: a `string` feature corresponding to the text content of the article

### Data Splits

Here are the sizes for several configurations:

|    name    | train |
|------------|------:|
|20200501.de |3140341|
|20200501.en |6078422|
|20200501.fr |2210508|
|20200501.frr|  11803|
|20200501.it |1931197|

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@ONLINE {wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}
```

### Contributions

Thanks to [@lewtun](https://github.com/lewtun), [@mariamabarham](https://github.com/mariamabarham), [@thomwolf](https://github.com/thomwolf), [@lhoestq](https://github.com/lhoestq), [@patrickvonplaten](https://github.com/patrickvonplaten) for adding this dataset.